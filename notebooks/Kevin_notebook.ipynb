{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd9d51cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T09:01:35.492079Z",
     "start_time": "2022-12-02T09:01:34.849946Z"
    }
   },
   "outputs": [],
   "source": [
    "#from yahooquery import Ticker\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3a913f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import/Data Yahoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5fdf9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# aapl = Ticker('aapl')\n",
    "\n",
    "# aapl.summary_detail\n",
    "\n",
    "# df = Ticker('aapl').history(period='20y', interval='1d')\n",
    "\n",
    "# df.plot()\n",
    "\n",
    "# aapl.all_modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d532ad65",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import API ALpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2711a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://www.alphavantage.co/documentation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee536ca",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Contrôle de la data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025321ef",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Récolte de la data en CSV, Check s'il y a des valeurs nulles ou doubles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e286cf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.alphavantage.co/query?function=REAL_GDP&interval=quarterly&apikey=1BKEZCMKUSE99CKJ&datatype=csv'\n",
    "r = requests.get(url)\n",
    "data_real_gdp = pd.read_csv(url)\n",
    "\n",
    "data_real_gdp.to_csv(f'data_real_gdp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8945b46",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_real_gdp.isnull().sum()\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd5bd9c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.alphavantage.co/query?function=REAL_GDP_PER_CAPITA&apikey=1BKEZCMKUSE99CKJ&datatype=csv'\n",
    "#r = requests.get(url)\n",
    "data_real_gdp_capita = pd.read_csv(url)\n",
    "\n",
    "len(data_real_gdp_capita)\n",
    "\n",
    "data_real_gdp_capita.to_csv(f'data_real_gdp_capita.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80a6dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_real_gdp_capita.isnull().sum()\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35800c33",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.alphavantage.co/query?function=FEDERAL_FUNDS_RATE&interval=daily&apikey=1BKEZCMKUSE99CKJ&datatype=csv'\n",
    "#r = requests.get(url)\n",
    "data_federal_funds = pd.read_csv(url)\n",
    "\n",
    "len(data_federal_funds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cd946d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_federal_funds.isnull().sum()\n",
    "data_federal_funds.duplicated().sum()\n",
    "data_federal_funds.to_csv(f'data_federal_funds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265fae6a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.alphavantage.co/query?function=CONSUMER_SENTIMENT&apikey=1BKEZCMKUSE99CKJ&datatype=csv'\n",
    "#r = requests.get(url)\n",
    "data_consumer_sentiment = pd.read_csv(url)\n",
    "\n",
    "len(data_consumer_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1996c3c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_consumer_sentiment.duplicated().sum()\n",
    "data_consumer_sentiment.isnull().sum()\n",
    "data_consumer_sentiment.to_csv(f'data_consumer_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f3fbc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.alphavantage.co/query?function=RETAIL_SALES&apikey=1BKEZCMKUSE99CKJ&datatype=csv'\n",
    "#r = requests.get(url)\n",
    "data_retails = pd.read_csv(url)\n",
    "\n",
    "len(data_retails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505cf432",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_retails.duplicated().sum()\n",
    "data_retails.isnull().sum()\n",
    "data_retails.to_csv(f'data_retails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196fc848",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.alphavantage.co/query?function=DURABLES&apikey=1BKEZCMKUSE99CKJ&datatype=csv'\n",
    "#r = requests.get(url)\n",
    "data_durable = pd.read_csv(url)\n",
    "\n",
    "data_durable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58685cc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_durable.duplicated().sum()\n",
    "data_durable.isnull().sum()\n",
    "data_durable.to_csv(f'data_durable.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c8d8f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Treasury yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d6b3b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.alphavantage.co/query?function=TREASURY_YIELD&interval=daily&maturity=10year&apikey=1BKEZCMKUSE99CKJ&datatype=csv'\n",
    "#r = requests.get(url)\n",
    "data_treasury_yield_10 = pd.read_csv(url)\n",
    "\n",
    "data_treasury_yield_10 = data_treasury_yield_10[data_treasury_yield_10['timestamp']>'2000']\n",
    "data_treasury_yield_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583f645",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.alphavantage.co/query?function=TREASURY_YIELD&interval=daily&maturity=2year&apikey=1BKEZCMKUSE99CKJ&datatype=csv'\n",
    "#r = requests.get(url)\n",
    "data_treasury_yield_2 = pd.read_csv(url)\n",
    "#data_treasury_yield_2.dtypes\n",
    "data_treasury_yield_2 = data_treasury_yield_2[data_treasury_yield_2['timestamp']>'2000']\n",
    "data_treasury_yield_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de74e89f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "treasury_yield = data_treasury_yield_10.merge(data_treasury_yield_2, on = 'timestamp')\n",
    "treasury_yield['value_x'] = pd.to_numeric(treasury_yield['value_x'],errors='coerce')\n",
    "treasury_yield['value_y'] = pd.to_numeric(treasury_yield['value_y'],errors='coerce')\n",
    "treasury_yield['value'] = treasury_yield['value_x']-treasury_yield['value_y']\n",
    "treasury_yield.columns = ['timestamp','value_10years','value_2years','value']\n",
    "treasury_yield\n",
    "\n",
    "treasury_yield.to_csv(f'treasury_yield.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fac817b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_treasury_yield.isnull().sum()\n",
    "data_treasury_yield.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e121b4a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Liste 500 entreprises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8685b2bf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list_ent = ['AAPL',\n",
    " 'MSFT',\n",
    " 'GOOG',\n",
    " 'AMZN', \n",
    " 'BRK.B',\n",
    " 'TSLA',\n",
    " 'UNH',\n",
    " 'XOM',\n",
    " 'JNJ',\n",
    " 'WMT',\n",
    " 'NVDA',\n",
    " 'JPM',\n",
    " 'V',\n",
    " 'CVX',\n",
    " 'PG',\n",
    " 'LLY',\n",
    " 'MA',\n",
    " 'HD',\n",
    " 'META',\n",
    " 'BAC',\n",
    " 'ABBV',\n",
    " 'PFE',\n",
    " 'KO',\n",
    " 'MRK',\n",
    " 'PEP',\n",
    " 'COST',\n",
    " 'ORCL',\n",
    " 'AVGO',\n",
    " 'TMO',\n",
    " 'MCD',\n",
    " 'CSCO',\n",
    " 'ACN',\n",
    " 'DHR',\n",
    " 'TMUS',\n",
    " 'ABT',\n",
    " 'WFC',\n",
    " 'DIS',\n",
    " 'LIN',\n",
    " 'NEE',\n",
    " 'BMY',\n",
    " 'NKE',\n",
    " 'VZ',\n",
    " 'TXN',\n",
    " 'UPS',\n",
    " 'COP',\n",
    " 'ADBE',\n",
    " 'CMCSA',\n",
    " 'CRM',\n",
    " 'PM',\n",
    " 'MS',\n",
    " 'AMGN',\n",
    " 'SCHW',\n",
    " 'HON',\n",
    " 'RTX',\n",
    " 'QCOM',\n",
    " 'T',\n",
    " 'IBM',\n",
    " 'DE',\n",
    " 'CVS',\n",
    " 'LOW',\n",
    " 'GS',\n",
    " 'UNP',\n",
    " 'NFLX',\n",
    " 'LMT',\n",
    " 'CAT',\n",
    " 'AMD',\n",
    " 'INTC',\n",
    " 'ELV',\n",
    " 'SPGI',\n",
    " 'AXP',\n",
    " 'SBUX',\n",
    " 'INTU',\n",
    " 'BLK',\n",
    " 'ADP',\n",
    " 'GILD',\n",
    " 'PLD',\n",
    " 'MDT',\n",
    " 'BA',\n",
    " 'AMT',\n",
    " 'CI',\n",
    " 'GE',\n",
    " 'TJX',\n",
    " 'ISRG',\n",
    " 'C',\n",
    " 'AMAT',\n",
    " 'PYPL',\n",
    " 'MDLZ',\n",
    " 'CB',\n",
    " 'SYK',\n",
    " 'ADI',\n",
    " 'MMC',\n",
    " 'EOG',\n",
    " 'NOW',\n",
    " 'VRTX',\n",
    " 'MO',\n",
    " 'NOC',\n",
    " 'EL',\n",
    " 'REGN',\n",
    " 'PGR',\n",
    " 'BKNG',\n",
    " 'DUK',\n",
    " 'TGT',\n",
    " 'SLB',\n",
    " 'SO',\n",
    " 'MMM',\n",
    " 'ITW',\n",
    " 'ZTS',\n",
    " 'GD',\n",
    " 'APD',\n",
    " 'HUM',\n",
    " 'MRNA',\n",
    " 'BDX',\n",
    " 'CSX',\n",
    " 'WM',\n",
    " 'PNC',\n",
    " 'HCA',\n",
    " 'ETN',\n",
    " 'USB',\n",
    " 'FISV',\n",
    " 'SHW',\n",
    " 'OXY',\n",
    " 'CL',\n",
    " 'MU',\n",
    " 'CME',\n",
    " 'AON',\n",
    " 'LRCX',\n",
    " 'BSX',\n",
    " 'EQIX',\n",
    " 'TFC',\n",
    " 'PXD',\n",
    " 'CHTR',\n",
    " 'CCI',\n",
    " 'MET',\n",
    " 'ATVI',\n",
    " 'ICE',\n",
    " 'MPC',\n",
    " 'NSC',\n",
    " 'DG',\n",
    " 'GM',\n",
    " 'EMR',\n",
    " 'F',\n",
    " 'KLAC',\n",
    " 'MCO',\n",
    " 'FCX',\n",
    " 'KDP',\n",
    " 'MNST',\n",
    " 'MCK',\n",
    " 'VLO',\n",
    " 'ORLY',\n",
    " 'ADM',\n",
    " 'PSX',\n",
    " 'PSA',\n",
    " 'SRE',\n",
    " 'SNPS',\n",
    " 'MAR',\n",
    " 'D',\n",
    " 'GIS',\n",
    " 'AEP',\n",
    " 'AZO',\n",
    " 'KHC',\n",
    " 'APH',\n",
    " 'HSY',\n",
    " 'CNC',\n",
    " 'CTVA',\n",
    " 'EW',\n",
    " 'CTAS',\n",
    " 'A',\n",
    " 'ROP',\n",
    " 'JCI',\n",
    " 'CDNS',\n",
    " 'FDX',\n",
    " 'NXPI',\n",
    " 'AIG',\n",
    " 'KMB',\n",
    " 'AFL',\n",
    " 'HES',\n",
    " 'MSI',\n",
    " 'PAYX',\n",
    " 'DVN',\n",
    " 'TRV',\n",
    " 'BIIB',\n",
    " 'DXCM',\n",
    " 'SYY',\n",
    " 'LHX',\n",
    " 'RSG',\n",
    " 'ENPH',\n",
    " 'ECL',\n",
    " 'ADSK',\n",
    " 'MCHP',\n",
    " 'ANET',\n",
    " 'KMI',\n",
    " 'CMG',\n",
    " 'FTNT',\n",
    " 'AJG',\n",
    " 'STZ',\n",
    " 'TT',\n",
    " 'WMB',\n",
    " 'MSCI',\n",
    " 'O',\n",
    " 'IQV',\n",
    " 'TEL',\n",
    " 'ROST',\n",
    " 'PRU',\n",
    " 'EXC',\n",
    " 'PH',\n",
    " 'FIS',\n",
    " 'SPG',\n",
    " 'COF',\n",
    " 'NUE',\n",
    " 'XEL',\n",
    " 'HLT',\n",
    " 'CARR',\n",
    " 'PCAR',\n",
    " 'BK',\n",
    " 'NEM',\n",
    " 'DOW',\n",
    " 'EA',\n",
    " 'WBA',\n",
    " 'DD',\n",
    " 'ALL',\n",
    " 'YUM',\n",
    " 'AMP',\n",
    " 'CMI',\n",
    " 'ILMN',\n",
    " 'BF.B',\n",
    " 'TDG',\n",
    " 'IDXX',\n",
    " 'ED',\n",
    " 'KR',\n",
    " 'ABC',\n",
    " 'DLTR',\n",
    " 'RMD',\n",
    " 'ALB',\n",
    " 'HAL',\n",
    " 'NDAQ',\n",
    " 'LVS',\n",
    " 'ODFL',\n",
    " 'WELL',\n",
    " 'AME',\n",
    " 'CSGP',\n",
    " 'OTIS',\n",
    " 'MTD',\n",
    " 'SBAC',\n",
    " 'ON',\n",
    " 'VICI',\n",
    " 'DLR',\n",
    " 'CEG',\n",
    " 'KEYS',\n",
    " 'PPG',\n",
    " 'WEC',\n",
    " 'CTSH',\n",
    " 'ROK',\n",
    " 'GWW',\n",
    " 'PCG',\n",
    " 'HPQ',\n",
    " 'FAST',\n",
    " 'DFS',\n",
    " 'MTB',\n",
    " 'PEG',\n",
    " 'OKE',\n",
    " 'DHI',\n",
    " 'APTV',\n",
    " 'BKR',\n",
    " 'GLW',\n",
    " 'LYB',\n",
    " 'ES',\n",
    " 'BAX',\n",
    " 'STT',\n",
    " 'VRSK',\n",
    " 'TROW',\n",
    " 'WBD',\n",
    " 'AWK',\n",
    " 'IT',\n",
    " 'GPN',\n",
    " 'HRL',\n",
    " 'FANG',\n",
    " 'WTW',\n",
    " 'RJF',\n",
    " 'GPC',\n",
    " 'IFF',\n",
    " 'CDW',\n",
    " 'TSCO',\n",
    " 'FITB',\n",
    " 'ARE',\n",
    " 'URI',\n",
    " 'ZBH',\n",
    " 'K',\n",
    " 'LEN',\n",
    " 'EBAY',\n",
    " 'EIX',\n",
    " 'CBRE',\n",
    " 'EFX',\n",
    " 'VMC',\n",
    " 'TSN',\n",
    " 'HIG',\n",
    " 'FTV',\n",
    " 'WY',\n",
    " 'EQR',\n",
    " 'AVB',\n",
    " 'MKC',\n",
    " 'ETR',\n",
    " 'LUV',\n",
    " 'ULTA',\n",
    " 'AEE',\n",
    " 'MLM',\n",
    " 'FE',\n",
    " 'PFG',\n",
    " 'FRC',\n",
    " 'DTE',\n",
    " 'DAL',\n",
    " 'HBAN',\n",
    " 'IR',\n",
    " 'CTRA',\n",
    " 'ANSS',\n",
    " 'ACGL',\n",
    " 'PPL',\n",
    " 'RF',\n",
    " 'VRSN',\n",
    " 'LH',\n",
    " 'EXR',\n",
    " 'PWR',\n",
    " 'CF',\n",
    " 'CAH',\n",
    " 'CFG',\n",
    " 'XYL',\n",
    " 'HPE',\n",
    " 'EPAM',\n",
    " 'DOV',\n",
    " 'WAT',\n",
    " 'WRB',\n",
    " 'TDY',\n",
    " 'PAYC',\n",
    " 'ROL',\n",
    " 'NTRS',\n",
    " 'MRO',\n",
    " 'CNP',\n",
    " 'INVH',\n",
    " 'CHD',\n",
    " 'AES',\n",
    " 'MOH',\n",
    " 'JBHT',\n",
    " 'MAA',\n",
    " 'BBY',\n",
    " 'CLX',\n",
    " 'HOLX',\n",
    " 'WAB',\n",
    " 'DRI',\n",
    " 'EXPD',\n",
    " 'STE',\n",
    " 'AMCR',\n",
    " 'VTR',\n",
    " 'IEX',\n",
    " 'CAG',\n",
    " 'CMS',\n",
    " 'KEY',\n",
    " 'MPWR',\n",
    " 'BALL',\n",
    " 'J',\n",
    " 'BR',\n",
    " 'GRMN',\n",
    " 'PKI',\n",
    " 'TTWO',\n",
    " 'INCY',\n",
    " 'FDS',\n",
    " 'MOS',\n",
    " 'SEDG',\n",
    " 'CINF',\n",
    " 'ABMD',\n",
    " 'DGX',\n",
    " 'WST',\n",
    " 'ATO',\n",
    " 'TRGP',\n",
    " 'BRO',\n",
    " 'SYF',\n",
    " 'FOX',\n",
    " 'FOXA',\n",
    " 'NTAP',\n",
    " 'FMC',\n",
    " 'EQT',\n",
    " 'OMC',\n",
    " 'SJM',\n",
    " 'LYV',\n",
    " 'CPB',\n",
    " 'HWM',\n",
    " 'CPRT',\n",
    " 'AVY',\n",
    " 'IRM',\n",
    " 'COO',\n",
    " 'ALGN',\n",
    " 'SWKS',\n",
    " 'EXPE',\n",
    " 'RCL',\n",
    " 'ETSY',\n",
    " 'APA',\n",
    " 'GEN',\n",
    " 'TXT',\n",
    " 'LDOS',\n",
    " 'LKQ',\n",
    " 'TER',\n",
    " 'PTC',\n",
    " 'TRMB',\n",
    " 'AKAM',\n",
    " 'NVR',\n",
    " 'UAL',\n",
    " 'LNT',\n",
    " 'FLT',\n",
    " 'KIM',\n",
    " 'ZBRA',\n",
    " 'TYL',\n",
    " 'DPZ',\n",
    " 'JKHY',\n",
    " 'MGM',\n",
    " 'ESS',\n",
    " 'L',\n",
    " 'PEAK',\n",
    " 'MTCH',\n",
    " 'NDSN',\n",
    " 'EVRG',\n",
    " 'VTRS',\n",
    " 'IPG',\n",
    " 'BEN',\n",
    " 'CBOE',\n",
    " 'TECH',\n",
    " 'SIVB',\n",
    " 'VFC',\n",
    " 'IP',\n",
    " 'HST',\n",
    " 'UDR',\n",
    " 'POOL',\n",
    " 'RE',\n",
    " 'PARA',\n",
    " 'SNA',\n",
    " 'CPT',\n",
    " 'LW',\n",
    " 'PKG',\n",
    " 'CRL',\n",
    " 'SWK',\n",
    " 'BIO',\n",
    " 'WDC',\n",
    " 'CHRW',\n",
    " 'STX',\n",
    " 'MAS',\n",
    " 'GL',\n",
    " 'CE',\n",
    " 'REG',\n",
    " 'NI',\n",
    " 'BXP',\n",
    " 'HSIC',\n",
    " 'CCL',\n",
    " 'TFX',\n",
    " 'CZR',\n",
    " 'NWS',\n",
    " 'NWSA',\n",
    " 'KMX',\n",
    " 'EMN',\n",
    " 'JNPR',\n",
    " 'PHM',\n",
    " 'CDAY',\n",
    " 'ALLE',\n",
    " 'QRVO',\n",
    " 'BWA',\n",
    " 'NRG',\n",
    " 'MKTX',\n",
    " 'WRK',\n",
    " 'UHS',\n",
    " 'FFIV',\n",
    " 'AOS',\n",
    " 'CMA',\n",
    " 'AAL',\n",
    " 'BBWI',\n",
    " 'HII',\n",
    " 'AAP',\n",
    " 'TPR',\n",
    " 'FRT',\n",
    " 'IVZ',\n",
    " 'PNW',\n",
    " 'HAS',\n",
    " 'WYNN',\n",
    " 'FBHS',\n",
    " 'SBNY',\n",
    " 'DISH',\n",
    " 'RHI',\n",
    " 'WHR',\n",
    " 'ZION',\n",
    " 'CTLT',\n",
    " 'PNR',\n",
    " 'SEE',\n",
    " 'RL',\n",
    " 'NCLH',\n",
    " 'DXC',\n",
    " 'GNRC',\n",
    " 'AIZ',\n",
    " 'XRAY',\n",
    " 'LNC',\n",
    " 'DVA',\n",
    " 'MHK',\n",
    " 'OGN',\n",
    " 'LUMN',\n",
    " 'ALK',\n",
    " 'NWL',\n",
    " 'VNO',\n",
    " 'TAP']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c92a8be",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Boucle download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e212a2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb690ee2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Import en CSV, lance la requète toutes les 21 secondes, le CSV se crée là où se situe le notebook\n",
    "\n",
    "#import requests\n",
    "\n",
    "#for i in list_comp :\n",
    "#    url = f'https://www.alphavantage.co/query?function=OVERVIEW&symbol={i}&apikey=1BKEZCMKUSE99CKJ&datatype=csv'\n",
    "#    response = requests.get(url)\n",
    "#    open(f'data_{i}.csv',\"wb\").write(response.content)\n",
    "#     time.sleep(21)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e593e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cce5d206",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fiscalDateEnding</th>\n",
       "      <th>reportedDate</th>\n",
       "      <th>reportedEPS</th>\n",
       "      <th>estimatedEPS</th>\n",
       "      <th>surprise</th>\n",
       "      <th>surprisePercentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.36</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-2.9412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>2022-08-02</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>2022-05-03</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.1</td>\n",
       "      <td>52.6316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>2022-02-23</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-5.814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>2021-10-28</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.22</td>\n",
       "      <td>14.3791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>1997-04-24</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1996-12-31</td>\n",
       "      <td>1997-02-18</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1996-09-30</td>\n",
       "      <td>1996-10-31</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-32.1429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1996-06-30</td>\n",
       "      <td>1996-07-29</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.11</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1996-03-31</td>\n",
       "      <td>1996-04-19</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fiscalDateEnding reportedDate reportedEPS estimatedEPS surprise  \\\n",
       "0         2022-09-30   2022-11-01        1.32         1.36    -0.04   \n",
       "1         2022-06-30   2022-08-02        1.19         1.19        0   \n",
       "2         2022-03-31   2022-05-03        0.29         0.19      0.1   \n",
       "3         2021-12-31   2022-02-23        0.81         0.86    -0.05   \n",
       "4         2021-09-30   2021-10-28        1.75         1.53     0.22   \n",
       "..               ...          ...         ...          ...      ...   \n",
       "102       1997-03-31   1997-04-24        0.11         0.01      0.1   \n",
       "103       1996-12-31   1997-02-18        0.05        -0.01     0.06   \n",
       "104       1996-09-30   1996-10-31        0.19         0.28    -0.09   \n",
       "105       1996-06-30   1996-07-29        0.36         0.25     0.11   \n",
       "106       1996-03-31   1996-04-19       -0.04        -0.01    -0.03   \n",
       "\n",
       "    surprisePercentage  \n",
       "0              -2.9412  \n",
       "1                    0  \n",
       "2              52.6316  \n",
       "3               -5.814  \n",
       "4              14.3791  \n",
       "..                 ...  \n",
       "102               1000  \n",
       "103                600  \n",
       "104           -32.1429  \n",
       "105                 44  \n",
       "106               -300  \n",
       "\n",
       "[107 rows x 6 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import en Json, transforme en CSV\n",
    "import requests\n",
    "import time\n",
    "\n",
    "for i in list_ent :\n",
    "    url = f'https://www.alphavantage.co/query?function=EARNINGS&symbol={i}&apikey=1BKEZCMKUSE99CKJ'\n",
    "    r = requests.get(url)\n",
    "    data = r.json()[\"quarterlyEarnings\"]\n",
    "    data = pd.DataFrame(data)\n",
    "    data.to_csv(f'data_{i}.csv')\n",
    "    time.sleep(13)\n",
    "#    open(f'data_{i}.csv',\"wb\").write(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eaf1b2",
   "metadata": {},
   "source": [
    "# Split the DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d43c8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Creating FOLDS for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "337bbb82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T10:59:06.741152Z",
     "start_time": "2022-12-02T10:59:06.734521Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_folds(\n",
    "    df: pd.DataFrame,\n",
    "    fold_length: int,\n",
    "    fold_stride: int):\n",
    "    '''\n",
    "    This function slides through the Time Series dataframe of shape (n_timesteps, n_features) to create folds\n",
    "    - of equal `fold_length`\n",
    "    - using `fold_stride` between each fold\n",
    "    \n",
    "    Returns a list of folds, each as a DataFrame\n",
    "    '''\n",
    "    # $CHALLENGIFY_BEGIN\n",
    "    \n",
    "    folds = []\n",
    "    for idx in range(0, len(df), fold_stride):\n",
    "        # Exits the loop as soon as the last fold index would exceed the last index\n",
    "        if (idx + fold_length) > len(df):\n",
    "            break\n",
    "        fold = df.iloc[idx:idx + fold_length, :]\n",
    "        folds.append(fold)\n",
    "    return folds\n",
    "\n",
    "    # $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be0da1c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Temporal Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f077b3b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T10:59:09.733723Z",
     "start_time": "2022-12-02T10:59:09.726337Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(fold: pd.DataFrame, \n",
    "                     train_test_ratio: float,\n",
    "                     input_length: int, \n",
    "                     horizon: int) :\n",
    "    '''\n",
    "    Returns a train dataframe and a test dataframe (fold_train, fold_test)\n",
    "    from which one can sample (X,y) sequences.\n",
    "    df_train should contain all the timesteps until round(train_test_ratio * len(fold))   \n",
    "    '''\n",
    "    # $CHALLENGIFY_BEGIN\n",
    "\n",
    "   # TRAIN SET\n",
    "    # ======================\n",
    "    last_train_idx = round(train_test_ratio * len(fold))\n",
    "    fold_train = fold.iloc[0:last_train_idx, :]\n",
    "\n",
    "    # TEST SET\n",
    "    # ======================\n",
    "    first_test_idx = last_train_idx - input_length\n",
    "    fold_test = fold.iloc[first_test_idx:, :]\n",
    "\n",
    "    return (fold_train, fold_test)\n",
    "\n",
    "    # $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dda813",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create (X, y) sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "afc137b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T10:59:14.404657Z",
     "start_time": "2022-12-02T10:59:14.399031Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TARGET = ['return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6adbb10e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T10:59:15.445665Z",
     "start_time": "2022-12-02T10:59:15.431703Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_Xi_yi(first_index: int, \n",
    "              fold: pd.DataFrame, \n",
    "              horizon: int,\n",
    "              input_length: int,\n",
    "              output_length: int):\n",
    "    '''\n",
    "    - extracts one sequence from a fold\n",
    "    - returns a pair (Xi, yi) with:\n",
    "        * len(Xi) = `input_length` and Xi starting at first_index\n",
    "        * len(yi) = `output_length`\n",
    "        * last_Xi and first_yi separated by the gap = horizon -1\n",
    "    '''\n",
    "\n",
    "    # $CHALLENGIFY_BEGIN\n",
    "\n",
    "    Xi_start = first_index\n",
    "    Xi_last = Xi_start + input_length\n",
    "    yi_start = Xi_last + horizon - 1\n",
    "    yi_last = yi_start + output_length\n",
    "\n",
    "    Xi = fold[Xi_start:Xi_last]\n",
    "    yi = fold[yi_start:yi_last][TARGET]\n",
    "\n",
    "    return (Xi, yi)\n",
    "\n",
    "    # $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f59f0a4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:01:09.189958Z",
     "start_time": "2022-12-02T11:01:09.180846Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_X_y(fold: pd.DataFrame,\n",
    "            horizon: int,\n",
    "            input_length: int,\n",
    "            output_length: int,\n",
    "            stride: int,\n",
    "            shuffle=True,\n",
    "           new=True):\n",
    "    \"\"\"\n",
    "    - Uses `data`, a 2D-array with axis=0 for timesteps, and axis=1 for (targets+covariates columns)\n",
    "    - Returns a Tuple (X,y) of two ndarrays :\n",
    "        * X.shape = (n_samples, input_length, n_covariates)\n",
    "        * y.shape =\n",
    "            (n_samples, output_length, n_targets) if all 3-dimensions are of size > 1\n",
    "            (n_samples, output_length) if n_targets == 1\n",
    "            (n_samples, n_targets) if output_length == 1\n",
    "            (n_samples, ) if both n_targets and lenghts == 1\n",
    "    - You can shuffle the pairs (Xi,yi) of your fold\n",
    "    \"\"\"\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(0, len(fold), stride):\n",
    "        ## Extracting a sequence starting at index_i\n",
    "        Xi, yi = get_Xi_yi(first_index=i,\n",
    "                           fold=fold_test,\n",
    "                           horizon=horizon,\n",
    "                           input_length=input_length,\n",
    "                           output_length=output_length)\n",
    "        ## Exits loop as soon as we reach the end of the dataset\n",
    "        if len(yi) < output_length:\n",
    "            break\n",
    "        X.append(Xi)\n",
    "        y.append(yi)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    y = np.squeeze(y)\n",
    "\n",
    "    if shuffle:\n",
    "        idx = np.arange(len(X))\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b2c35c3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:01:11.352789Z",
     "start_time": "2022-12-02T11:01:11.205273Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes for the training set:\n",
      "X_train.shape = (76, 10, 50), y_train.shape = (76,)\n",
      "Shapes for the test set:\n",
      "X_test.shape = (76, 10, 50), y_test.shape = (76,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_X_y(fold=fold_train,\n",
    "                           horizon=horizon,\n",
    "                           input_length=input_length,\n",
    "                           output_length=output_length,\n",
    "                           stride=stride)\n",
    "X_test, y_test = get_X_y(fold=fold_test,\n",
    "                         horizon=horizon,\n",
    "                         input_length=input_length,\n",
    "                         output_length=output_length,\n",
    "                         stride=stride)\n",
    "\n",
    "print(\"Shapes for the training set:\")\n",
    "print(f\"X_train.shape = {X_train.shape}, y_train.shape = {y_train.shape}\")\n",
    "\n",
    "print(\"Shapes for the test set:\")\n",
    "print(f\"X_test.shape = {X_test.shape}, y_test.shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e4bea",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d58847",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "921da3a1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Mean Absolute Error seems to be a reasonable metrics to evaluate a model's capability to predict the temperature:\n",
    "\n",
    "$$ MAE = \\frac{1}{n_{samples}} \\times \\sum_{i = 1}^{n_{samples}} |y_{true}^{(i)} - y_{pred}^{(i)}|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1f10cad8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TARGET_COLUMN_IDX = 1 # the second column of each fold corresponds to the temperature\n",
    "\n",
    "def last_seen_value_baseline(X, y):\n",
    "\n",
    "    # How many values do you want to predict in the future ?\n",
    "    output_length = y.shape[-1]\n",
    "    \n",
    "    # For each sequence, let's consider the last seen value\n",
    "    # and only the temperature column\n",
    "    last_seen_values = X[:,-1, TARGET_COLUMN_IDX].reshape(-1,1)\n",
    "\n",
    "    # We need to duplicate these values as many times as output_length\n",
    "    # The author of this notebook did not know how to do it, so they searched on Stackoverflow\n",
    "    # and found this nice np.repeat in Numpy, which is self-explanatory\n",
    "    repeated = np.repeat(last_seen_values, axis = 1, repeats = output_length)\n",
    "\n",
    "    return np.mean(np.abs(y_test - repeated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0cece1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Cross-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b1bed87f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def cross_validate():\n",
    "    '''\n",
    "    This function cross-validates \n",
    "    '''\n",
    "    list_of_score_model = []\n",
    "    \n",
    "    # 0 - Creating folds\n",
    "    # =========================================    \n",
    "    folds = get_folds(df, FOLD_LENGTH, FOLD_STRIDE)\n",
    "    \n",
    "    for fold_id, fold in enumerate(folds):\n",
    "        \n",
    "        # 1 - Train/Test split the current fold\n",
    "        # =========================================\n",
    "        (fold_train, fold_test) = train_test_split(fold, TRAIN_TEST_RATIO, INPUT_LENGTH)                   \n",
    "\n",
    "        X_train, y_train = get_X_y_strides(fold_train, INPUT_LENGTH, OUTPUT_LENGTH, SEQUENCE_STRIDE)\n",
    "        X_test, y_test = get_X_y_strides(fold_test, INPUT_LENGTH, OUTPUT_LENGTH, SEQUENCE_STRIDE)\n",
    "        \n",
    "        # 2 - Modelling\n",
    "        # =========================================\n",
    "        model = LinearRegression()\n",
    "        cv_results = cross_validate(model, X, y, cv=5)\n",
    "        cvr = cv_results['test_score'].mean()\n",
    "        list_of_score_model.append(cvr)\n",
    "\n",
    "    return list_of_score_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "799022cf",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cv \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[101], line 11\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m list_of_score_model \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 0 - Creating folds\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# =========================================    \u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m folds \u001b[38;5;241m=\u001b[39m get_folds(\u001b[43mdf\u001b[49m, FOLD_LENGTH, FOLD_STRIDE)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_id, fold \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(folds):\n\u001b[1;32m     14\u001b[0m     \n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# 1 - Train/Test split the current fold\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# =========================================\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     (fold_train, fold_test) \u001b[38;5;241m=\u001b[39m train_test_split(fold, TRAIN_TEST_RATIO, INPUT_LENGTH)                   \n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "cv = cross_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487edc9d",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9959a5a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e19179",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:54:51.972166Z",
     "start_time": "2022-12-02T08:54:50.024331Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from dynamic_portfolio.utils import features_creation\n",
    "from dynamic_portfolio.preprocess import scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "374a3832",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3a4ea95c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd9390b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:54:53.701042Z",
     "start_time": "2022-12-02T08:54:53.108461Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogersiv/code/RogerVI/dynamic_portfolio/dynamic_portfolio/utils.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['return'][0]=0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>dividend_amount</th>\n",
       "      <th>split_coefficient</th>\n",
       "      <th>reportedEPS</th>\n",
       "      <th>surprisePercentage</th>\n",
       "      <th>10Y_yield</th>\n",
       "      <th>2Y_yield</th>\n",
       "      <th>10_2_spread</th>\n",
       "      <th>oil_price</th>\n",
       "      <th>orders</th>\n",
       "      <th>...</th>\n",
       "      <th>gold_return</th>\n",
       "      <th>10Y_return</th>\n",
       "      <th>2Y_return</th>\n",
       "      <th>spread_return</th>\n",
       "      <th>oil_return</th>\n",
       "      <th>usd_return</th>\n",
       "      <th>unemployement_return</th>\n",
       "      <th>cpi_return</th>\n",
       "      <th>non_farm_payroll_return</th>\n",
       "      <th>gdp_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.346003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.520563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.683449</td>\n",
       "      <td>2.008561</td>\n",
       "      <td>-1.025447</td>\n",
       "      <td>-1.302865</td>\n",
       "      <td>-1.106639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433807</td>\n",
       "      <td>-0.548798</td>\n",
       "      <td>-0.525452</td>\n",
       "      <td>0.685932</td>\n",
       "      <td>0.415832</td>\n",
       "      <td>-1.588763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661811</td>\n",
       "      <td>-2.436691</td>\n",
       "      <td>-0.129723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.614623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.520563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.741110</td>\n",
       "      <td>2.029087</td>\n",
       "      <td>-0.979457</td>\n",
       "      <td>-1.237085</td>\n",
       "      <td>-1.106639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106335</td>\n",
       "      <td>0.535720</td>\n",
       "      <td>0.104186</td>\n",
       "      <td>0.479318</td>\n",
       "      <td>1.062331</td>\n",
       "      <td>1.322936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661811</td>\n",
       "      <td>-2.436691</td>\n",
       "      <td>-0.129723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.768118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.520563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.790534</td>\n",
       "      <td>2.029087</td>\n",
       "      <td>-0.910473</td>\n",
       "      <td>-1.233602</td>\n",
       "      <td>-1.106639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.692156</td>\n",
       "      <td>0.451417</td>\n",
       "      <td>-0.024290</td>\n",
       "      <td>0.654627</td>\n",
       "      <td>0.054477</td>\n",
       "      <td>-0.768696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661811</td>\n",
       "      <td>-2.436691</td>\n",
       "      <td>-0.129723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.553266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.520563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.831721</td>\n",
       "      <td>2.076982</td>\n",
       "      <td>-0.933468</td>\n",
       "      <td>-1.254884</td>\n",
       "      <td>-1.106639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.174732</td>\n",
       "      <td>0.370081</td>\n",
       "      <td>0.273594</td>\n",
       "      <td>-0.195624</td>\n",
       "      <td>-0.322929</td>\n",
       "      <td>-0.001263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661811</td>\n",
       "      <td>-2.436691</td>\n",
       "      <td>-0.129723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.650157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.520563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.856433</td>\n",
       "      <td>2.070140</td>\n",
       "      <td>-0.887478</td>\n",
       "      <td>-1.262623</td>\n",
       "      <td>-1.106639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.588911</td>\n",
       "      <td>0.215807</td>\n",
       "      <td>-0.066227</td>\n",
       "      <td>0.398967</td>\n",
       "      <td>-0.118665</td>\n",
       "      <td>2.639928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661811</td>\n",
       "      <td>-2.436691</td>\n",
       "      <td>-0.129723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5429</th>\n",
       "      <td>1.067172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.920188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.489039</td>\n",
       "      <td>1.755402</td>\n",
       "      <td>-2.267170</td>\n",
       "      <td>0.833050</td>\n",
       "      <td>1.298006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268393</td>\n",
       "      <td>-1.376304</td>\n",
       "      <td>-0.116800</td>\n",
       "      <td>0.927587</td>\n",
       "      <td>-0.271928</td>\n",
       "      <td>-0.229311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.426518</td>\n",
       "      <td>-0.373957</td>\n",
       "      <td>-0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>1.607330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.920188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571412</td>\n",
       "      <td>1.810139</td>\n",
       "      <td>-2.244175</td>\n",
       "      <td>0.680982</td>\n",
       "      <td>1.298006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705066</td>\n",
       "      <td>1.077836</td>\n",
       "      <td>0.347453</td>\n",
       "      <td>-0.144578</td>\n",
       "      <td>-0.870220</td>\n",
       "      <td>0.778768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.426518</td>\n",
       "      <td>-0.373957</td>\n",
       "      <td>-0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5431</th>\n",
       "      <td>1.421618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.920188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612599</td>\n",
       "      <td>1.864876</td>\n",
       "      <td>-2.278668</td>\n",
       "      <td>0.618298</td>\n",
       "      <td>1.298006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.617618</td>\n",
       "      <td>0.519377</td>\n",
       "      <td>0.340740</td>\n",
       "      <td>0.216354</td>\n",
       "      <td>-0.375249</td>\n",
       "      <td>0.453578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.426518</td>\n",
       "      <td>-0.373957</td>\n",
       "      <td>-0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5432</th>\n",
       "      <td>0.883652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.920188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.620836</td>\n",
       "      <td>1.844350</td>\n",
       "      <td>-2.232678</td>\n",
       "      <td>0.605528</td>\n",
       "      <td>1.298006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.663414</td>\n",
       "      <td>0.094312</td>\n",
       "      <td>-0.158748</td>\n",
       "      <td>-0.282262</td>\n",
       "      <td>-0.076975</td>\n",
       "      <td>1.719479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.426518</td>\n",
       "      <td>-0.373957</td>\n",
       "      <td>-0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5433</th>\n",
       "      <td>0.652508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.920188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.563175</td>\n",
       "      <td>1.837508</td>\n",
       "      <td>-2.301663</td>\n",
       "      <td>0.652348</td>\n",
       "      <td>1.298006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091611</td>\n",
       "      <td>-0.740043</td>\n",
       "      <td>-0.069409</td>\n",
       "      <td>0.442233</td>\n",
       "      <td>0.289386</td>\n",
       "      <td>-1.163740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.426518</td>\n",
       "      <td>-0.373957</td>\n",
       "      <td>-0.999994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5434 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        volume  dividend_amount  split_coefficient  reportedEPS  \\\n",
       "0    -0.346003              0.0                0.0    -0.520563   \n",
       "1    -0.614623              0.0                0.0    -0.520563   \n",
       "2    -0.768118              0.0                0.0    -0.520563   \n",
       "3    -0.553266              0.0                0.0    -0.520563   \n",
       "4    -0.650157              0.0                0.0    -0.520563   \n",
       "...        ...              ...                ...          ...   \n",
       "5429  1.067172              0.0                0.0     1.920188   \n",
       "5430  1.607330              0.0                0.0     1.920188   \n",
       "5431  1.421618              0.0                0.0     1.920188   \n",
       "5432  0.883652              0.0                0.0     1.920188   \n",
       "5433  0.652508              0.0                0.0     1.920188   \n",
       "\n",
       "      surprisePercentage  10Y_yield  2Y_yield  10_2_spread  oil_price  \\\n",
       "0                    0.0   1.683449  2.008561    -1.025447  -1.302865   \n",
       "1                    0.0   1.741110  2.029087    -0.979457  -1.237085   \n",
       "2                    0.0   1.790534  2.029087    -0.910473  -1.233602   \n",
       "3                    0.0   1.831721  2.076982    -0.933468  -1.254884   \n",
       "4                    0.0   1.856433  2.070140    -0.887478  -1.262623   \n",
       "...                  ...        ...       ...          ...        ...   \n",
       "5429                 0.0   0.489039  1.755402    -2.267170   0.833050   \n",
       "5430                 0.0   0.571412  1.810139    -2.244175   0.680982   \n",
       "5431                 0.0   0.612599  1.864876    -2.278668   0.618298   \n",
       "5432                 0.0   0.620836  1.844350    -2.232678   0.605528   \n",
       "5433                 0.0   0.563175  1.837508    -2.301663   0.652348   \n",
       "\n",
       "        orders  ...  gold_return  10Y_return  2Y_return  spread_return  \\\n",
       "0    -1.106639  ...     0.433807   -0.548798  -0.525452       0.685932   \n",
       "1    -1.106639  ...    -0.106335    0.535720   0.104186       0.479318   \n",
       "2    -1.106639  ...     0.692156    0.451417  -0.024290       0.654627   \n",
       "3    -1.106639  ...    -0.174732    0.370081   0.273594      -0.195624   \n",
       "4    -1.106639  ...    -0.588911    0.215807  -0.066227       0.398967   \n",
       "...        ...  ...          ...         ...        ...            ...   \n",
       "5429  1.298006  ...    -0.268393   -1.376304  -0.116800       0.927587   \n",
       "5430  1.298006  ...    -0.705066    1.077836   0.347453      -0.144578   \n",
       "5431  1.298006  ...    -0.617618    0.519377   0.340740       0.216354   \n",
       "5432  1.298006  ...    -0.663414    0.094312  -0.158748      -0.282262   \n",
       "5433  1.298006  ...     0.091611   -0.740043  -0.069409       0.442233   \n",
       "\n",
       "      oil_return  usd_return  unemployement_return  cpi_return  \\\n",
       "0       0.415832   -1.588763                   0.0    0.661811   \n",
       "1       1.062331    1.322936                   0.0    0.661811   \n",
       "2       0.054477   -0.768696                   0.0    0.661811   \n",
       "3      -0.322929   -0.001263                   0.0    0.661811   \n",
       "4      -0.118665    2.639928                   0.0    0.661811   \n",
       "...          ...         ...                   ...         ...   \n",
       "5429   -0.271928   -0.229311                   0.0   -0.426518   \n",
       "5430   -0.870220    0.778768                   0.0   -0.426518   \n",
       "5431   -0.375249    0.453578                   0.0   -0.426518   \n",
       "5432   -0.076975    1.719479                   0.0   -0.426518   \n",
       "5433    0.289386   -1.163740                   0.0   -0.426518   \n",
       "\n",
       "      non_farm_payroll_return  gdp_return  \n",
       "0                   -2.436691   -0.129723  \n",
       "1                   -2.436691   -0.129723  \n",
       "2                   -2.436691   -0.129723  \n",
       "3                   -2.436691   -0.129723  \n",
       "4                   -2.436691   -0.129723  \n",
       "...                       ...         ...  \n",
       "5429                -0.373957   -0.999994  \n",
       "5430                -0.373957   -0.999994  \n",
       "5431                -0.373957   -0.999994  \n",
       "5432                -0.373957   -0.999994  \n",
       "5433                -0.373957   -0.999994  \n",
       "\n",
       "[5434 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = features_creation('AAPL')\n",
    "test = scaler(test)\n",
    "test = test.drop(columns='date')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "672f8885",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5.434000e+03\n",
       "mean    -3.190512e-16\n",
       "std      1.000092e+00\n",
       "min     -2.485537e+00\n",
       "25%     -6.678984e-01\n",
       "50%     -6.534540e-02\n",
       "75%      5.454258e-01\n",
       "max      1.294764e+01\n",
       "Name: volume_20days, dtype: float64"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['volume_20days'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "eeba7b8b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='volume_20days', ylabel='Count'>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(test['volume_20days'], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99247938",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f6022f10",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fold_length = 252 #1 an\n",
    "fold_stride = 60 #1 trimestre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "69d6b19f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "folds_test = get_folds(test, fold_length, fold_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "3415865d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "folds_test = folds_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "0e1460a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>dividend_amount</th>\n",
       "      <th>split_coefficient</th>\n",
       "      <th>reportedEPS</th>\n",
       "      <th>surprisePercentage</th>\n",
       "      <th>10Y_yield</th>\n",
       "      <th>2Y_yield</th>\n",
       "      <th>10_2_spread</th>\n",
       "      <th>oil_price</th>\n",
       "      <th>orders</th>\n",
       "      <th>fed_funds</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>inf_exp</th>\n",
       "      <th>non_farm_payroll</th>\n",
       "      <th>CPI</th>\n",
       "      <th>retail_sales</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>gold_price</th>\n",
       "      <th>usd_price</th>\n",
       "      <th>return</th>\n",
       "      <th>high/low</th>\n",
       "      <th>volatility_5days</th>\n",
       "      <th>volatility_10days</th>\n",
       "      <th>volatility_20days</th>\n",
       "      <th>momentum_5days</th>\n",
       "      <th>momentum_10days</th>\n",
       "      <th>momentum_20days</th>\n",
       "      <th>distance_5days</th>\n",
       "      <th>distance_10days</th>\n",
       "      <th>distance_20days</th>\n",
       "      <th>volume_5days</th>\n",
       "      <th>volume_10days</th>\n",
       "      <th>volume_20days</th>\n",
       "      <th>volume_momentum_5days</th>\n",
       "      <th>volume_momentum_10days</th>\n",
       "      <th>volume_momentum_20days</th>\n",
       "      <th>price/eps</th>\n",
       "      <th>momentum_5days/eps</th>\n",
       "      <th>momentum_10days/eps</th>\n",
       "      <th>momentum_20days/eps</th>\n",
       "      <th>gold_return</th>\n",
       "      <th>10Y_return</th>\n",
       "      <th>2Y_return</th>\n",
       "      <th>spread_return</th>\n",
       "      <th>oil_return</th>\n",
       "      <th>usd_return</th>\n",
       "      <th>unemployement_return</th>\n",
       "      <th>cpi_return</th>\n",
       "      <th>non_farm_payroll_return</th>\n",
       "      <th>gdp_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.907754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.520563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.815246</td>\n",
       "      <td>1.851192</td>\n",
       "      <td>-0.577047</td>\n",
       "      <td>-1.365162</td>\n",
       "      <td>-1.109940</td>\n",
       "      <td>2.341591</td>\n",
       "      <td>-0.434783</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.372520</td>\n",
       "      <td>-1.157703</td>\n",
       "      <td>-0.930548</td>\n",
       "      <td>-1.074003</td>\n",
       "      <td>-1.623383</td>\n",
       "      <td>2.370717</td>\n",
       "      <td>-0.043711</td>\n",
       "      <td>2.504654</td>\n",
       "      <td>1.390193</td>\n",
       "      <td>2.223102</td>\n",
       "      <td>2.433076</td>\n",
       "      <td>1.381883</td>\n",
       "      <td>-0.152540</td>\n",
       "      <td>1.216896</td>\n",
       "      <td>0.664370</td>\n",
       "      <td>1.299521</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.180336</td>\n",
       "      <td>0.629591</td>\n",
       "      <td>1.367242</td>\n",
       "      <td>-0.934614</td>\n",
       "      <td>-0.919944</td>\n",
       "      <td>-0.876576</td>\n",
       "      <td>-1.034180</td>\n",
       "      <td>-0.496821</td>\n",
       "      <td>-0.063924</td>\n",
       "      <td>-0.536806</td>\n",
       "      <td>1.111833</td>\n",
       "      <td>0.839389</td>\n",
       "      <td>0.578407</td>\n",
       "      <td>-0.121821</td>\n",
       "      <td>0.040901</td>\n",
       "      <td>1.118750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257408</td>\n",
       "      <td>-0.078999</td>\n",
       "      <td>-0.927652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.669180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.520563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.757585</td>\n",
       "      <td>1.796455</td>\n",
       "      <td>-0.565550</td>\n",
       "      <td>-1.400761</td>\n",
       "      <td>-1.109940</td>\n",
       "      <td>2.229461</td>\n",
       "      <td>-0.434783</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.372520</td>\n",
       "      <td>-1.157703</td>\n",
       "      <td>-0.930548</td>\n",
       "      <td>-1.074003</td>\n",
       "      <td>-1.628149</td>\n",
       "      <td>2.396675</td>\n",
       "      <td>-0.048507</td>\n",
       "      <td>2.951707</td>\n",
       "      <td>2.008878</td>\n",
       "      <td>2.390024</td>\n",
       "      <td>2.579241</td>\n",
       "      <td>0.762514</td>\n",
       "      <td>-0.489142</td>\n",
       "      <td>0.808909</td>\n",
       "      <td>-2.590241</td>\n",
       "      <td>-1.933475</td>\n",
       "      <td>-2.219975</td>\n",
       "      <td>0.267719</td>\n",
       "      <td>0.695483</td>\n",
       "      <td>1.488856</td>\n",
       "      <td>-0.939661</td>\n",
       "      <td>-0.927448</td>\n",
       "      <td>-0.882765</td>\n",
       "      <td>-1.016139</td>\n",
       "      <td>-0.311877</td>\n",
       "      <td>0.034995</td>\n",
       "      <td>-0.409100</td>\n",
       "      <td>-0.874803</td>\n",
       "      <td>-0.539618</td>\n",
       "      <td>-0.384442</td>\n",
       "      <td>0.058242</td>\n",
       "      <td>-0.605031</td>\n",
       "      <td>0.486041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257408</td>\n",
       "      <td>-0.078999</td>\n",
       "      <td>-0.927652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.421259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.483568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.699924</td>\n",
       "      <td>1.693823</td>\n",
       "      <td>-0.473570</td>\n",
       "      <td>-1.403082</td>\n",
       "      <td>-1.109940</td>\n",
       "      <td>2.030120</td>\n",
       "      <td>-0.434783</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.372520</td>\n",
       "      <td>-1.157703</td>\n",
       "      <td>-0.930548</td>\n",
       "      <td>-1.074003</td>\n",
       "      <td>-1.627652</td>\n",
       "      <td>2.366081</td>\n",
       "      <td>0.117157</td>\n",
       "      <td>6.185921</td>\n",
       "      <td>2.593196</td>\n",
       "      <td>2.577283</td>\n",
       "      <td>2.617154</td>\n",
       "      <td>-0.179567</td>\n",
       "      <td>-0.875811</td>\n",
       "      <td>-0.170702</td>\n",
       "      <td>-2.372619</td>\n",
       "      <td>-2.031668</td>\n",
       "      <td>-2.218872</td>\n",
       "      <td>-1.561741</td>\n",
       "      <td>-1.418738</td>\n",
       "      <td>-1.036291</td>\n",
       "      <td>-0.886397</td>\n",
       "      <td>-0.905005</td>\n",
       "      <td>-0.871757</td>\n",
       "      <td>-0.278768</td>\n",
       "      <td>-0.050284</td>\n",
       "      <td>-0.238856</td>\n",
       "      <td>-0.026941</td>\n",
       "      <td>0.051131</td>\n",
       "      <td>-0.546731</td>\n",
       "      <td>-0.711825</td>\n",
       "      <td>0.479318</td>\n",
       "      <td>-0.039570</td>\n",
       "      <td>-0.574211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257408</td>\n",
       "      <td>-0.078999</td>\n",
       "      <td>-0.927652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.039713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.483568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.807009</td>\n",
       "      <td>1.714350</td>\n",
       "      <td>-0.358596</td>\n",
       "      <td>-1.399600</td>\n",
       "      <td>-1.109940</td>\n",
       "      <td>1.905531</td>\n",
       "      <td>-0.434783</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.372520</td>\n",
       "      <td>-1.157703</td>\n",
       "      <td>-0.930548</td>\n",
       "      <td>-1.074003</td>\n",
       "      <td>-1.619213</td>\n",
       "      <td>2.263176</td>\n",
       "      <td>0.128565</td>\n",
       "      <td>3.433239</td>\n",
       "      <td>3.961000</td>\n",
       "      <td>3.390815</td>\n",
       "      <td>3.200521</td>\n",
       "      <td>0.722637</td>\n",
       "      <td>1.720708</td>\n",
       "      <td>1.449545</td>\n",
       "      <td>5.350638</td>\n",
       "      <td>4.807150</td>\n",
       "      <td>4.907594</td>\n",
       "      <td>-1.928345</td>\n",
       "      <td>-1.889394</td>\n",
       "      <td>-1.685050</td>\n",
       "      <td>-0.804525</td>\n",
       "      <td>-0.857276</td>\n",
       "      <td>-0.851640</td>\n",
       "      <td>-0.229465</td>\n",
       "      <td>0.211034</td>\n",
       "      <td>0.501303</td>\n",
       "      <td>0.465008</td>\n",
       "      <td>1.458973</td>\n",
       "      <td>0.999724</td>\n",
       "      <td>0.118059</td>\n",
       "      <td>0.545059</td>\n",
       "      <td>0.062678</td>\n",
       "      <td>-1.933919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257408</td>\n",
       "      <td>-0.078999</td>\n",
       "      <td>-0.927652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-0.664286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.483568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.823483</td>\n",
       "      <td>1.693823</td>\n",
       "      <td>-0.301108</td>\n",
       "      <td>-1.435199</td>\n",
       "      <td>-1.109940</td>\n",
       "      <td>1.849466</td>\n",
       "      <td>-0.434783</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.372520</td>\n",
       "      <td>-1.157703</td>\n",
       "      <td>-0.930548</td>\n",
       "      <td>-1.074003</td>\n",
       "      <td>-1.620901</td>\n",
       "      <td>2.198281</td>\n",
       "      <td>-0.026439</td>\n",
       "      <td>0.787062</td>\n",
       "      <td>5.311561</td>\n",
       "      <td>4.079131</td>\n",
       "      <td>3.876676</td>\n",
       "      <td>3.573767</td>\n",
       "      <td>4.106838</td>\n",
       "      <td>2.533896</td>\n",
       "      <td>4.538980</td>\n",
       "      <td>4.570802</td>\n",
       "      <td>5.182002</td>\n",
       "      <td>-2.055787</td>\n",
       "      <td>-2.236214</td>\n",
       "      <td>-2.099150</td>\n",
       "      <td>-0.608636</td>\n",
       "      <td>-0.779819</td>\n",
       "      <td>-0.801408</td>\n",
       "      <td>-0.240907</td>\n",
       "      <td>1.036843</td>\n",
       "      <td>1.181489</td>\n",
       "      <td>0.794245</td>\n",
       "      <td>-0.331340</td>\n",
       "      <td>0.141322</td>\n",
       "      <td>-0.165643</td>\n",
       "      <td>0.243187</td>\n",
       "      <td>-0.624358</td>\n",
       "      <td>-1.231811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257408</td>\n",
       "      <td>-0.078999</td>\n",
       "      <td>-0.927652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>-0.853963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.498592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.757585</td>\n",
       "      <td>1.078032</td>\n",
       "      <td>0.641681</td>\n",
       "      <td>-1.457254</td>\n",
       "      <td>-1.037559</td>\n",
       "      <td>0.161293</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.518211</td>\n",
       "      <td>-1.090239</td>\n",
       "      <td>-0.855211</td>\n",
       "      <td>-1.037967</td>\n",
       "      <td>-1.546038</td>\n",
       "      <td>2.380915</td>\n",
       "      <td>-0.016922</td>\n",
       "      <td>-0.256054</td>\n",
       "      <td>0.131298</td>\n",
       "      <td>-0.213271</td>\n",
       "      <td>0.057187</td>\n",
       "      <td>0.320408</td>\n",
       "      <td>0.107521</td>\n",
       "      <td>-0.052084</td>\n",
       "      <td>-1.544993</td>\n",
       "      <td>-1.342076</td>\n",
       "      <td>-1.264671</td>\n",
       "      <td>-0.197207</td>\n",
       "      <td>-0.725304</td>\n",
       "      <td>-0.997661</td>\n",
       "      <td>-0.890046</td>\n",
       "      <td>-0.954382</td>\n",
       "      <td>-1.004913</td>\n",
       "      <td>1.439429</td>\n",
       "      <td>0.634962</td>\n",
       "      <td>0.399515</td>\n",
       "      <td>0.301886</td>\n",
       "      <td>-0.430579</td>\n",
       "      <td>-0.162923</td>\n",
       "      <td>-0.143896</td>\n",
       "      <td>-0.002783</td>\n",
       "      <td>0.419775</td>\n",
       "      <td>-0.174985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536311</td>\n",
       "      <td>0.060453</td>\n",
       "      <td>-0.675370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>-0.917177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.498592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.741110</td>\n",
       "      <td>1.084874</td>\n",
       "      <td>0.607189</td>\n",
       "      <td>-1.463058</td>\n",
       "      <td>-1.037559</td>\n",
       "      <td>0.198670</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.518211</td>\n",
       "      <td>-1.090239</td>\n",
       "      <td>-0.855211</td>\n",
       "      <td>-1.037967</td>\n",
       "      <td>-1.544946</td>\n",
       "      <td>2.393894</td>\n",
       "      <td>-0.018014</td>\n",
       "      <td>0.089389</td>\n",
       "      <td>0.281764</td>\n",
       "      <td>-0.134442</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>-0.190050</td>\n",
       "      <td>-0.044160</td>\n",
       "      <td>0.040993</td>\n",
       "      <td>-0.812219</td>\n",
       "      <td>-0.835662</td>\n",
       "      <td>-0.838544</td>\n",
       "      <td>0.136353</td>\n",
       "      <td>-0.478645</td>\n",
       "      <td>-0.945912</td>\n",
       "      <td>-0.883027</td>\n",
       "      <td>-0.948055</td>\n",
       "      <td>-1.012969</td>\n",
       "      <td>1.401568</td>\n",
       "      <td>-0.104291</td>\n",
       "      <td>0.183324</td>\n",
       "      <td>0.443188</td>\n",
       "      <td>0.130304</td>\n",
       "      <td>-0.163509</td>\n",
       "      <td>0.035870</td>\n",
       "      <td>-0.080962</td>\n",
       "      <td>-0.106480</td>\n",
       "      <td>0.242159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536311</td>\n",
       "      <td>0.060453</td>\n",
       "      <td>-0.675370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>-0.938625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.498592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.732873</td>\n",
       "      <td>1.098558</td>\n",
       "      <td>0.572697</td>\n",
       "      <td>-1.463058</td>\n",
       "      <td>-1.037559</td>\n",
       "      <td>0.186211</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.518211</td>\n",
       "      <td>-1.090239</td>\n",
       "      <td>-0.855211</td>\n",
       "      <td>-1.037967</td>\n",
       "      <td>-1.543456</td>\n",
       "      <td>2.406873</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>0.076101</td>\n",
       "      <td>0.392497</td>\n",
       "      <td>-0.057848</td>\n",
       "      <td>0.041606</td>\n",
       "      <td>-0.509403</td>\n",
       "      <td>-0.199339</td>\n",
       "      <td>-0.066682</td>\n",
       "      <td>-0.712152</td>\n",
       "      <td>-0.836499</td>\n",
       "      <td>-0.863944</td>\n",
       "      <td>1.517418</td>\n",
       "      <td>0.590862</td>\n",
       "      <td>-0.126360</td>\n",
       "      <td>-0.886837</td>\n",
       "      <td>-0.947539</td>\n",
       "      <td>-1.010721</td>\n",
       "      <td>1.378011</td>\n",
       "      <td>-0.566782</td>\n",
       "      <td>-0.037850</td>\n",
       "      <td>0.279723</td>\n",
       "      <td>0.190708</td>\n",
       "      <td>-0.087161</td>\n",
       "      <td>0.095672</td>\n",
       "      <td>-0.082250</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.241866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536311</td>\n",
       "      <td>0.060453</td>\n",
       "      <td>-0.675370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>-0.994110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.498592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.675212</td>\n",
       "      <td>1.023295</td>\n",
       "      <td>0.618687</td>\n",
       "      <td>-1.463058</td>\n",
       "      <td>-1.037559</td>\n",
       "      <td>0.217358</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.518211</td>\n",
       "      <td>-1.090239</td>\n",
       "      <td>-0.855211</td>\n",
       "      <td>-1.037967</td>\n",
       "      <td>-1.542662</td>\n",
       "      <td>2.367936</td>\n",
       "      <td>-0.019794</td>\n",
       "      <td>0.397834</td>\n",
       "      <td>-0.308511</td>\n",
       "      <td>-0.118648</td>\n",
       "      <td>-0.074409</td>\n",
       "      <td>-1.347941</td>\n",
       "      <td>-0.093634</td>\n",
       "      <td>0.129443</td>\n",
       "      <td>0.016785</td>\n",
       "      <td>-0.564776</td>\n",
       "      <td>-0.609300</td>\n",
       "      <td>1.213857</td>\n",
       "      <td>1.223079</td>\n",
       "      <td>0.291247</td>\n",
       "      <td>-0.935328</td>\n",
       "      <td>-0.944805</td>\n",
       "      <td>-1.011703</td>\n",
       "      <td>1.337626</td>\n",
       "      <td>-1.781166</td>\n",
       "      <td>0.112810</td>\n",
       "      <td>0.577467</td>\n",
       "      <td>0.084168</td>\n",
       "      <td>-0.549838</td>\n",
       "      <td>-0.680186</td>\n",
       "      <td>0.104949</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>-0.729775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536311</td>\n",
       "      <td>0.060453</td>\n",
       "      <td>-0.675370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>-0.962047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.498592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666975</td>\n",
       "      <td>1.002768</td>\n",
       "      <td>0.641681</td>\n",
       "      <td>-1.459963</td>\n",
       "      <td>-1.037559</td>\n",
       "      <td>0.254735</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.518211</td>\n",
       "      <td>-1.090239</td>\n",
       "      <td>-0.855211</td>\n",
       "      <td>-1.037967</td>\n",
       "      <td>-1.535513</td>\n",
       "      <td>2.303040</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.339637</td>\n",
       "      <td>-1.126450</td>\n",
       "      <td>-0.192852</td>\n",
       "      <td>-0.015075</td>\n",
       "      <td>-2.046508</td>\n",
       "      <td>-0.715436</td>\n",
       "      <td>-0.119527</td>\n",
       "      <td>-0.059324</td>\n",
       "      <td>-0.753517</td>\n",
       "      <td>-0.932607</td>\n",
       "      <td>3.165744</td>\n",
       "      <td>3.962160</td>\n",
       "      <td>2.324760</td>\n",
       "      <td>-0.967875</td>\n",
       "      <td>-0.950316</td>\n",
       "      <td>-1.015628</td>\n",
       "      <td>1.367073</td>\n",
       "      <td>-2.792842</td>\n",
       "      <td>-0.773439</td>\n",
       "      <td>0.199498</td>\n",
       "      <td>1.051669</td>\n",
       "      <td>-0.088365</td>\n",
       "      <td>-0.209170</td>\n",
       "      <td>0.049905</td>\n",
       "      <td>0.059074</td>\n",
       "      <td>-1.219853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536311</td>\n",
       "      <td>0.060453</td>\n",
       "      <td>-0.675370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       volume  dividend_amount  split_coefficient  reportedEPS  \\\n",
       "60  -0.907754              0.0                0.0    -0.520563   \n",
       "61  -0.669180              0.0                0.0    -0.520563   \n",
       "62  -0.421259              0.0                0.0    -0.483568   \n",
       "63   0.039713              0.0                0.0    -0.483568   \n",
       "64  -0.664286              0.0                0.0    -0.483568   \n",
       "..        ...              ...                ...          ...   \n",
       "307 -0.853963              0.0                0.0    -0.498592   \n",
       "308 -0.917177              0.0                0.0    -0.498592   \n",
       "309 -0.938625              0.0                0.0    -0.498592   \n",
       "310 -0.994110              0.0                0.0    -0.498592   \n",
       "311 -0.962047              0.0                0.0    -0.498592   \n",
       "\n",
       "     surprisePercentage  10Y_yield  2Y_yield  10_2_spread  oil_price  \\\n",
       "60                  0.0   1.815246  1.851192    -0.577047  -1.365162   \n",
       "61                  0.0   1.757585  1.796455    -0.565550  -1.400761   \n",
       "62                  0.0   1.699924  1.693823    -0.473570  -1.403082   \n",
       "63                  0.0   1.807009  1.714350    -0.358596  -1.399600   \n",
       "64                  0.0   1.823483  1.693823    -0.301108  -1.435199   \n",
       "..                  ...        ...       ...          ...        ...   \n",
       "307                 0.0   1.757585  1.078032     0.641681  -1.457254   \n",
       "308                 0.0   1.741110  1.084874     0.607189  -1.463058   \n",
       "309                 0.0   1.732873  1.098558     0.572697  -1.463058   \n",
       "310                 0.0   1.675212  1.023295     0.618687  -1.463058   \n",
       "311                 0.0   1.666975  1.002768     0.641681  -1.459963   \n",
       "\n",
       "       orders  fed_funds  unemployment_rate   inf_exp  non_farm_payroll  \\\n",
       "60  -1.109940   2.341591          -0.434783  0.166667         -0.372520   \n",
       "61  -1.109940   2.229461          -0.434783  0.166667         -0.372520   \n",
       "62  -1.109940   2.030120          -0.434783  0.166667         -0.372520   \n",
       "63  -1.109940   1.905531          -0.434783  0.166667         -0.372520   \n",
       "64  -1.109940   1.849466          -0.434783  0.166667         -0.372520   \n",
       "..        ...        ...                ...       ...               ...   \n",
       "307 -1.037559   0.161293           0.217391 -0.333333         -0.518211   \n",
       "308 -1.037559   0.198670           0.217391 -0.333333         -0.518211   \n",
       "309 -1.037559   0.186211           0.217391 -0.333333         -0.518211   \n",
       "310 -1.037559   0.217358           0.217391 -0.333333         -0.518211   \n",
       "311 -1.037559   0.254735           0.217391 -0.333333         -0.518211   \n",
       "\n",
       "          CPI  retail_sales  gdp_per_capita  gold_price  usd_price    return  \\\n",
       "60  -1.157703     -0.930548       -1.074003   -1.623383   2.370717 -0.043711   \n",
       "61  -1.157703     -0.930548       -1.074003   -1.628149   2.396675 -0.048507   \n",
       "62  -1.157703     -0.930548       -1.074003   -1.627652   2.366081  0.117157   \n",
       "63  -1.157703     -0.930548       -1.074003   -1.619213   2.263176  0.128565   \n",
       "64  -1.157703     -0.930548       -1.074003   -1.620901   2.198281 -0.026439   \n",
       "..        ...           ...             ...         ...        ...       ...   \n",
       "307 -1.090239     -0.855211       -1.037967   -1.546038   2.380915 -0.016922   \n",
       "308 -1.090239     -0.855211       -1.037967   -1.544946   2.393894 -0.018014   \n",
       "309 -1.090239     -0.855211       -1.037967   -1.543456   2.406873 -0.011415   \n",
       "310 -1.090239     -0.855211       -1.037967   -1.542662   2.367936 -0.019794   \n",
       "311 -1.090239     -0.855211       -1.037967   -1.535513   2.303040  0.014724   \n",
       "\n",
       "     high/low  volatility_5days  volatility_10days  volatility_20days  \\\n",
       "60   2.504654          1.390193           2.223102           2.433076   \n",
       "61   2.951707          2.008878           2.390024           2.579241   \n",
       "62   6.185921          2.593196           2.577283           2.617154   \n",
       "63   3.433239          3.961000           3.390815           3.200521   \n",
       "64   0.787062          5.311561           4.079131           3.876676   \n",
       "..        ...               ...                ...                ...   \n",
       "307 -0.256054          0.131298          -0.213271           0.057187   \n",
       "308  0.089389          0.281764          -0.134442           0.002701   \n",
       "309  0.076101          0.392497          -0.057848           0.041606   \n",
       "310  0.397834         -0.308511          -0.118648          -0.074409   \n",
       "311  0.339637         -1.126450          -0.192852          -0.015075   \n",
       "\n",
       "     momentum_5days  momentum_10days  momentum_20days  distance_5days  \\\n",
       "60         1.381883        -0.152540         1.216896        0.664370   \n",
       "61         0.762514        -0.489142         0.808909       -2.590241   \n",
       "62        -0.179567        -0.875811        -0.170702       -2.372619   \n",
       "63         0.722637         1.720708         1.449545        5.350638   \n",
       "64         3.573767         4.106838         2.533896        4.538980   \n",
       "..              ...              ...              ...             ...   \n",
       "307        0.320408         0.107521        -0.052084       -1.544993   \n",
       "308       -0.190050        -0.044160         0.040993       -0.812219   \n",
       "309       -0.509403        -0.199339        -0.066682       -0.712152   \n",
       "310       -1.347941        -0.093634         0.129443        0.016785   \n",
       "311       -2.046508        -0.715436        -0.119527       -0.059324   \n",
       "\n",
       "     distance_10days  distance_20days  volume_5days  volume_10days  \\\n",
       "60          1.299521         0.950000      0.180336       0.629591   \n",
       "61         -1.933475        -2.219975      0.267719       0.695483   \n",
       "62         -2.031668        -2.218872     -1.561741      -1.418738   \n",
       "63          4.807150         4.907594     -1.928345      -1.889394   \n",
       "64          4.570802         5.182002     -2.055787      -2.236214   \n",
       "..               ...              ...           ...            ...   \n",
       "307        -1.342076        -1.264671     -0.197207      -0.725304   \n",
       "308        -0.835662        -0.838544      0.136353      -0.478645   \n",
       "309        -0.836499        -0.863944      1.517418       0.590862   \n",
       "310        -0.564776        -0.609300      1.213857       1.223079   \n",
       "311        -0.753517        -0.932607      3.165744       3.962160   \n",
       "\n",
       "     volume_20days  volume_momentum_5days  volume_momentum_10days  \\\n",
       "60        1.367242              -0.934614               -0.919944   \n",
       "61        1.488856              -0.939661               -0.927448   \n",
       "62       -1.036291              -0.886397               -0.905005   \n",
       "63       -1.685050              -0.804525               -0.857276   \n",
       "64       -2.099150              -0.608636               -0.779819   \n",
       "..             ...                    ...                     ...   \n",
       "307      -0.997661              -0.890046               -0.954382   \n",
       "308      -0.945912              -0.883027               -0.948055   \n",
       "309      -0.126360              -0.886837               -0.947539   \n",
       "310       0.291247              -0.935328               -0.944805   \n",
       "311       2.324760              -0.967875               -0.950316   \n",
       "\n",
       "     volume_momentum_20days  price/eps  momentum_5days/eps  \\\n",
       "60                -0.876576  -1.034180           -0.496821   \n",
       "61                -0.882765  -1.016139           -0.311877   \n",
       "62                -0.871757  -0.278768           -0.050284   \n",
       "63                -0.851640  -0.229465            0.211034   \n",
       "64                -0.801408  -0.240907            1.036843   \n",
       "..                      ...        ...                 ...   \n",
       "307               -1.004913   1.439429            0.634962   \n",
       "308               -1.012969   1.401568           -0.104291   \n",
       "309               -1.010721   1.378011           -0.566782   \n",
       "310               -1.011703   1.337626           -1.781166   \n",
       "311               -1.015628   1.367073           -2.792842   \n",
       "\n",
       "     momentum_10days/eps  momentum_20days/eps  gold_return  10Y_return  \\\n",
       "60             -0.063924            -0.536806     1.111833    0.839389   \n",
       "61              0.034995            -0.409100    -0.874803   -0.539618   \n",
       "62             -0.238856            -0.026941     0.051131   -0.546731   \n",
       "63              0.501303             0.465008     1.458973    0.999724   \n",
       "64              1.181489             0.794245    -0.331340    0.141322   \n",
       "..                   ...                  ...          ...         ...   \n",
       "307             0.399515             0.301886    -0.430579   -0.162923   \n",
       "308             0.183324             0.443188     0.130304   -0.163509   \n",
       "309            -0.037850             0.279723     0.190708   -0.087161   \n",
       "310             0.112810             0.577467     0.084168   -0.549838   \n",
       "311            -0.773439             0.199498     1.051669   -0.088365   \n",
       "\n",
       "     2Y_return  spread_return  oil_return  usd_return  unemployement_return  \\\n",
       "60    0.578407      -0.121821    0.040901    1.118750                   0.0   \n",
       "61   -0.384442       0.058242   -0.605031    0.486041                   0.0   \n",
       "62   -0.711825       0.479318   -0.039570   -0.574211                   0.0   \n",
       "63    0.118059       0.545059    0.062678   -1.933919                   0.0   \n",
       "64   -0.165643       0.243187   -0.624358   -1.231811                   0.0   \n",
       "..         ...            ...         ...         ...                   ...   \n",
       "307  -0.143896      -0.002783    0.419775   -0.174985                   0.0   \n",
       "308   0.035870      -0.080962   -0.106480    0.242159                   0.0   \n",
       "309   0.095672      -0.082250    0.001276    0.241866                   0.0   \n",
       "310  -0.680186       0.104949    0.001276   -0.729775                   0.0   \n",
       "311  -0.209170       0.049905    0.059074   -1.219853                   0.0   \n",
       "\n",
       "     cpi_return  non_farm_payroll_return  gdp_return  \n",
       "60     0.257408                -0.078999   -0.927652  \n",
       "61     0.257408                -0.078999   -0.927652  \n",
       "62     0.257408                -0.078999   -0.927652  \n",
       "63     0.257408                -0.078999   -0.927652  \n",
       "64     0.257408                -0.078999   -0.927652  \n",
       "..          ...                      ...         ...  \n",
       "307    0.536311                 0.060453   -0.675370  \n",
       "308    0.536311                 0.060453   -0.675370  \n",
       "309    0.536311                 0.060453   -0.675370  \n",
       "310    0.536311                 0.060453   -0.675370  \n",
       "311    0.536311                 0.060453   -0.675370  \n",
       "\n",
       "[252 rows x 50 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb027978",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12c303f9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_test_ratio = 0.7\n",
    "input_length = 10\n",
    "horizon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1ef6364f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fold_train, fold_test = train_test_split(folds_test, train_test_ratio, input_length, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4892dd0a",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>dividend_amount</th>\n",
       "      <th>split_coefficient</th>\n",
       "      <th>reportedEPS</th>\n",
       "      <th>surprisePercentage</th>\n",
       "      <th>10Y_yield</th>\n",
       "      <th>2Y_yield</th>\n",
       "      <th>10_2_spread</th>\n",
       "      <th>oil_price</th>\n",
       "      <th>orders</th>\n",
       "      <th>...</th>\n",
       "      <th>gold_return</th>\n",
       "      <th>10Y_return</th>\n",
       "      <th>2Y_return</th>\n",
       "      <th>spread_return</th>\n",
       "      <th>oil_return</th>\n",
       "      <th>usd_return</th>\n",
       "      <th>unemployement_return</th>\n",
       "      <th>cpi_return</th>\n",
       "      <th>non_farm_payroll_return</th>\n",
       "      <th>gdp_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.907754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.520563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.815246</td>\n",
       "      <td>1.851192</td>\n",
       "      <td>-0.577047</td>\n",
       "      <td>-1.365162</td>\n",
       "      <td>-1.109940</td>\n",
       "      <td>...</td>\n",
       "      <td>1.111833</td>\n",
       "      <td>0.839389</td>\n",
       "      <td>0.578407</td>\n",
       "      <td>-0.121821</td>\n",
       "      <td>0.040901</td>\n",
       "      <td>1.118750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257408</td>\n",
       "      <td>-0.078999</td>\n",
       "      <td>-0.927652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.669180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.520563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.757585</td>\n",
       "      <td>1.796455</td>\n",
       "      <td>-0.565550</td>\n",
       "      <td>-1.400761</td>\n",
       "      <td>-1.109940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.874803</td>\n",
       "      <td>-0.539618</td>\n",
       "      <td>-0.384442</td>\n",
       "      <td>0.058242</td>\n",
       "      <td>-0.605031</td>\n",
       "      <td>0.486041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257408</td>\n",
       "      <td>-0.078999</td>\n",
       "      <td>-0.927652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-0.421259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.483568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.699924</td>\n",
       "      <td>1.693823</td>\n",
       "      <td>-0.473570</td>\n",
       "      <td>-1.403082</td>\n",
       "      <td>-1.109940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051131</td>\n",
       "      <td>-0.546731</td>\n",
       "      <td>-0.711825</td>\n",
       "      <td>0.479318</td>\n",
       "      <td>-0.039570</td>\n",
       "      <td>-0.574211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257408</td>\n",
       "      <td>-0.078999</td>\n",
       "      <td>-0.927652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.039713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.483568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.807009</td>\n",
       "      <td>1.714350</td>\n",
       "      <td>-0.358596</td>\n",
       "      <td>-1.399600</td>\n",
       "      <td>-1.109940</td>\n",
       "      <td>...</td>\n",
       "      <td>1.458973</td>\n",
       "      <td>0.999724</td>\n",
       "      <td>0.118059</td>\n",
       "      <td>0.545059</td>\n",
       "      <td>0.062678</td>\n",
       "      <td>-1.933919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257408</td>\n",
       "      <td>-0.078999</td>\n",
       "      <td>-0.927652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-0.664286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.483568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.823483</td>\n",
       "      <td>1.693823</td>\n",
       "      <td>-0.301108</td>\n",
       "      <td>-1.435199</td>\n",
       "      <td>-1.109940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.331340</td>\n",
       "      <td>0.141322</td>\n",
       "      <td>-0.165643</td>\n",
       "      <td>0.243187</td>\n",
       "      <td>-0.624358</td>\n",
       "      <td>-1.231811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257408</td>\n",
       "      <td>-0.078999</td>\n",
       "      <td>-0.927652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>-0.899464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.495962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.708161</td>\n",
       "      <td>0.941189</td>\n",
       "      <td>0.802646</td>\n",
       "      <td>-1.689806</td>\n",
       "      <td>-1.038305</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202894</td>\n",
       "      <td>0.145458</td>\n",
       "      <td>-0.214386</td>\n",
       "      <td>0.121470</td>\n",
       "      <td>-0.584453</td>\n",
       "      <td>-0.534399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.105818</td>\n",
       "      <td>-0.651034</td>\n",
       "      <td>-1.464643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>-0.995700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.495962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.642263</td>\n",
       "      <td>0.879610</td>\n",
       "      <td>0.814143</td>\n",
       "      <td>-1.707605</td>\n",
       "      <td>-1.038305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761029</td>\n",
       "      <td>-0.630519</td>\n",
       "      <td>-0.599992</td>\n",
       "      <td>0.021443</td>\n",
       "      <td>-0.426434</td>\n",
       "      <td>-0.880686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.105818</td>\n",
       "      <td>-0.651034</td>\n",
       "      <td>-1.464643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>-0.762052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.495962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.749348</td>\n",
       "      <td>0.982242</td>\n",
       "      <td>0.791148</td>\n",
       "      <td>-1.662333</td>\n",
       "      <td>-1.436121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069912</td>\n",
       "      <td>1.013668</td>\n",
       "      <td>0.963342</td>\n",
       "      <td>-0.050993</td>\n",
       "      <td>1.114221</td>\n",
       "      <td>-1.663871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.717646</td>\n",
       "      <td>-2.742891</td>\n",
       "      <td>-1.055014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>-0.712837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.495962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.716398</td>\n",
       "      <td>0.961716</td>\n",
       "      <td>0.779651</td>\n",
       "      <td>-1.680906</td>\n",
       "      <td>-1.436121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168847</td>\n",
       "      <td>-0.317388</td>\n",
       "      <td>-0.212615</td>\n",
       "      <td>-0.027132</td>\n",
       "      <td>-0.430034</td>\n",
       "      <td>0.557534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.717646</td>\n",
       "      <td>-2.742891</td>\n",
       "      <td>-1.055014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>-0.833344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.495962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.732873</td>\n",
       "      <td>0.961716</td>\n",
       "      <td>0.802646</td>\n",
       "      <td>-1.649177</td>\n",
       "      <td>-1.436121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128161</td>\n",
       "      <td>0.144553</td>\n",
       "      <td>-0.024290</td>\n",
       "      <td>0.046161</td>\n",
       "      <td>0.755225</td>\n",
       "      <td>0.381854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.717646</td>\n",
       "      <td>-2.742891</td>\n",
       "      <td>-1.055014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       volume  dividend_amount  split_coefficient  reportedEPS  \\\n",
       "60  -0.907754              0.0                0.0    -0.520563   \n",
       "61  -0.669180              0.0                0.0    -0.520563   \n",
       "62  -0.421259              0.0                0.0    -0.483568   \n",
       "63   0.039713              0.0                0.0    -0.483568   \n",
       "64  -0.664286              0.0                0.0    -0.483568   \n",
       "..        ...              ...                ...          ...   \n",
       "231 -0.899464              0.0                0.0    -0.495962   \n",
       "232 -0.995700              0.0                0.0    -0.495962   \n",
       "233 -0.762052              0.0                0.0    -0.495962   \n",
       "234 -0.712837              0.0                0.0    -0.495962   \n",
       "235 -0.833344              0.0                0.0    -0.495962   \n",
       "\n",
       "     surprisePercentage  10Y_yield  2Y_yield  10_2_spread  oil_price  \\\n",
       "60                  0.0   1.815246  1.851192    -0.577047  -1.365162   \n",
       "61                  0.0   1.757585  1.796455    -0.565550  -1.400761   \n",
       "62                  0.0   1.699924  1.693823    -0.473570  -1.403082   \n",
       "63                  0.0   1.807009  1.714350    -0.358596  -1.399600   \n",
       "64                  0.0   1.823483  1.693823    -0.301108  -1.435199   \n",
       "..                  ...        ...       ...          ...        ...   \n",
       "231                 0.0   1.708161  0.941189     0.802646  -1.689806   \n",
       "232                 0.0   1.642263  0.879610     0.814143  -1.707605   \n",
       "233                 0.0   1.749348  0.982242     0.791148  -1.662333   \n",
       "234                 0.0   1.716398  0.961716     0.779651  -1.680906   \n",
       "235                 0.0   1.732873  0.961716     0.802646  -1.649177   \n",
       "\n",
       "       orders  ...  gold_return  10Y_return  2Y_return  spread_return  \\\n",
       "60  -1.109940  ...     1.111833    0.839389   0.578407      -0.121821   \n",
       "61  -1.109940  ...    -0.874803   -0.539618  -0.384442       0.058242   \n",
       "62  -1.109940  ...     0.051131   -0.546731  -0.711825       0.479318   \n",
       "63  -1.109940  ...     1.458973    0.999724   0.118059       0.545059   \n",
       "64  -1.109940  ...    -0.331340    0.141322  -0.165643       0.243187   \n",
       "..        ...  ...          ...         ...        ...            ...   \n",
       "231 -1.038305  ...    -0.202894    0.145458  -0.214386       0.121470   \n",
       "232 -1.038305  ...     0.761029   -0.630519  -0.599992       0.021443   \n",
       "233 -1.436121  ...    -0.069912    1.013668   0.963342      -0.050993   \n",
       "234 -1.436121  ...    -0.168847   -0.317388  -0.212615      -0.027132   \n",
       "235 -1.436121  ...     0.128161    0.144553  -0.024290       0.046161   \n",
       "\n",
       "     oil_return  usd_return  unemployement_return  cpi_return  \\\n",
       "60     0.040901    1.118750                   0.0    0.257408   \n",
       "61    -0.605031    0.486041                   0.0    0.257408   \n",
       "62    -0.039570   -0.574211                   0.0    0.257408   \n",
       "63     0.062678   -1.933919                   0.0    0.257408   \n",
       "64    -0.624358   -1.231811                   0.0    0.257408   \n",
       "..          ...         ...                   ...         ...   \n",
       "231   -0.584453   -0.534399                   0.0   -1.105818   \n",
       "232   -0.426434   -0.880686                   0.0   -1.105818   \n",
       "233    1.114221   -1.663871                   0.0   -0.717646   \n",
       "234   -0.430034    0.557534                   0.0   -0.717646   \n",
       "235    0.755225    0.381854                   0.0   -0.717646   \n",
       "\n",
       "     non_farm_payroll_return  gdp_return  \n",
       "60                 -0.078999   -0.927652  \n",
       "61                 -0.078999   -0.927652  \n",
       "62                 -0.078999   -0.927652  \n",
       "63                 -0.078999   -0.927652  \n",
       "64                 -0.078999   -0.927652  \n",
       "..                       ...         ...  \n",
       "231                -0.651034   -1.464643  \n",
       "232                -0.651034   -1.464643  \n",
       "233                -2.742891   -1.055014  \n",
       "234                -2.742891   -1.055014  \n",
       "235                -2.742891   -1.055014  \n",
       "\n",
       "[176 rows x 50 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d5a67d8",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>dividend_amount</th>\n",
       "      <th>split_coefficient</th>\n",
       "      <th>reportedEPS</th>\n",
       "      <th>surprisePercentage</th>\n",
       "      <th>10Y_yield</th>\n",
       "      <th>2Y_yield</th>\n",
       "      <th>10_2_spread</th>\n",
       "      <th>oil_price</th>\n",
       "      <th>...</th>\n",
       "      <th>gold_return</th>\n",
       "      <th>10Y_return</th>\n",
       "      <th>2Y_return</th>\n",
       "      <th>spread_return</th>\n",
       "      <th>oil_return</th>\n",
       "      <th>usd_return</th>\n",
       "      <th>unemployement_return</th>\n",
       "      <th>cpi_return</th>\n",
       "      <th>non_farm_payroll_return</th>\n",
       "      <th>gdp_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>20.58</td>\n",
       "      <td>5177800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0311</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>19.37</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015709</td>\n",
       "      <td>-0.015504</td>\n",
       "      <td>-0.006390</td>\n",
       "      <td>-0.029557</td>\n",
       "      <td>-0.000516</td>\n",
       "      <td>0.002866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003946</td>\n",
       "      <td>-0.002891</td>\n",
       "      <td>-0.008060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>21.40</td>\n",
       "      <td>3944000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>18.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012862</td>\n",
       "      <td>-0.020305</td>\n",
       "      <td>-0.036138</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003946</td>\n",
       "      <td>-0.002891</td>\n",
       "      <td>-0.008060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>21.01</td>\n",
       "      <td>4577400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0512</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>19.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007249</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>0.006349</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.033744</td>\n",
       "      <td>0.012025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003946</td>\n",
       "      <td>-0.002891</td>\n",
       "      <td>-0.008060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>21.35</td>\n",
       "      <td>2614300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>21.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.028391</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.104663</td>\n",
       "      <td>0.007437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003946</td>\n",
       "      <td>-0.002891</td>\n",
       "      <td>-0.008060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>21.58</td>\n",
       "      <td>3419800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0513</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>21.07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007885</td>\n",
       "      <td>-0.017241</td>\n",
       "      <td>-0.021472</td>\n",
       "      <td>-0.010204</td>\n",
       "      <td>-0.011726</td>\n",
       "      <td>-0.002376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003946</td>\n",
       "      <td>-0.002891</td>\n",
       "      <td>-0.008060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>25.49</td>\n",
       "      <td>6703700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0521</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>26.43</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004285</td>\n",
       "      <td>-0.003824</td>\n",
       "      <td>-0.005917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022042</td>\n",
       "      <td>-0.000859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005593</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>0.003598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>24.84</td>\n",
       "      <td>4811200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>26.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>-0.003839</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>-0.016216</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005593</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>0.003598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>24.54</td>\n",
       "      <td>4169100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>26.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>-0.001927</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>-0.016484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005593</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>0.003598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>24.30</td>\n",
       "      <td>2508000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0511</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>26.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>-0.013514</td>\n",
       "      <td>-0.032448</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005593</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>0.003598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>23.56</td>\n",
       "      <td>3467900.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>26.36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011850</td>\n",
       "      <td>-0.001957</td>\n",
       "      <td>-0.009146</td>\n",
       "      <td>0.010929</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>-0.006027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005593</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>0.003598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      open     volume  dividend_amount  split_coefficient  reportedEPS  \\\n",
       "226  20.58  5177800.0              0.0                1.0       0.0034   \n",
       "227  21.40  3944000.0              0.0                1.0       0.0034   \n",
       "228  21.01  4577400.0              0.0                1.0       0.0034   \n",
       "229  21.35  2614300.0              0.0                1.0       0.0034   \n",
       "230  21.58  3419800.0              0.0                1.0       0.0034   \n",
       "..     ...        ...              ...                ...          ...   \n",
       "307  25.49  6703700.0              0.0                1.0       0.0020   \n",
       "308  24.84  4811200.0              0.0                1.0       0.0020   \n",
       "309  24.54  4169100.0              0.0                1.0       0.0020   \n",
       "310  24.30  2508000.0              0.0                1.0       0.0020   \n",
       "311  23.56  3467900.0              0.0                1.0       0.0020   \n",
       "\n",
       "     surprisePercentage  10Y_yield  2Y_yield  10_2_spread  oil_price  ...  \\\n",
       "226                 0.0     0.0508    0.0311       0.0197      19.37  ...   \n",
       "227                 0.0     0.0508    0.0315       0.0193      18.67  ...   \n",
       "228                 0.0     0.0512    0.0317       0.0195      19.30  ...   \n",
       "229                 0.0     0.0522    0.0326       0.0196      21.32  ...   \n",
       "230                 0.0     0.0513    0.0319       0.0194      21.07  ...   \n",
       "..                  ...        ...       ...          ...        ...  ...   \n",
       "307                 0.0     0.0521    0.0336       0.0185      26.43  ...   \n",
       "308                 0.0     0.0519    0.0337       0.0182      26.28  ...   \n",
       "309                 0.0     0.0518    0.0339       0.0179      26.28  ...   \n",
       "310                 0.0     0.0511    0.0328       0.0183      26.28  ...   \n",
       "311                 0.0     0.0510    0.0325       0.0185      26.36  ...   \n",
       "\n",
       "     gold_return  10Y_return  2Y_return  spread_return  oil_return  \\\n",
       "226    -0.015709   -0.015504  -0.006390      -0.029557   -0.000516   \n",
       "227     0.000725    0.000000   0.012862      -0.020305   -0.036138   \n",
       "228     0.007249    0.007874   0.006349       0.010363    0.033744   \n",
       "229     0.003958    0.019531   0.028391       0.005128    0.104663   \n",
       "230    -0.007885   -0.017241  -0.021472      -0.010204   -0.011726   \n",
       "..           ...         ...        ...            ...         ...   \n",
       "307    -0.004285   -0.003824  -0.005917       0.000000    0.022042   \n",
       "308     0.001821   -0.003839   0.002976      -0.016216   -0.005675   \n",
       "309     0.002478   -0.001927   0.005935      -0.016484    0.000000   \n",
       "310     0.001318   -0.013514  -0.032448       0.022346    0.000000   \n",
       "311     0.011850   -0.001957  -0.009146       0.010929    0.003044   \n",
       "\n",
       "     usd_return  unemployement_return  cpi_return  non_farm_payroll_return  \\\n",
       "226    0.002866                   0.0   -0.003946                -0.002891   \n",
       "227    0.000952                   0.0   -0.003946                -0.002891   \n",
       "228    0.012025                   0.0   -0.003946                -0.002891   \n",
       "229    0.007437                   0.0   -0.003946                -0.002891   \n",
       "230   -0.002376                   0.0   -0.003946                -0.002891   \n",
       "..          ...                   ...         ...                      ...   \n",
       "307   -0.000859                   0.0    0.005593                 0.004532   \n",
       "308    0.001204                   0.0    0.005593                 0.004532   \n",
       "309    0.001202                   0.0    0.005593                 0.004532   \n",
       "310   -0.003603                   0.0    0.005593                 0.004532   \n",
       "311   -0.006027                   0.0    0.005593                 0.004532   \n",
       "\n",
       "     gdp_return  \n",
       "226   -0.008060  \n",
       "227   -0.008060  \n",
       "228   -0.008060  \n",
       "229   -0.008060  \n",
       "230   -0.008060  \n",
       "..          ...  \n",
       "307    0.003598  \n",
       "308    0.003598  \n",
       "309    0.003598  \n",
       "310    0.003598  \n",
       "311    0.003598  \n",
       "\n",
       "[86 rows x 51 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3a743a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d76d0715",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output_length = 1\n",
    "stride = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c09baa0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = get_X_y(fold=fold_train,\n",
    "                           horizon=horizon,\n",
    "                           input_length=input_length,\n",
    "                           output_length=output_length,\n",
    "                           stride=stride)\n",
    "X_test, y_test = get_X_y(fold=fold_test,\n",
    "                         horizon=horizon,\n",
    "                         input_length=input_length,\n",
    "                         output_length=output_length,\n",
    "                         stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6cb6d3c8",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes for the training set:\n",
      "X_train.shape = (176, 10, 51), y_train.shape = (176,)\n",
      "Shapes for the test set:\n",
      "X_test.shape = (86, 10, 51), y_test.shape = (86,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes for the training set:\")\n",
    "print(f\"X_train.shape = {X_train.shape}, y_train.shape = {y_train.shape}\")\n",
    "\n",
    "print(\"Shapes for the test set:\")\n",
    "print(f\"X_test.shape = {X_test.shape}, y_test.shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6597dba4",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2c372acb",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03987408,  0.02795699, -0.0081591 ,  0.07771788,  0.02376238,\n",
       "        0.00856898, -0.01548318,  0.00318878, -0.02401302,  0.04577241,\n",
       "       -0.0235    ,  0.02011494, -0.00652174,  0.03041307,  0.06622517,\n",
       "        0.04038138, -0.01569714,  0.03165064,  0.06392694,  0.02621919,\n",
       "       -0.0787309 ,  0.00127497, -0.04474394, -0.04178929, -0.0053816 ,\n",
       "       -0.015625  ,  0.01521984, -0.00107585,  0.07330362, -0.02467866,\n",
       "       -0.01614531,  0.0446527 , -0.01101005, -0.0015748 , -0.00254669,\n",
       "       -0.03992304, -0.01002865, -0.01939954,  0.01128004,  0.00368034,\n",
       "       -0.05570434, -0.03097826, -0.04124579,  0.01618304, -0.02282158,\n",
       "       -0.17171315, -0.0439408 ,  0.02545314,  0.06071429,  0.02816901,\n",
       "       -0.00084926,  0.01891368,  0.00193424,  0.08180943, -0.01266376,\n",
       "        0.        , -0.01896552, -0.03153153,  0.        , -0.05531915,\n",
       "        0.01596517,  0.03934426,  0.00475059,  0.01201717, -0.02904762,\n",
       "       -0.02238095, -0.01726316,  0.        ,  0.05125   , -0.00739567,\n",
       "        0.0163728 ,  0.01925602,  0.04711779, -0.0333474 ,  0.00793651,\n",
       "       -0.00170164,  0.04267266, -0.06130124, -0.05002156,  0.01666667,\n",
       "       -0.0228013 ,  0.01239029,  0.04486252, -0.02509653,  0.01436935,\n",
       "        0.00466497, -0.00535801,  0.05429626, -0.0218768 , -0.01818182,\n",
       "       -0.06003752, -0.00472306,  0.08587398,  0.01138829, -0.01617251,\n",
       "       -0.00465116, -0.00251046, -0.02362907,  0.001002  , -0.0272651 ,\n",
       "       -0.03067961, -0.02709745, -0.02882267, -0.01885804, -0.04245909,\n",
       "        0.00276702, -0.02196595,  0.02903379,  0.00787789,  0.06008011,\n",
       "       -0.03371783,  0.00111173, -0.01173709, -0.00158228,  0.05944673,\n",
       "       -0.01148936, -0.02302968, -0.04239631,  0.03527525,  0.01631174,\n",
       "       -0.01331497, -0.0248307 ,  0.06413302, -0.02748414, -0.01231946,\n",
       "        0.00649702,  0.        ,  0.05865604,  0.01726167,  0.03968939,\n",
       "        0.00627287, -0.06872247,  0.01266491,  0.02333333, -0.00847857,\n",
       "       -0.00688637,  0.01990317,  0.00102197,  0.04545455, -0.0310992 ,\n",
       "        0.0188476 ,  0.03784033,  0.043095  ,  0.01355276, -0.01022677,\n",
       "       -0.03581489,  0.0475793 , -0.05667838,  0.00520833,  0.00768049,\n",
       "       -0.04086739,  0.01347305, -0.01897436, -0.04626709, -0.00227273,\n",
       "        0.02600473, -0.01000417,  0.08074534,  0.02692515, -0.00111049,\n",
       "       -0.02709924, -0.0086741 , -0.01379004, -0.04492088, -0.01103753,\n",
       "       -0.02202202,  0.0269893 , -0.0566352 , -0.01053556, -0.07871449,\n",
       "       -0.03247774, -0.00129143,  0.01643836,  0.00859454, -0.01731399,\n",
       "        0.05469679])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b717eb8b",
   "metadata": {},
   "source": [
    "# Cross Val DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "72bcd6c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:01:27.515298Z",
     "start_time": "2022-12-02T11:01:27.506675Z"
    }
   },
   "outputs": [],
   "source": [
    "fold_length = 252 #1 an\n",
    "fold_stride = 60 #1 trimestre\n",
    "train_test_ratio = 0.7\n",
    "input_length = 10\n",
    "horizon = 1\n",
    "output_length = 1\n",
    "stride = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5fc4c80e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:03:28.327762Z",
     "start_time": "2022-12-02T11:03:28.316738Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers, metrics\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "def init_model(X_train, y_train):\n",
    "\n",
    "    # 0 - Normalization\n",
    "    # ======================    \n",
    "    normalizer = Normalization()\n",
    "    normalizer.adapt(X_train)\n",
    "    \n",
    "    # 1 - RNN architecture\n",
    "    # ======================    \n",
    "    model = models.Sequential()    \n",
    "    # Normalizing Inputs\n",
    "    model.add(normalizer)\n",
    "    # Recurrent Layer\n",
    "    model.add(layers.LSTM(units=64, activation='tanh', return_sequences=False, \n",
    "                          recurrent_dropout=0.5, dropout=0.5))\n",
    "    # Hidden Dense Layer that we are regularizing\n",
    "    reg_l2 = regularizers.L2(0.5)\n",
    "    model.add(layers.Dense(32, activation=\"relu\", kernel_regularizer = reg_l2))\n",
    "    model.add(layers.Dropout(rate=0.5))\n",
    "    \n",
    "    # Predictive Dense Layer\n",
    "    ### model.add(layers.Dense(1, activation='linear'))\n",
    "    ### QUESTION: HOW DO YOU CHANGE THIS PART FOR MULTIPLE STEPS ?    \n",
    "    # $CHALLENGIFY_BEGIN    \n",
    "    output_length = 1 #y_train.shape[1]\n",
    "    model.add(layers.Dense(output_length, activation='linear'))\n",
    "    # $CHALLENGIFY_END\n",
    "    \n",
    "    # 2 - Compiler\n",
    "    # ======================\n",
    "    initial_learning_rate = 0.01\n",
    "\n",
    "    lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=1000, decay_rate=0.5)\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=lr_schedule)\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=[\"mae\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3b8440bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:03:29.778343Z",
     "start_time": "2022-12-02T11:03:29.771645Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def fit_model(model: tf.keras.Model, verbose=1):\n",
    "\n",
    "    es = EarlyStopping(monitor=\"val_loss\",\n",
    "                       patience=10,\n",
    "                       mode=\"min\",\n",
    "                       restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        validation_split=0.3,\n",
    "                        shuffle=False,\n",
    "                        batch_size=32,\n",
    "                        epochs=200,\n",
    "                        callbacks=[es],\n",
    "                        verbose=verbose)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "615cd631",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:03:30.745937Z",
     "start_time": "2022-12-02T11:03:30.736332Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "    # --- LOSS: MSE --- \n",
    "    ax[0].plot(history.history['loss'])\n",
    "    ax[0].plot(history.history['val_loss'])\n",
    "    ax[0].set_title('MSE')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].legend(['Train', 'Validation'], loc='best')\n",
    "    ax[0].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[0].grid(axis=\"y\",linewidth=0.5)\n",
    "    \n",
    "    # --- METRICS:MAE ---\n",
    "    \n",
    "    ax[1].plot(history.history['mae'])\n",
    "    ax[1].plot(history.history['val_mae'])\n",
    "    ax[1].set_title('MAE')\n",
    "    ax[1].set_ylabel('MAE')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].legend(['Train', 'Validation'], loc='best')\n",
    "    ax[1].grid(axis=\"x\",linewidth=0.5)\n",
    "    ax[1].grid(axis=\"y\",linewidth=0.5)\n",
    "                        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "66452593",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:04:30.141922Z",
     "start_time": "2022-12-02T11:03:31.466826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE LSTM fold n°0 = 0.03\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x135395c60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "MAE LSTM fold n°1 = 0.02\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x1361bc9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "MAE LSTM fold n°2 = 0.03\n",
      "MAE LSTM fold n°3 = 0.02\n",
      "MAE LSTM fold n°4 = 0.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m model \u001b[38;5;241m=\u001b[39m init_model(X_train, y_train)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[1;32m     50\u001b[0m res \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[67], line 11\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(model, verbose)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_model\u001b[39m(model: tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      6\u001b[0m     es \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m                        patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      8\u001b[0m                        mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m                        restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/keras/engine/training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1593\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1594\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1605\u001b[0m     )\n\u001b[0;32m-> 1606\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1621\u001b[0m }\n\u001b[1;32m   1622\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_of_mae_baseline_model = []\n",
    "list_of_mae_recurrent_model = []\n",
    "    \n",
    "# 1 - Creating FOLDS\n",
    "# =======================================================\n",
    "    \n",
    "folds = get_folds(test, fold_length, fold_stride)\n",
    "    \n",
    "for fold_id, fold in enumerate(folds):\n",
    "\n",
    "    # 2 - CHRONOLOGICAL TRAIN TEST SPLIT of the current FOLD\n",
    "\n",
    "    (fold_train, fold_test) = train_test_split(fold = fold,\n",
    "                                            train_test_ratio = train_test_ratio,\n",
    "                                            input_length = input_length,\n",
    "                                            horizon = horizon)\n",
    "    \n",
    "    # 3 - Scanninng fold_train and fold_test for SEQUENCES \n",
    "    # =======================================================       \n",
    "    \n",
    "    X_train, y_train = get_X_y(fold = fold_train, \n",
    "                               horizon = horizon, \n",
    "                               input_length = input_length, \n",
    "                               output_length = output_length, \n",
    "                               stride = stride)\n",
    "    \n",
    "    X_test, y_test = get_X_y(fold_test, \n",
    "                             horizon = horizon, \n",
    "                             input_length = input_length, \n",
    "                             output_length = output_length,\n",
    "                             stride = stride)\n",
    "    \n",
    "    # 4.1 - Baseline Model\n",
    "    # =======================================================\n",
    "#     mae_baseline = last_seen_value_baseline(X_test, y_test)\n",
    "#     list_of_mae_baseline_model.append(mae_baseline)\n",
    "    \n",
    "#     print(\"-\"*50)\n",
    "#     print(f\"MAE baseline fold n°{fold_id} = {round(mae_baseline, 2)}\")        \n",
    "    \n",
    "    \n",
    "    # 4.2 - LSTM Model\n",
    "    # =======================================================\n",
    "    \n",
    "    # Initializing the LSTM Model\n",
    "    model = init_model(X_train, y_train)\n",
    "    # Training\n",
    "    model, history = fit_model(model, verbose=0)\n",
    "    # Evaluation\n",
    "    res = model.evaluate(X_test, y_test, verbose = 0)\n",
    "    mae_lstm = res[1]\n",
    "    \n",
    "    list_of_mae_recurrent_model.append(mae_lstm)\n",
    "    \n",
    "    print(f\"MAE LSTM fold n°{fold_id} = {round(mae_lstm, 2)}\")\n",
    "    \n",
    "#    # 4.3 - Comparison LSTM vs Baseline for the current fold\n",
    "#    # =======================================================\n",
    "#    print(f\"🏋🏽‍♂️ Improvement/Decrease vs. Baseline: {round((1 - (mae_lstm/mae_baseline))*100,2)} % \\n\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "598e6bf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:05:10.659526Z",
     "start_time": "2022-12-02T11:04:55.470121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization_10 (Normaliza  (None, None, 50)         101       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 64)                29440     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,654\n",
      "Trainable params: 31,553\n",
      "Non-trainable params: 101\n",
      "_________________________________________________________________\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 3.7380e-04 - mae: 0.0139\n",
      "CPU times: user 19.7 s, sys: 2.19 s, total: 21.9 s\n",
      "Wall time: 14.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0003738042723853141, 0.013859637081623077]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# 1 - Initialising the RNN model\n",
    "# ==================================== #\n",
    "model = init_model(X_train, y_train)\n",
    "model.summary()\n",
    "\n",
    "# 2 - Training\n",
    "# ====================================\n",
    "model, history = fit_model(model, verbose=0)\n",
    "plot_history(history);\n",
    "\n",
    "# 3 - Evaluation\n",
    "# ====================================\n",
    "res = model.evaluate(X_test, y_test)\n",
    "#print(\"-\"*50)\n",
    "#print(f\"The LSTM MAE on the test set is equal to {round(res[1],2)} Celsius degrees\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327998cb",
   "metadata": {},
   "source": [
    "# Cross Val ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f133c49",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601ea0ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T09:01:00.078179Z",
     "start_time": "2022-12-02T09:01:00.070635Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fold_length = 252 #1 an\n",
    "fold_stride = 60 #1 trimestre\n",
    "train_test_ratio = 0.7\n",
    "input_length = 10\n",
    "horizon = 1\n",
    "output_length = 1\n",
    "stride = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d8351a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Façon TimeserieSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa9e92ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T09:01:01.377927Z",
     "start_time": "2022-12-02T09:01:01.106298Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GradientBoostingRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     X_train, X_test \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[train_index], X\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[1;32m     12\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_index], y\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[0;32m---> 14\u001b[0m     model_gradboostr \u001b[38;5;241m=\u001b[39m \u001b[43mGradientBoostingRegressor\u001b[49m()\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     15\u001b[0m     score_gradboostr\u001b[38;5;241m.\u001b[39mappend(model_gradboostr\u001b[38;5;241m.\u001b[39mscore(X_test, y_test))\n\u001b[1;32m     17\u001b[0m score_gradboostr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(score_gradboostr)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GradientBoostingRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, HuberRegressor\n",
    "from sklearn.linear_model import Lars, Lasso, RANSACRegressor, Ridge, TheilSenRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, VotingRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "folds = TimeSeriesSplit(n_splits=91)\n",
    "\n",
    "score_gradboostr = []\n",
    "\n",
    "X = test[['volume', 'volatility_5days' , 'momentum_5days', 'distance_5days','return']]\n",
    "y = test['return']\n",
    "\n",
    "for train_index, test_index in folds.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model_gradboostr = GradientBoostingRegressor().fit(X_train, y_train)\n",
    "    score_gradboostr.append(model_gradboostr.score(X_test, y_test))\n",
    "\n",
    "score_gradboostr = np.mean(score_gradboostr)\n",
    "score_gradboostr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5269edc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Façon RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31056fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T09:33:24.473658Z",
     "start_time": "2022-12-02T09:33:24.452823Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, SGDRegressor, HuberRegressor\n",
    "from sklearn.linear_model import Lars, Lasso, RANSACRegressor, Ridge, TheilSenRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, VotingRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def cross_validate_model() :\n",
    "    '''\n",
    "    get_folds() create many FOLDS, train_test_split() create a split on ONE FOLDS.\n",
    "    The goal of this function is to make splits and sequences on each FOLDS.\n",
    "    Then, apply a model.\n",
    "    '''\n",
    "    folds = get_folds(test, fold_length, fold_stride) # 1 - Creating FOLDS\n",
    "    \n",
    "    liste_score = []\n",
    "    \n",
    "    score_lr = [] \n",
    "    score_knn = [] \n",
    "    score_lin_reg_sgd = [] \n",
    "    score_huberr = [] \n",
    "    score_lars = [] \n",
    "    score_lasso = [] \n",
    "    score_ransacr = [] \n",
    "    score_ridge = [] \n",
    "    score_theilsenr = [] \n",
    "    score_svr = []\n",
    "    score_elasticnet = []\n",
    "    score_forestreg = []\n",
    "    score_baggingr = []\n",
    "    score_daboostr = []\n",
    "    score_gradboostr = []\n",
    "#    score_XGBR = []\n",
    "    score_Votingr = []\n",
    "    score_XGBR = []\n",
    "    score_stackingr = []\n",
    "\n",
    "    for fold_id, fold in enumerate(folds):\n",
    "\n",
    "        # 2 - CHRONOLOGICAL TRAIN TEST SPLIT of the current FOLD\n",
    "\n",
    "        (fold_train, fold_test) = train_test_split(fold = fold,\n",
    "                                                train_test_ratio = train_test_ratio,\n",
    "                                                input_length = input_length,\n",
    "                                                horizon = horizon)\n",
    "\n",
    "        # 3 - Scanninng fold_train and fold_test for SEQUENCES\n",
    "\n",
    "        X_train, y_train = fold_train, fold_train['return']\n",
    "\n",
    "        X_test, y_test = fold_test, fold_test['return']\n",
    "        \n",
    "        # 4 - fit model\n",
    "\n",
    "        model_lr = LinearRegression().fit(X_train, y_train)\n",
    "        score_lr.append(model_lr.score(X_test, y_test))\n",
    "\n",
    "        model_knn = KNeighborsRegressor(n_neighbors=20).fit(X_train, y_train)\n",
    "        score_knn.append(model_knn.score(X_test, y_test))\n",
    "\n",
    "        model_lin_reg_sgd = SGDRegressor(loss='squared_error').fit(X_train, y_train)\n",
    "        score_lin_reg_sgd.append(model_lin_reg_sgd.score(X_test, y_test))\n",
    "\n",
    "        model_huberr = HuberRegressor().fit(X_train, y_train)\n",
    "        score_huberr.append(model_huberr.score(X_test, y_test))\n",
    "\n",
    "        model_lars = Lars().fit(X_train, y_train)\n",
    "        score_lars.append(model_lars.score(X_test, y_test))\n",
    "\n",
    "        model_lasso = Lasso().fit(X_train, y_train)\n",
    "        score_lasso.append(model_lasso.score(X_test, y_test))\n",
    "\n",
    "        model_ransacr = RANSACRegressor().fit(X_train, y_train)\n",
    "        score_ransacr.append(model_ransacr.score(X_test, y_test))\n",
    "\n",
    "        model_ridge = Ridge().fit(X_train, y_train)\n",
    "        score_ridge.append(model_ridge.score(X_test, y_test))\n",
    "\n",
    "##        model_theilsenr = TheilSenRegressor().fit(X_train, y_train)\n",
    "##        score_theilsenr.append(model_theilsenr.score(X_test, y_test))\n",
    "\n",
    "        model_svr = SVR().fit(X_train, y_train)\n",
    "        score_svr.append(model_svr.score(X_test, y_test))\n",
    "\n",
    "        model_elasticnet = ElasticNet().fit(X_train, y_train)\n",
    "        score_elasticnet.append(model_elasticnet.score(X_test, y_test))\n",
    "\n",
    "        model_forestreg = RandomForestRegressor().fit(X_train, y_train)\n",
    "        score_forestreg.append(model_forestreg.score(X_test, y_test))\n",
    "\n",
    "        model_baggingr = BaggingRegressor().fit(X_train, y_train)\n",
    "        score_baggingr.append(model_baggingr.score(X_test, y_test))\n",
    "\n",
    "        model_daboostr = AdaBoostRegressor().fit(X_train, y_train)\n",
    "        score_daboostr.append(model_daboostr.score(X_test, y_test))\n",
    "\n",
    "        model_gradboostr = GradientBoostingRegressor().fit(X_train, y_train)\n",
    "        score_gradboostr.append(model_gradboostr.score(X_test, y_test))\n",
    "\n",
    "##        model_XGBR = XGBRegressor().fit(X_train, y_train)\n",
    "#        score_XGBR.append(model_XGBR.score(X_test, y_test))\n",
    "\n",
    "# #       model_Votingr = VotingRegressor().fit(X_train, y_train)\n",
    "#        score_Votingr.append(model_Votingr.score(X_test, y_test))\n",
    "\n",
    "#  #      model_stackingr = StackingRegressor().fit(X_train, y_train)\n",
    "#        score_stackingr.append(model_stackingr.score(X_test, y_test))\n",
    "\n",
    "    liste_score = {\n",
    "    'score_lr': np.mean(score_lr), #Possible faire un DF et utiliser mean de pandas\n",
    "    'score_knn': np.mean(score_knn),\n",
    "    'score_lin_reg_sgd': np.mean(score_lin_reg_sgd), #alt + shift pour sélectionner\n",
    "    'score_huberr': np.mean(score_huberr),\n",
    "    'score_lars': np.mean(score_lars),\n",
    "    'score_lasso': np.mean(score_lasso),\n",
    "    'score_ransacr': np.mean(score_ransacr), \n",
    "    'score_ridge':np.mean(score_ridge),\n",
    "#    'score_theilsenr': np.mean(score_theilsenr),\n",
    "    'score_svr': np.mean(score_svr),\n",
    "    'score_elasticnet': np.mean(score_elasticnet),\n",
    "    'score_forestreg': np.mean(score_forestreg),\n",
    "    'score_baggingr': np.mean(score_baggingr),\n",
    "    'score_daboostr': np.mean(score_daboostr),\n",
    "    'score_gradboostr': np.mean(score_gradboostr),\n",
    "#    'score_XGBR': np.mean(score_XGBR),\n",
    "#    'score_Votingr': p.mean(score_Votingr),\n",
    "#    'score_stackingr': p.mean(score_stackingr),\n",
    "    }\n",
    "\n",
    "    return liste_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b05a9258",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T09:34:44.377653Z",
     "start_time": "2022-12-02T09:33:26.374952Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rogersiv/.pyenv/versions/3.10.6/envs/dynamic_portfolio/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score_lr': 0.9999999293665786,\n",
       " 'score_knn': -0.0015272315772421486,\n",
       " 'score_lin_reg_sgd': -1.4556838880148892,\n",
       " 'score_huberr': -7.368667962325332,\n",
       " 'score_lars': 1.0,\n",
       " 'score_lasso': -0.017300736292231934,\n",
       " 'score_ransacr': 0.9999990410182286,\n",
       " 'score_ridge': -0.9353362556996139,\n",
       " 'score_svr': -0.693536424701204,\n",
       " 'score_elasticnet': -0.017300736292231934,\n",
       " 'score_forestreg': 0.9590892586317511,\n",
       " 'score_baggingr': 0.958923936083025,\n",
       " 'score_daboostr': 0.964512672448231,\n",
       " 'score_gradboostr': 0.9741301141015648,\n",
       " 'score_XGBR': nan}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_validate_model()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e76543",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
