{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e0f1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:19:27.106744Z",
     "start_time": "2022-12-02T08:19:26.858703Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d1bd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:19:27.179623Z",
     "start_time": "2022-12-02T08:19:27.177831Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41a4a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:19:28.868554Z",
     "start_time": "2022-12-02T08:19:28.537505Z"
    }
   },
   "outputs": [],
   "source": [
    "from dynamic_portfolio.utils import load_csv\n",
    "from dynamic_portfolio.utils import features_creation, clean_data\n",
    "from dynamic_portfolio.preprocess import scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d01300",
   "metadata": {},
   "source": [
    "# Loading data and creating clean dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9420e32e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:52:46.475428Z",
     "start_time": "2022-12-02T08:52:46.368828Z"
    }
   },
   "outputs": [],
   "source": [
    "df = features_creation('META')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6363374",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:52:48.330587Z",
     "start_time": "2022-12-02T08:52:48.324007Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaebcbd",
   "metadata": {},
   "source": [
    "# Cross vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc31db2",
   "metadata": {},
   "source": [
    "We use sklearn's time series split to break up the data in different folds\n",
    "We use a sklearn example to run the first tests.\n",
    "\n",
    "Metrics used : \n",
    " - rmse\n",
    " - mae\n",
    " - R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c719f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:19:59.345931Z",
     "start_time": "2022-12-02T08:19:59.326127Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aff74a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:55:44.151243Z",
     "start_time": "2022-12-02T08:55:44.141942Z"
    }
   },
   "outputs": [],
   "source": [
    "ts_cv = TimeSeriesSplit(\n",
    "    n_splits=20,\n",
    "    gap=0,\n",
    "    max_train_size=252,\n",
    "    test_size=45,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc575dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = list(ts_cv.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcdc4ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:29:09.695999Z",
     "start_time": "2022-12-02T08:29:09.687968Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286365a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:21:46.267703Z",
     "start_time": "2022-12-02T08:21:46.259434Z"
    }
   },
   "outputs": [],
   "source": [
    "model = HistGradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f16d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:36:25.880376Z",
     "start_time": "2022-12-02T08:31:50.247799Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse = []\n",
    "for train_index, test_index in ts_cv.split(X):\n",
    "    cv_train, cv_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    model = model.fit(X,y)\n",
    "    predictions = model.predict(cv_test)\n",
    "    true_values = cv_test['return']\n",
    "    rmse.append(np.sqrt(mean_squared_error(true_values, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d38cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:37:48.457651Z",
     "start_time": "2022-12-02T08:37:48.449394Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(rmse)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f2a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:54:01.669818Z",
     "start_time": "2022-12-02T08:54:01.661065Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, X, y, cv):\n",
    "    cv_results = cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n",
    "    )\n",
    "    mae = -cv_results[\"test_neg_mean_absolute_error\"]\n",
    "    rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\n",
    "    print(\n",
    "        f\"Mean Absolute Error:     {mae.mean():.6f} +/- {mae.std():.6f}\\n\"\n",
    "        f\"Root Mean Squared Error: {rmse.mean():.6f} +/- {rmse.std():.6f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b4b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7200e5be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:56:07.338741Z",
     "start_time": "2022-12-02T08:55:50.048380Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(model, X, y, ts_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d32eb",
   "metadata": {},
   "source": [
    "# PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e2fe0e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:37:24.095118Z",
     "start_time": "2022-12-06T10:37:24.069698Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4fe089e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T14:23:12.992906Z",
     "start_time": "2022-12-06T14:23:12.775302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=0.9)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=0.9)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=0.9)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=0.9)\n",
    "ibm = prep.ready_to_train_df('IBM')\n",
    "pca.fit(ibm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "503a3509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T14:23:16.952880Z",
     "start_time": "2022-12-06T14:23:16.927409Z"
    }
   },
   "outputs": [],
   "source": [
    "n_pcs= pca.n_components_ # get number of component\n",
    "# get the index of the most important feature on EACH component\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "initial_feature_names = ibm.columns\n",
    "# get the most important feature names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5e399519",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T14:23:25.977502Z",
     "start_time": "2022-12-06T14:23:25.949855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['volume_momentum_20days',\n",
       " 'momentum_10days/eps',\n",
       " '2Y_yield',\n",
       " 'distance_10days',\n",
       " 'distance_10days',\n",
       " 'inf_exp',\n",
       " '10_2_spread',\n",
       " '2Y_return',\n",
       " 'oil_return',\n",
       " 'non_farm_payroll_return',\n",
       " 'spread_return',\n",
       " 'non_farm_payroll_return',\n",
       " 'cpi_return',\n",
       " 'gdp_return',\n",
       " 'oil_return',\n",
       " 'volume',\n",
       " 'non_farm_payroll_return']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_important_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa42da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77661ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4df44c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ee0b487",
   "metadata": {},
   "source": [
    "## Using PCA analysis on all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bd25e55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:37:12.647715Z",
     "start_time": "2022-12-06T10:37:12.621135Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08534078",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:38:17.401120Z",
     "start_time": "2022-12-06T10:38:17.380088Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('pca', PCA()),\n",
       "  ('gradientboostingregressor', GradientBoostingRegressor())],\n",
       " 'verbose': False,\n",
       " 'pca': PCA(),\n",
       " 'gradientboostingregressor': GradientBoostingRegressor(),\n",
       " 'pca__copy': True,\n",
       " 'pca__iterated_power': 'auto',\n",
       " 'pca__n_components': None,\n",
       " 'pca__n_oversamples': 10,\n",
       " 'pca__power_iteration_normalizer': 'auto',\n",
       " 'pca__random_state': None,\n",
       " 'pca__svd_solver': 'auto',\n",
       " 'pca__tol': 0.0,\n",
       " 'pca__whiten': False,\n",
       " 'gradientboostingregressor__alpha': 0.9,\n",
       " 'gradientboostingregressor__ccp_alpha': 0.0,\n",
       " 'gradientboostingregressor__criterion': 'friedman_mse',\n",
       " 'gradientboostingregressor__init': None,\n",
       " 'gradientboostingregressor__learning_rate': 0.1,\n",
       " 'gradientboostingregressor__loss': 'squared_error',\n",
       " 'gradientboostingregressor__max_depth': 3,\n",
       " 'gradientboostingregressor__max_features': None,\n",
       " 'gradientboostingregressor__max_leaf_nodes': None,\n",
       " 'gradientboostingregressor__min_impurity_decrease': 0.0,\n",
       " 'gradientboostingregressor__min_samples_leaf': 1,\n",
       " 'gradientboostingregressor__min_samples_split': 2,\n",
       " 'gradientboostingregressor__min_weight_fraction_leaf': 0.0,\n",
       " 'gradientboostingregressor__n_estimators': 100,\n",
       " 'gradientboostingregressor__n_iter_no_change': None,\n",
       " 'gradientboostingregressor__random_state': None,\n",
       " 'gradientboostingregressor__subsample': 1.0,\n",
       " 'gradientboostingregressor__tol': 0.0001,\n",
       " 'gradientboostingregressor__validation_fraction': 0.1,\n",
       " 'gradientboostingregressor__verbose': 0,\n",
       " 'gradientboostingregressor__warm_start': False}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f5b1163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:39:58.699205Z",
     "start_time": "2022-12-06T10:39:58.666792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=0.9)),\n",
       "                (&#x27;gradientboostingregressor&#x27;,\n",
       "                 GradientBoostingRegressor(learning_rate=0.08))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=0.9)),\n",
       "                (&#x27;gradientboostingregressor&#x27;,\n",
       "                 GradientBoostingRegressor(learning_rate=0.08))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=0.9)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.08)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('pca', PCA(n_components=0.9)),\n",
       "                ('gradientboostingregressor',\n",
       "                 GradientBoostingRegressor(learning_rate=0.08))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(PCA(), GradientBoostingRegressor())\n",
    "\n",
    "params = {'pca__n_components':0.9,\n",
    "                'gradientboostingregressor__max_depth':3,\n",
    "                'gradientboostingregressor__criterion':'friedman_mse',\n",
    "                'gradientboostingregressor__n_estimators':100,\n",
    "                'gradientboostingregressor__learning_rate':0.08}\n",
    "pipe.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8fa4ef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:03:57.855604Z",
     "start_time": "2022-12-06T10:42:04.427027Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker AAPL index # 0\n",
      "done for ticker MSFT index # 1\n",
      "done for ticker GOOG index # 2\n",
      "done for ticker AMZN index # 3\n",
      "done for ticker TSLA index # 4\n",
      "done for ticker UNH index # 5\n",
      "done for ticker XOM index # 6\n",
      "done for ticker JNJ index # 7\n",
      "done for ticker WMT index # 8\n",
      "done for ticker NVDA index # 9\n",
      "done for ticker JPM index # 10\n",
      "done for ticker V index # 11\n",
      "done for ticker CVX index # 12\n",
      "done for ticker PG index # 13\n",
      "done for ticker LLY index # 14\n",
      "done for ticker MA index # 15\n",
      "done for ticker HD index # 16\n",
      "done for ticker META index # 17\n",
      "done for ticker BAC index # 18\n",
      "done for ticker ABBV index # 19\n",
      "done for ticker PFE index # 20\n",
      "done for ticker KO index # 21\n",
      "done for ticker MRK index # 22\n",
      "done for ticker PEP index # 23\n",
      "done for ticker COST index # 24\n",
      "done for ticker ORCL index # 25\n",
      "done for ticker AVGO index # 26\n",
      "done for ticker TMO index # 27\n",
      "done for ticker MCD index # 28\n",
      "done for ticker CSCO index # 29\n",
      "done for ticker ACN index # 30\n",
      "done for ticker DHR index # 31\n",
      "done for ticker TMUS index # 32\n",
      "done for ticker ABT index # 33\n",
      "done for ticker WFC index # 34\n",
      "done for ticker DIS index # 35\n",
      "done for ticker LIN index # 36\n",
      "done for ticker NEE index # 37\n",
      "done for ticker BMY index # 38\n",
      "done for ticker NKE index # 39\n",
      "done for ticker VZ index # 40\n",
      "done for ticker TXN index # 41\n",
      "done for ticker UPS index # 42\n",
      "done for ticker COP index # 43\n",
      "done for ticker ADBE index # 44\n",
      "done for ticker CMCSA index # 45\n",
      "done for ticker CRM index # 46\n",
      "done for ticker PM index # 47\n",
      "done for ticker MS index # 48\n",
      "done for ticker AMGN index # 49\n",
      "done for ticker SCHW index # 50\n",
      "done for ticker HON index # 51\n",
      "done for ticker RTX index # 52\n",
      "done for ticker QCOM index # 53\n",
      "done for ticker T index # 54\n",
      "done for ticker IBM index # 55\n",
      "done for ticker DE index # 56\n",
      "done for ticker CVS index # 57\n",
      "done for ticker LOW index # 58\n",
      "done for ticker GS index # 59\n",
      "done for ticker UNP index # 60\n",
      "done for ticker NFLX index # 61\n",
      "done for ticker LMT index # 62\n",
      "done for ticker CAT index # 63\n",
      "done for ticker AMD index # 64\n",
      "done for ticker INTC index # 65\n",
      "done for ticker ELV index # 66\n",
      "done for ticker SPGI index # 67\n",
      "done for ticker AXP index # 68\n",
      "done for ticker SBUX index # 69\n",
      "done for ticker INTU index # 70\n",
      "done for ticker BLK index # 71\n",
      "done for ticker ADP index # 72\n",
      "done for ticker GILD index # 73\n",
      "done for ticker PLD index # 74\n",
      "done for ticker MDT index # 75\n",
      "done for ticker BA index # 76\n",
      "done for ticker AMT index # 77\n",
      "done for ticker CI index # 78\n",
      "done for ticker GE index # 79\n",
      "done for ticker TJX index # 80\n",
      "done for ticker ISRG index # 81\n",
      "done for ticker C index # 82\n",
      "done for ticker AMAT index # 83\n",
      "done for ticker PYPL index # 84\n",
      "done for ticker MDLZ index # 85\n",
      "done for ticker CB index # 86\n",
      "done for ticker SYK index # 87\n",
      "done for ticker ADI index # 88\n",
      "done for ticker MMC index # 89\n",
      "done for ticker EOG index # 90\n",
      "done for ticker NOW index # 91\n",
      "done for ticker VRTX index # 92\n",
      "done for ticker MO index # 93\n",
      "done for ticker NOC index # 94\n",
      "done for ticker EL index # 95\n",
      "done for ticker REGN index # 96\n",
      "done for ticker PGR index # 97\n",
      "done for ticker BKNG index # 98\n",
      "done for ticker DUK index # 99\n",
      "done for ticker TGT index # 100\n",
      "done for ticker SLB index # 101\n",
      "done for ticker SO index # 102\n",
      "done for ticker MMM index # 103\n",
      "done for ticker ITW index # 104\n",
      "done for ticker ZTS index # 105\n",
      "done for ticker GD index # 106\n",
      "done for ticker APD index # 107\n",
      "done for ticker HUM index # 108\n",
      "done for ticker MRNA index # 109\n",
      "done for ticker BDX index # 110\n",
      "done for ticker CSX index # 111\n",
      "done for ticker WM index # 112\n",
      "done for ticker PNC index # 113\n",
      "done for ticker HCA index # 114\n",
      "done for ticker ETN index # 115\n",
      "done for ticker USB index # 116\n",
      "done for ticker FISV index # 117\n",
      "done for ticker SHW index # 118\n",
      "done for ticker OXY index # 119\n",
      "done for ticker CL index # 120\n",
      "done for ticker MU index # 121\n",
      "done for ticker CME index # 122\n",
      "done for ticker AON index # 123\n",
      "done for ticker LRCX index # 124\n",
      "done for ticker BSX index # 125\n",
      "done for ticker EQIX index # 126\n",
      "done for ticker TFC index # 127\n",
      "done for ticker PXD index # 128\n",
      "done for ticker CHTR index # 129\n",
      "done for ticker CCI index # 130\n",
      "done for ticker MET index # 131\n",
      "done for ticker ATVI index # 132\n",
      "done for ticker ICE index # 133\n",
      "done for ticker MPC index # 134\n",
      "done for ticker NSC index # 135\n",
      "done for ticker DG index # 136\n",
      "done for ticker GM index # 137\n",
      "done for ticker EMR index # 138\n",
      "done for ticker F index # 139\n",
      "done for ticker KLAC index # 140\n",
      "done for ticker MCO index # 141\n",
      "done for ticker FCX index # 142\n",
      "done for ticker KDP index # 143\n",
      "done for ticker MNST index # 144\n",
      "done for ticker MCK index # 145\n",
      "done for ticker VLO index # 146\n",
      "done for ticker ORLY index # 147\n",
      "done for ticker ADM index # 148\n",
      "done for ticker PSX index # 149\n",
      "done for ticker PSA index # 150\n",
      "done for ticker SRE index # 151\n",
      "done for ticker SNPS index # 152\n",
      "done for ticker MAR index # 153\n",
      "done for ticker D index # 154\n",
      "done for ticker GIS index # 155\n",
      "done for ticker AEP index # 156\n",
      "done for ticker AZO index # 157\n",
      "done for ticker KHC index # 158\n",
      "done for ticker APH index # 159\n",
      "done for ticker HSY index # 160\n",
      "done for ticker CNC index # 161\n",
      "done for ticker CTVA index # 162\n",
      "done for ticker EW index # 163\n",
      "done for ticker CTAS index # 164\n",
      "done for ticker A index # 165\n",
      "done for ticker ROP index # 166\n",
      "done for ticker JCI index # 167\n",
      "done for ticker CDNS index # 168\n",
      "done for ticker FDX index # 169\n",
      "done for ticker NXPI index # 170\n",
      "done for ticker AIG index # 171\n",
      "done for ticker KMB index # 172\n",
      "done for ticker AFL index # 173\n",
      "done for ticker HES index # 174\n",
      "done for ticker MSI index # 175\n",
      "done for ticker PAYX index # 176\n",
      "done for ticker DVN index # 177\n",
      "done for ticker TRV index # 178\n",
      "done for ticker BIIB index # 179\n",
      "done for ticker DXCM index # 180\n",
      "done for ticker SYY index # 181\n",
      "done for ticker LHX index # 182\n",
      "done for ticker RSG index # 183\n",
      "done for ticker ENPH index # 184\n",
      "done for ticker ECL index # 185\n",
      "done for ticker ADSK index # 186\n",
      "done for ticker MCHP index # 187\n",
      "done for ticker ANET index # 188\n",
      "done for ticker KMI index # 189\n",
      "done for ticker CMG index # 190\n",
      "done for ticker FTNT index # 191\n",
      "done for ticker AJG index # 192\n",
      "done for ticker STZ index # 193\n",
      "done for ticker TT index # 194\n",
      "done for ticker WMB index # 195\n",
      "done for ticker MSCI index # 196\n",
      "done for ticker O index # 197\n",
      "done for ticker IQV index # 198\n",
      "done for ticker TEL index # 199\n",
      "done for ticker ROST index # 200\n",
      "done for ticker PRU index # 201\n",
      "done for ticker EXC index # 202\n",
      "done for ticker PH index # 203\n",
      "done for ticker FIS index # 204\n",
      "done for ticker SPG index # 205\n",
      "done for ticker COF index # 206\n",
      "done for ticker NUE index # 207\n",
      "done for ticker XEL index # 208\n",
      "done for ticker HLT index # 209\n",
      "done for ticker CARR index # 210\n",
      "done for ticker PCAR index # 211\n",
      "done for ticker BK index # 212\n",
      "done for ticker NEM index # 213\n",
      "done for ticker DOW index # 214\n",
      "done for ticker EA index # 215\n",
      "done for ticker WBA index # 216\n",
      "done for ticker DD index # 217\n",
      "done for ticker ALL index # 218\n",
      "done for ticker YUM index # 219\n",
      "done for ticker AMP index # 220\n",
      "done for ticker CMI index # 221\n",
      "done for ticker ILMN index # 222\n",
      "done for ticker TDG index # 223\n",
      "done for ticker IDXX index # 224\n",
      "done for ticker ED index # 225\n",
      "done for ticker KR index # 226\n",
      "done for ticker ABC index # 227\n",
      "done for ticker DLTR index # 228\n",
      "done for ticker RMD index # 229\n",
      "done for ticker ALB index # 230\n",
      "done for ticker HAL index # 231\n",
      "done for ticker NDAQ index # 232\n",
      "done for ticker LVS index # 233\n",
      "done for ticker ODFL index # 234\n",
      "done for ticker WELL index # 235\n",
      "done for ticker AME index # 236\n",
      "done for ticker CSGP index # 237\n",
      "done for ticker OTIS index # 238\n",
      "done for ticker MTD index # 239\n",
      "done for ticker SBAC index # 240\n",
      "done for ticker ON index # 241\n",
      "done for ticker VICI index # 242\n",
      "done for ticker DLR index # 243\n",
      "done for ticker KEYS index # 244\n",
      "done for ticker PPG index # 245\n",
      "done for ticker WEC index # 246\n",
      "done for ticker CTSH index # 247\n",
      "done for ticker ROK index # 248\n",
      "done for ticker GWW index # 249\n",
      "done for ticker PCG index # 250\n",
      "done for ticker HPQ index # 251\n",
      "done for ticker FAST index # 252\n",
      "done for ticker DFS index # 253\n",
      "done for ticker MTB index # 254\n",
      "done for ticker PEG index # 255\n",
      "done for ticker OKE index # 256\n",
      "done for ticker DHI index # 257\n",
      "done for ticker APTV index # 258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker BKR index # 259\n",
      "done for ticker GLW index # 260\n",
      "done for ticker LYB index # 261\n",
      "done for ticker ES index # 262\n",
      "done for ticker BAX index # 263\n",
      "done for ticker STT index # 264\n",
      "done for ticker VRSK index # 265\n",
      "done for ticker TROW index # 266\n",
      "done for ticker WBD index # 267\n",
      "done for ticker AWK index # 268\n",
      "done for ticker IT index # 269\n",
      "done for ticker GPN index # 270\n",
      "done for ticker HRL index # 271\n",
      "done for ticker FANG index # 272\n",
      "done for ticker WTW index # 273\n",
      "done for ticker RJF index # 274\n",
      "done for ticker GPC index # 275\n",
      "done for ticker IFF index # 276\n",
      "done for ticker CDW index # 277\n",
      "done for ticker TSCO index # 278\n",
      "done for ticker FITB index # 279\n",
      "done for ticker ARE index # 280\n",
      "done for ticker URI index # 281\n",
      "done for ticker ZBH index # 282\n",
      "done for ticker K index # 283\n",
      "done for ticker LEN index # 284\n",
      "done for ticker EBAY index # 285\n",
      "done for ticker EIX index # 286\n",
      "done for ticker CBRE index # 287\n",
      "done for ticker EFX index # 288\n",
      "done for ticker VMC index # 289\n",
      "done for ticker TSN index # 290\n",
      "done for ticker HIG index # 291\n",
      "done for ticker FTV index # 292\n",
      "done for ticker WY index # 293\n",
      "done for ticker EQR index # 294\n",
      "done for ticker AVB index # 295\n",
      "done for ticker MKC index # 296\n",
      "done for ticker ETR index # 297\n",
      "done for ticker LUV index # 298\n",
      "done for ticker ULTA index # 299\n",
      "done for ticker AEE index # 300\n",
      "done for ticker MLM index # 301\n",
      "done for ticker FE index # 302\n",
      "done for ticker PFG index # 303\n",
      "done for ticker FRC index # 304\n",
      "done for ticker DTE index # 305\n",
      "done for ticker DAL index # 306\n",
      "done for ticker HBAN index # 307\n",
      "done for ticker IR index # 308\n",
      "done for ticker CTRA index # 309\n",
      "done for ticker ANSS index # 310\n",
      "done for ticker ACGL index # 311\n",
      "done for ticker PPL index # 312\n",
      "done for ticker RF index # 313\n",
      "done for ticker VRSN index # 314\n",
      "done for ticker LH index # 315\n",
      "done for ticker EXR index # 316\n",
      "done for ticker PWR index # 317\n",
      "done for ticker CF index # 318\n",
      "done for ticker CAH index # 319\n",
      "done for ticker CFG index # 320\n",
      "done for ticker XYL index # 321\n",
      "done for ticker HPE index # 322\n",
      "done for ticker EPAM index # 323\n",
      "done for ticker DOV index # 324\n",
      "done for ticker WAT index # 325\n",
      "done for ticker WRB index # 326\n",
      "done for ticker TDY index # 327\n",
      "done for ticker PAYC index # 328\n",
      "done for ticker ROL index # 329\n",
      "done for ticker NTRS index # 330\n",
      "done for ticker MRO index # 331\n",
      "done for ticker CNP index # 332\n",
      "done for ticker INVH index # 333\n",
      "done for ticker CHD index # 334\n",
      "done for ticker AES index # 335\n",
      "done for ticker MOH index # 336\n",
      "done for ticker JBHT index # 337\n",
      "done for ticker MAA index # 338\n",
      "done for ticker BBY index # 339\n",
      "done for ticker CLX index # 340\n",
      "done for ticker HOLX index # 341\n",
      "done for ticker WAB index # 342\n",
      "done for ticker DRI index # 343\n",
      "done for ticker EXPD index # 344\n",
      "done for ticker STE index # 345\n",
      "done for ticker AMCR index # 346\n",
      "done for ticker VTR index # 347\n",
      "done for ticker IEX index # 348\n",
      "done for ticker CAG index # 349\n",
      "done for ticker CMS index # 350\n",
      "done for ticker KEY index # 351\n",
      "done for ticker MPWR index # 352\n",
      "done for ticker BALL index # 353\n",
      "done for ticker J index # 354\n",
      "done for ticker BR index # 355\n",
      "done for ticker GRMN index # 356\n",
      "done for ticker PKI index # 357\n",
      "done for ticker TTWO index # 358\n",
      "done for ticker INCY index # 359\n",
      "done for ticker FDS index # 360\n",
      "done for ticker MOS index # 361\n",
      "done for ticker SEDG index # 362\n",
      "done for ticker CINF index # 363\n",
      "done for ticker ABMD index # 364\n",
      "done for ticker DGX index # 365\n",
      "done for ticker WST index # 366\n",
      "done for ticker ATO index # 367\n",
      "done for ticker TRGP index # 368\n",
      "done for ticker BRO index # 369\n",
      "done for ticker SYF index # 370\n",
      "done for ticker FOX index # 371\n",
      "done for ticker FOXA index # 372\n",
      "done for ticker NTAP index # 373\n",
      "done for ticker FMC index # 374\n",
      "done for ticker EQT index # 375\n",
      "done for ticker OMC index # 376\n",
      "done for ticker SJM index # 377\n",
      "done for ticker LYV index # 378\n",
      "done for ticker CPB index # 379\n",
      "done for ticker HWM index # 380\n",
      "done for ticker CPRT index # 381\n",
      "done for ticker AVY index # 382\n",
      "done for ticker IRM index # 383\n",
      "done for ticker COO index # 384\n",
      "done for ticker ALGN index # 385\n",
      "done for ticker SWKS index # 386\n",
      "done for ticker EXPE index # 387\n",
      "done for ticker RCL index # 388\n",
      "done for ticker ETSY index # 389\n",
      "done for ticker APA index # 390\n",
      "done for ticker GEN index # 391\n",
      "done for ticker TXT index # 392\n",
      "done for ticker LDOS index # 393\n",
      "done for ticker LKQ index # 394\n",
      "done for ticker TER index # 395\n",
      "done for ticker PTC index # 396\n",
      "done for ticker TRMB index # 397\n",
      "done for ticker AKAM index # 398\n",
      "done for ticker NVR index # 399\n",
      "done for ticker UAL index # 400\n",
      "done for ticker LNT index # 401\n",
      "done for ticker FLT index # 402\n",
      "done for ticker KIM index # 403\n",
      "done for ticker ZBRA index # 404\n",
      "done for ticker TYL index # 405\n",
      "done for ticker DPZ index # 406\n",
      "done for ticker JKHY index # 407\n",
      "done for ticker MGM index # 408\n",
      "done for ticker ESS index # 409\n",
      "done for ticker L index # 410\n",
      "done for ticker PEAK index # 411\n",
      "done for ticker MTCH index # 412\n",
      "done for ticker NDSN index # 413\n",
      "done for ticker EVRG index # 414\n",
      "done for ticker IPG index # 415\n",
      "done for ticker BEN index # 416\n",
      "done for ticker CBOE index # 417\n",
      "done for ticker TECH index # 418\n",
      "done for ticker SIVB index # 419\n",
      "done for ticker VFC index # 420\n",
      "done for ticker IP index # 421\n",
      "done for ticker HST index # 422\n",
      "done for ticker UDR index # 423\n",
      "done for ticker POOL index # 424\n",
      "done for ticker RE index # 425\n",
      "done for ticker PARA index # 426\n",
      "done for ticker SNA index # 427\n",
      "done for ticker CPT index # 428\n",
      "done for ticker LW index # 429\n",
      "done for ticker PKG index # 430\n",
      "done for ticker CRL index # 431\n",
      "done for ticker SWK index # 432\n",
      "done for ticker BIO index # 433\n",
      "done for ticker WDC index # 434\n",
      "done for ticker CHRW index # 435\n",
      "done for ticker STX index # 436\n",
      "done for ticker MAS index # 437\n",
      "done for ticker GL index # 438\n",
      "done for ticker CE index # 439\n",
      "done for ticker REG index # 440\n",
      "done for ticker NI index # 441\n",
      "done for ticker BXP index # 442\n",
      "done for ticker HSIC index # 443\n",
      "done for ticker CCL index # 444\n",
      "done for ticker TFX index # 445\n",
      "done for ticker CZR index # 446\n",
      "done for ticker NWS index # 447\n",
      "done for ticker NWSA index # 448\n",
      "done for ticker KMX index # 449\n",
      "done for ticker EMN index # 450\n",
      "done for ticker JNPR index # 451\n",
      "done for ticker PHM index # 452\n",
      "done for ticker CDAY index # 453\n",
      "done for ticker ALLE index # 454\n",
      "done for ticker QRVO index # 455\n",
      "done for ticker BWA index # 456\n",
      "done for ticker NRG index # 457\n",
      "done for ticker MKTX index # 458\n",
      "done for ticker WRK index # 459\n",
      "done for ticker UHS index # 460\n",
      "done for ticker FFIV index # 461\n",
      "done for ticker AOS index # 462\n",
      "done for ticker CMA index # 463\n",
      "done for ticker AAL index # 464\n",
      "done for ticker BBWI index # 465\n",
      "done for ticker HII index # 466\n",
      "done for ticker AAP index # 467\n",
      "done for ticker TPR index # 468\n",
      "done for ticker FRT index # 469\n",
      "done for ticker IVZ index # 470\n",
      "done for ticker PNW index # 471\n",
      "done for ticker HAS index # 472\n",
      "done for ticker WYNN index # 473\n",
      "done for ticker FBHS index # 474\n",
      "done for ticker SBNY index # 475\n",
      "done for ticker DISH index # 476\n",
      "done for ticker RHI index # 477\n",
      "done for ticker WHR index # 478\n",
      "done for ticker ZION index # 479\n",
      "done for ticker CTLT index # 480\n",
      "done for ticker PNR index # 481\n",
      "done for ticker SEE index # 482\n",
      "done for ticker RL index # 483\n",
      "done for ticker NCLH index # 484\n",
      "done for ticker DXC index # 485\n",
      "done for ticker GNRC index # 486\n",
      "done for ticker AIZ index # 487\n",
      "done for ticker XRAY index # 488\n",
      "done for ticker LNC index # 489\n",
      "done for ticker DVA index # 490\n",
      "done for ticker MHK index # 491\n",
      "done for ticker LUMN index # 492\n",
      "done for ticker ALK index # 493\n",
      "done for ticker NWL index # 494\n",
      "done for ticker VNO index # 495\n",
      "done for ticker TAP index # 496\n"
     ]
    }
   ],
   "source": [
    "pca_dict_score = {}\n",
    "tickers = utils.return_tickers()\n",
    "for ticker in tickers:\n",
    "        dict_pca_score[ticker] = cross_validate_ml(prep.ready_to_train_df(ticker), pipe)\n",
    "        print(f\"done for ticker {ticker} index # {tickers.index(ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8e305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3db6f9ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:26:43.265897Z",
     "start_time": "2022-12-06T11:06:29.287564Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker AAPL index # 0\n",
      "done for ticker MSFT index # 1\n",
      "done for ticker GOOG index # 2\n",
      "done for ticker AMZN index # 3\n",
      "done for ticker TSLA index # 4\n",
      "done for ticker UNH index # 5\n",
      "done for ticker XOM index # 6\n",
      "done for ticker JNJ index # 7\n",
      "done for ticker WMT index # 8\n",
      "done for ticker NVDA index # 9\n",
      "done for ticker JPM index # 10\n",
      "done for ticker V index # 11\n",
      "done for ticker CVX index # 12\n",
      "done for ticker PG index # 13\n",
      "done for ticker LLY index # 14\n",
      "done for ticker MA index # 15\n",
      "done for ticker HD index # 16\n",
      "done for ticker META index # 17\n",
      "done for ticker BAC index # 18\n",
      "done for ticker ABBV index # 19\n",
      "done for ticker PFE index # 20\n",
      "done for ticker KO index # 21\n",
      "done for ticker MRK index # 22\n",
      "done for ticker PEP index # 23\n",
      "done for ticker COST index # 24\n",
      "done for ticker ORCL index # 25\n",
      "done for ticker AVGO index # 26\n",
      "done for ticker TMO index # 27\n",
      "done for ticker MCD index # 28\n",
      "done for ticker CSCO index # 29\n",
      "done for ticker ACN index # 30\n",
      "done for ticker DHR index # 31\n",
      "done for ticker TMUS index # 32\n",
      "done for ticker ABT index # 33\n",
      "done for ticker WFC index # 34\n",
      "done for ticker DIS index # 35\n",
      "done for ticker LIN index # 36\n",
      "done for ticker NEE index # 37\n",
      "done for ticker BMY index # 38\n",
      "done for ticker NKE index # 39\n",
      "done for ticker VZ index # 40\n",
      "done for ticker TXN index # 41\n",
      "done for ticker UPS index # 42\n",
      "done for ticker COP index # 43\n",
      "done for ticker ADBE index # 44\n",
      "done for ticker CMCSA index # 45\n",
      "done for ticker CRM index # 46\n",
      "done for ticker PM index # 47\n",
      "done for ticker MS index # 48\n",
      "done for ticker AMGN index # 49\n",
      "done for ticker SCHW index # 50\n",
      "done for ticker HON index # 51\n",
      "done for ticker RTX index # 52\n",
      "done for ticker QCOM index # 53\n",
      "done for ticker T index # 54\n",
      "done for ticker IBM index # 55\n",
      "done for ticker DE index # 56\n",
      "done for ticker CVS index # 57\n",
      "done for ticker LOW index # 58\n",
      "done for ticker GS index # 59\n",
      "done for ticker UNP index # 60\n",
      "done for ticker NFLX index # 61\n",
      "done for ticker LMT index # 62\n",
      "done for ticker CAT index # 63\n",
      "done for ticker AMD index # 64\n",
      "done for ticker INTC index # 65\n",
      "done for ticker ELV index # 66\n",
      "done for ticker SPGI index # 67\n",
      "done for ticker AXP index # 68\n",
      "done for ticker SBUX index # 69\n",
      "done for ticker INTU index # 70\n",
      "done for ticker BLK index # 71\n",
      "done for ticker ADP index # 72\n",
      "done for ticker GILD index # 73\n",
      "done for ticker PLD index # 74\n",
      "done for ticker MDT index # 75\n",
      "done for ticker BA index # 76\n",
      "done for ticker AMT index # 77\n",
      "done for ticker CI index # 78\n",
      "done for ticker GE index # 79\n",
      "done for ticker TJX index # 80\n",
      "done for ticker ISRG index # 81\n",
      "done for ticker C index # 82\n",
      "done for ticker AMAT index # 83\n",
      "done for ticker PYPL index # 84\n",
      "done for ticker MDLZ index # 85\n",
      "done for ticker CB index # 86\n",
      "done for ticker SYK index # 87\n",
      "done for ticker ADI index # 88\n",
      "done for ticker MMC index # 89\n",
      "done for ticker EOG index # 90\n",
      "done for ticker NOW index # 91\n",
      "done for ticker VRTX index # 92\n",
      "done for ticker MO index # 93\n",
      "done for ticker NOC index # 94\n",
      "done for ticker EL index # 95\n",
      "done for ticker REGN index # 96\n",
      "done for ticker PGR index # 97\n",
      "done for ticker BKNG index # 98\n",
      "done for ticker DUK index # 99\n",
      "done for ticker TGT index # 100\n",
      "done for ticker SLB index # 101\n",
      "done for ticker SO index # 102\n",
      "done for ticker MMM index # 103\n",
      "done for ticker ITW index # 104\n",
      "done for ticker ZTS index # 105\n",
      "done for ticker GD index # 106\n",
      "done for ticker APD index # 107\n",
      "done for ticker HUM index # 108\n",
      "done for ticker MRNA index # 109\n",
      "done for ticker BDX index # 110\n",
      "done for ticker CSX index # 111\n",
      "done for ticker WM index # 112\n",
      "done for ticker PNC index # 113\n",
      "done for ticker HCA index # 114\n",
      "done for ticker ETN index # 115\n",
      "done for ticker USB index # 116\n",
      "done for ticker FISV index # 117\n",
      "done for ticker SHW index # 118\n",
      "done for ticker OXY index # 119\n",
      "done for ticker CL index # 120\n",
      "done for ticker MU index # 121\n",
      "done for ticker CME index # 122\n",
      "done for ticker AON index # 123\n",
      "done for ticker LRCX index # 124\n",
      "done for ticker BSX index # 125\n",
      "done for ticker EQIX index # 126\n",
      "done for ticker TFC index # 127\n",
      "done for ticker PXD index # 128\n",
      "done for ticker CHTR index # 129\n",
      "done for ticker CCI index # 130\n",
      "done for ticker MET index # 131\n",
      "done for ticker ATVI index # 132\n",
      "done for ticker ICE index # 133\n",
      "done for ticker MPC index # 134\n",
      "done for ticker NSC index # 135\n",
      "done for ticker DG index # 136\n",
      "done for ticker GM index # 137\n",
      "done for ticker EMR index # 138\n",
      "done for ticker F index # 139\n",
      "done for ticker KLAC index # 140\n",
      "done for ticker MCO index # 141\n",
      "done for ticker FCX index # 142\n",
      "done for ticker KDP index # 143\n",
      "done for ticker MNST index # 144\n",
      "done for ticker MCK index # 145\n",
      "done for ticker VLO index # 146\n",
      "done for ticker ORLY index # 147\n",
      "done for ticker ADM index # 148\n",
      "done for ticker PSX index # 149\n",
      "done for ticker PSA index # 150\n",
      "done for ticker SRE index # 151\n",
      "done for ticker SNPS index # 152\n",
      "done for ticker MAR index # 153\n",
      "done for ticker D index # 154\n",
      "done for ticker GIS index # 155\n",
      "done for ticker AEP index # 156\n",
      "done for ticker AZO index # 157\n",
      "done for ticker KHC index # 158\n",
      "done for ticker APH index # 159\n",
      "done for ticker HSY index # 160\n",
      "done for ticker CNC index # 161\n",
      "done for ticker CTVA index # 162\n",
      "done for ticker EW index # 163\n",
      "done for ticker CTAS index # 164\n",
      "done for ticker A index # 165\n",
      "done for ticker ROP index # 166\n",
      "done for ticker JCI index # 167\n",
      "done for ticker CDNS index # 168\n",
      "done for ticker FDX index # 169\n",
      "done for ticker NXPI index # 170\n",
      "done for ticker AIG index # 171\n",
      "done for ticker KMB index # 172\n",
      "done for ticker AFL index # 173\n",
      "done for ticker HES index # 174\n",
      "done for ticker MSI index # 175\n",
      "done for ticker PAYX index # 176\n",
      "done for ticker DVN index # 177\n",
      "done for ticker TRV index # 178\n",
      "done for ticker BIIB index # 179\n",
      "done for ticker DXCM index # 180\n",
      "done for ticker SYY index # 181\n",
      "done for ticker LHX index # 182\n",
      "done for ticker RSG index # 183\n",
      "done for ticker ENPH index # 184\n",
      "done for ticker ECL index # 185\n",
      "done for ticker ADSK index # 186\n",
      "done for ticker MCHP index # 187\n",
      "done for ticker ANET index # 188\n",
      "done for ticker KMI index # 189\n",
      "done for ticker CMG index # 190\n",
      "done for ticker FTNT index # 191\n",
      "done for ticker AJG index # 192\n",
      "done for ticker STZ index # 193\n",
      "done for ticker TT index # 194\n",
      "done for ticker WMB index # 195\n",
      "done for ticker MSCI index # 196\n",
      "done for ticker O index # 197\n",
      "done for ticker IQV index # 198\n",
      "done for ticker TEL index # 199\n",
      "done for ticker ROST index # 200\n",
      "done for ticker PRU index # 201\n",
      "done for ticker EXC index # 202\n",
      "done for ticker PH index # 203\n",
      "done for ticker FIS index # 204\n",
      "done for ticker SPG index # 205\n",
      "done for ticker COF index # 206\n",
      "done for ticker NUE index # 207\n",
      "done for ticker XEL index # 208\n",
      "done for ticker HLT index # 209\n",
      "done for ticker CARR index # 210\n",
      "done for ticker PCAR index # 211\n",
      "done for ticker BK index # 212\n",
      "done for ticker NEM index # 213\n",
      "done for ticker DOW index # 214\n",
      "done for ticker EA index # 215\n",
      "done for ticker WBA index # 216\n",
      "done for ticker DD index # 217\n",
      "done for ticker ALL index # 218\n",
      "done for ticker YUM index # 219\n",
      "done for ticker AMP index # 220\n",
      "done for ticker CMI index # 221\n",
      "done for ticker ILMN index # 222\n",
      "done for ticker TDG index # 223\n",
      "done for ticker IDXX index # 224\n",
      "done for ticker ED index # 225\n",
      "done for ticker KR index # 226\n",
      "done for ticker ABC index # 227\n",
      "done for ticker DLTR index # 228\n",
      "done for ticker RMD index # 229\n",
      "done for ticker ALB index # 230\n",
      "done for ticker HAL index # 231\n",
      "done for ticker NDAQ index # 232\n",
      "done for ticker LVS index # 233\n",
      "done for ticker ODFL index # 234\n",
      "done for ticker WELL index # 235\n",
      "done for ticker AME index # 236\n",
      "done for ticker CSGP index # 237\n",
      "done for ticker OTIS index # 238\n",
      "done for ticker MTD index # 239\n",
      "done for ticker SBAC index # 240\n",
      "done for ticker ON index # 241\n",
      "done for ticker VICI index # 242\n",
      "done for ticker DLR index # 243\n",
      "done for ticker KEYS index # 244\n",
      "done for ticker PPG index # 245\n",
      "done for ticker WEC index # 246\n",
      "done for ticker CTSH index # 247\n",
      "done for ticker ROK index # 248\n",
      "done for ticker GWW index # 249\n",
      "done for ticker PCG index # 250\n",
      "done for ticker HPQ index # 251\n",
      "done for ticker FAST index # 252\n",
      "done for ticker DFS index # 253\n",
      "done for ticker MTB index # 254\n",
      "done for ticker PEG index # 255\n",
      "done for ticker OKE index # 256\n",
      "done for ticker DHI index # 257\n",
      "done for ticker APTV index # 258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker BKR index # 259\n",
      "done for ticker GLW index # 260\n",
      "done for ticker LYB index # 261\n",
      "done for ticker ES index # 262\n",
      "done for ticker BAX index # 263\n",
      "done for ticker STT index # 264\n",
      "done for ticker VRSK index # 265\n",
      "done for ticker TROW index # 266\n",
      "done for ticker WBD index # 267\n",
      "done for ticker AWK index # 268\n",
      "done for ticker IT index # 269\n",
      "done for ticker GPN index # 270\n",
      "done for ticker HRL index # 271\n",
      "done for ticker FANG index # 272\n",
      "done for ticker WTW index # 273\n",
      "done for ticker RJF index # 274\n",
      "done for ticker GPC index # 275\n",
      "done for ticker IFF index # 276\n",
      "done for ticker CDW index # 277\n",
      "done for ticker TSCO index # 278\n",
      "done for ticker FITB index # 279\n",
      "done for ticker ARE index # 280\n",
      "done for ticker URI index # 281\n",
      "done for ticker ZBH index # 282\n",
      "done for ticker K index # 283\n",
      "done for ticker LEN index # 284\n",
      "done for ticker EBAY index # 285\n",
      "done for ticker EIX index # 286\n",
      "done for ticker CBRE index # 287\n",
      "done for ticker EFX index # 288\n",
      "done for ticker VMC index # 289\n",
      "done for ticker TSN index # 290\n",
      "done for ticker HIG index # 291\n",
      "done for ticker FTV index # 292\n",
      "done for ticker WY index # 293\n",
      "done for ticker EQR index # 294\n",
      "done for ticker AVB index # 295\n",
      "done for ticker MKC index # 296\n",
      "done for ticker ETR index # 297\n",
      "done for ticker LUV index # 298\n",
      "done for ticker ULTA index # 299\n",
      "done for ticker AEE index # 300\n",
      "done for ticker MLM index # 301\n",
      "done for ticker FE index # 302\n",
      "done for ticker PFG index # 303\n",
      "done for ticker FRC index # 304\n",
      "done for ticker DTE index # 305\n",
      "done for ticker DAL index # 306\n",
      "done for ticker HBAN index # 307\n",
      "done for ticker IR index # 308\n",
      "done for ticker CTRA index # 309\n",
      "done for ticker ANSS index # 310\n",
      "done for ticker ACGL index # 311\n",
      "done for ticker PPL index # 312\n",
      "done for ticker RF index # 313\n",
      "done for ticker VRSN index # 314\n",
      "done for ticker LH index # 315\n",
      "done for ticker EXR index # 316\n",
      "done for ticker PWR index # 317\n",
      "done for ticker CF index # 318\n",
      "done for ticker CAH index # 319\n",
      "done for ticker CFG index # 320\n",
      "done for ticker XYL index # 321\n",
      "done for ticker HPE index # 322\n",
      "done for ticker EPAM index # 323\n",
      "done for ticker DOV index # 324\n",
      "done for ticker WAT index # 325\n",
      "done for ticker WRB index # 326\n",
      "done for ticker TDY index # 327\n",
      "done for ticker PAYC index # 328\n",
      "done for ticker ROL index # 329\n",
      "done for ticker NTRS index # 330\n",
      "done for ticker MRO index # 331\n",
      "done for ticker CNP index # 332\n",
      "done for ticker INVH index # 333\n",
      "done for ticker CHD index # 334\n",
      "done for ticker AES index # 335\n",
      "done for ticker MOH index # 336\n",
      "done for ticker JBHT index # 337\n",
      "done for ticker MAA index # 338\n",
      "done for ticker BBY index # 339\n",
      "done for ticker CLX index # 340\n",
      "done for ticker HOLX index # 341\n",
      "done for ticker WAB index # 342\n",
      "done for ticker DRI index # 343\n",
      "done for ticker EXPD index # 344\n",
      "done for ticker STE index # 345\n",
      "done for ticker AMCR index # 346\n",
      "done for ticker VTR index # 347\n",
      "done for ticker IEX index # 348\n",
      "done for ticker CAG index # 349\n",
      "done for ticker CMS index # 350\n",
      "done for ticker KEY index # 351\n",
      "done for ticker MPWR index # 352\n",
      "done for ticker BALL index # 353\n",
      "done for ticker J index # 354\n",
      "done for ticker BR index # 355\n",
      "done for ticker GRMN index # 356\n",
      "done for ticker PKI index # 357\n",
      "done for ticker TTWO index # 358\n",
      "done for ticker INCY index # 359\n",
      "done for ticker FDS index # 360\n",
      "done for ticker MOS index # 361\n",
      "done for ticker SEDG index # 362\n",
      "done for ticker CINF index # 363\n",
      "done for ticker ABMD index # 364\n",
      "done for ticker DGX index # 365\n",
      "done for ticker WST index # 366\n",
      "done for ticker ATO index # 367\n",
      "done for ticker TRGP index # 368\n",
      "done for ticker BRO index # 369\n",
      "done for ticker SYF index # 370\n",
      "done for ticker FOX index # 371\n",
      "done for ticker FOXA index # 372\n",
      "done for ticker NTAP index # 373\n",
      "done for ticker FMC index # 374\n",
      "done for ticker EQT index # 375\n",
      "done for ticker OMC index # 376\n",
      "done for ticker SJM index # 377\n",
      "done for ticker LYV index # 378\n",
      "done for ticker CPB index # 379\n",
      "done for ticker HWM index # 380\n",
      "done for ticker CPRT index # 381\n",
      "done for ticker AVY index # 382\n",
      "done for ticker IRM index # 383\n",
      "done for ticker COO index # 384\n",
      "done for ticker ALGN index # 385\n",
      "done for ticker SWKS index # 386\n",
      "done for ticker EXPE index # 387\n",
      "done for ticker RCL index # 388\n",
      "done for ticker ETSY index # 389\n",
      "done for ticker APA index # 390\n",
      "done for ticker GEN index # 391\n",
      "done for ticker TXT index # 392\n",
      "done for ticker LDOS index # 393\n",
      "done for ticker LKQ index # 394\n",
      "done for ticker TER index # 395\n",
      "done for ticker PTC index # 396\n",
      "done for ticker TRMB index # 397\n",
      "done for ticker AKAM index # 398\n",
      "done for ticker NVR index # 399\n",
      "done for ticker UAL index # 400\n",
      "done for ticker LNT index # 401\n",
      "done for ticker FLT index # 402\n",
      "done for ticker KIM index # 403\n",
      "done for ticker ZBRA index # 404\n",
      "done for ticker TYL index # 405\n",
      "done for ticker DPZ index # 406\n",
      "done for ticker JKHY index # 407\n",
      "done for ticker MGM index # 408\n",
      "done for ticker ESS index # 409\n",
      "done for ticker L index # 410\n",
      "done for ticker PEAK index # 411\n",
      "done for ticker MTCH index # 412\n",
      "done for ticker NDSN index # 413\n",
      "done for ticker EVRG index # 414\n",
      "done for ticker IPG index # 415\n",
      "done for ticker BEN index # 416\n",
      "done for ticker CBOE index # 417\n",
      "done for ticker TECH index # 418\n",
      "done for ticker SIVB index # 419\n",
      "done for ticker VFC index # 420\n",
      "done for ticker IP index # 421\n",
      "done for ticker HST index # 422\n",
      "done for ticker UDR index # 423\n",
      "done for ticker POOL index # 424\n",
      "done for ticker RE index # 425\n",
      "done for ticker PARA index # 426\n",
      "done for ticker SNA index # 427\n",
      "done for ticker CPT index # 428\n",
      "done for ticker LW index # 429\n",
      "done for ticker PKG index # 430\n",
      "done for ticker CRL index # 431\n",
      "done for ticker SWK index # 432\n",
      "done for ticker BIO index # 433\n",
      "done for ticker WDC index # 434\n",
      "done for ticker CHRW index # 435\n",
      "done for ticker STX index # 436\n",
      "done for ticker MAS index # 437\n",
      "done for ticker GL index # 438\n",
      "done for ticker CE index # 439\n",
      "done for ticker REG index # 440\n",
      "done for ticker NI index # 441\n",
      "done for ticker BXP index # 442\n",
      "done for ticker HSIC index # 443\n",
      "done for ticker CCL index # 444\n",
      "done for ticker TFX index # 445\n",
      "done for ticker CZR index # 446\n",
      "done for ticker NWS index # 447\n",
      "done for ticker NWSA index # 448\n",
      "done for ticker KMX index # 449\n",
      "done for ticker EMN index # 450\n",
      "done for ticker JNPR index # 451\n",
      "done for ticker PHM index # 452\n",
      "done for ticker CDAY index # 453\n",
      "done for ticker ALLE index # 454\n",
      "done for ticker QRVO index # 455\n",
      "done for ticker BWA index # 456\n",
      "done for ticker NRG index # 457\n",
      "done for ticker MKTX index # 458\n",
      "done for ticker WRK index # 459\n",
      "done for ticker UHS index # 460\n",
      "done for ticker FFIV index # 461\n",
      "done for ticker AOS index # 462\n",
      "done for ticker CMA index # 463\n",
      "done for ticker AAL index # 464\n",
      "done for ticker BBWI index # 465\n",
      "done for ticker HII index # 466\n",
      "done for ticker AAP index # 467\n",
      "done for ticker TPR index # 468\n",
      "done for ticker FRT index # 469\n",
      "done for ticker IVZ index # 470\n",
      "done for ticker PNW index # 471\n",
      "done for ticker HAS index # 472\n",
      "done for ticker WYNN index # 473\n",
      "done for ticker FBHS index # 474\n",
      "done for ticker SBNY index # 475\n",
      "done for ticker DISH index # 476\n",
      "done for ticker RHI index # 477\n",
      "done for ticker WHR index # 478\n",
      "done for ticker ZION index # 479\n",
      "done for ticker CTLT index # 480\n",
      "done for ticker PNR index # 481\n",
      "done for ticker SEE index # 482\n",
      "done for ticker RL index # 483\n",
      "done for ticker NCLH index # 484\n",
      "done for ticker DXC index # 485\n",
      "done for ticker GNRC index # 486\n",
      "done for ticker AIZ index # 487\n",
      "done for ticker XRAY index # 488\n",
      "done for ticker LNC index # 489\n",
      "done for ticker DVA index # 490\n",
      "done for ticker MHK index # 491\n",
      "done for ticker LUMN index # 492\n",
      "done for ticker ALK index # 493\n",
      "done for ticker NWL index # 494\n",
      "done for ticker VNO index # 495\n",
      "done for ticker TAP index # 496\n"
     ]
    }
   ],
   "source": [
    "pca_dict_score = {}\n",
    "tickers = utils.return_tickers()\n",
    "for ticker in tickers:\n",
    "    pca = PCA(n_components=0.9)\n",
    "    ticker_df = prep.ready_to_train_df(ticker)\n",
    "    pca.fit(ticker_df)\n",
    "    n_pcs= pca.n_components_\n",
    "    most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "    initial_feature_names = ticker_df.columns\n",
    "    most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "    pca_df = ticker_df[list(np.unique(most_important_names))] \n",
    "    returns = ticker_df[['return']]\n",
    "    final_pca = pd.merge(pca_df, returns, how='outer', left_index=True, right_index=True)\n",
    "    pca_dict_score[ticker] = cross_validate_ml(final_pca, GradientBoostingRegressor(max_depth = 3, \n",
    "                                                                                    criterion='friedman_mse',\n",
    "                                                                                    learning_rate=0.08,\n",
    "                                                                                    n_estimators = 100))\n",
    "    print(f\"done for ticker {ticker} index # {tickers.index(ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6bdf029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:31:28.459421Z",
     "start_time": "2022-12-06T11:31:28.426417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0033498792960944923 0.02096529727516623\n"
     ]
    }
   ],
   "source": [
    "rmse_pca= []\n",
    "baseline = []\n",
    "for key in pca_dict_score.keys():\n",
    "    rmse_pca.append(pca_dict_score[key][0])\n",
    "    baseline.append(pca_dict_score[key][1])\n",
    "print(np.mean(rmse_pca), np.mean(baseline) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c29d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:44:19.570067Z",
     "start_time": "2022-12-02T08:44:19.401956Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f203d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:44:22.236582Z",
     "start_time": "2022-12-02T08:44:22.123704Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.ylim(ymin=0)\n",
    "plt.title('cumulated share of explained variance')\n",
    "plt.xlabel('# of principal component used');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559acad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T08:21:43.359186Z",
     "start_time": "2022-12-06T08:21:42.741327Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dynamic_portfolio.preprocess as prep\n",
    "import dynamic_portfolio.utils as utils\n",
    "import dynamic_portfolio.cross_validate as cv\n",
    "import dynamic_portfolio.models as model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import warnings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings(action='ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f573a850",
   "metadata": {},
   "source": [
    "### Cross validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f0601391",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T15:20:14.608657Z",
     "start_time": "2022-12-06T15:20:14.576866Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "cross_val = {\n",
    "    'fold_length' : 252, # Working days for 1 year\n",
    "    'fold_stride' : 60, # Step between folds, here one quarter\n",
    "    'train_test_ratio' : 0.7, # Split in fold\n",
    "    'input_length' : 0, # Number of days to move back from last train_index, here 0\n",
    "    'horizon' : 1, # Number of days ahead to make prediction, here 1\n",
    "    'output_length' : 1, # Number of targets wanted\n",
    "}\n",
    "#Split the dataset by FOLDS\n",
    "def get_folds(\n",
    "    df: pd.DataFrame,\n",
    "    fold_length = cross_val['fold_length'],\n",
    "    fold_stride = cross_val['fold_stride']):\n",
    "    '''\n",
    "    This function slides through the Time Series dataframe of shape (n_timesteps, n_features) to create folds\n",
    "    - of equal `fold_length`\n",
    "    - using `fold_stride` between each fold\n",
    "    Returns a list of folds, each as a DataFrame\n",
    "    '''\n",
    "    folds = []\n",
    "    for idx in range(0, len(df), fold_stride):\n",
    "        # Exits the loop as soon as the last fold index would exceed the last index\n",
    "        if (idx + fold_length) > len(df):\n",
    "            break\n",
    "        fold = df.iloc[idx:idx + fold_length, :]\n",
    "        folds.append(fold)\n",
    "    return folds\n",
    "#Split FOLDS by Train et Test\n",
    "#### FOR ONE FOLD !!!!!\n",
    "def train_test_split(fold: pd.DataFrame,\n",
    "                     train_test_ratio = cross_val['train_test_ratio'],\n",
    "                     input_length = cross_val['input_length']):\n",
    "    '''\n",
    "    Returns a train dataframe and a test dataframe (fold_train, fold_test)\n",
    "    from which one can sample (X,y) sequences.\n",
    "    df_train should contain all the timesteps until round(train_test_ratio * len(fold))\n",
    "    '''\n",
    "    # TRAIN SET\n",
    "    # ======================\n",
    "    last_train_idx = round(train_test_ratio * len(fold))\n",
    "    fold_train = fold.iloc[0:last_train_idx, :]\n",
    "    # TEST SET\n",
    "    # ======================\n",
    "    first_test_idx = last_train_idx - input_length \n",
    "    fold_test = fold.iloc[first_test_idx:, :]\n",
    "    return (fold_train, fold_test)\n",
    "def cross_validate_ml(df, model) :\n",
    "    '''\n",
    "    get_folds() create many FOLDS, train_test_split() create a split on ONE FOLDS.\n",
    "    The goal of this function is to make splits and sequences on each FOLDS.\n",
    "    Then, apply a model.\n",
    "    '''\n",
    "    folds = get_folds(df, fold_length = cross_val['fold_length'], fold_stride = cross_val['fold_stride']) # 1 - Creating FOLDS\n",
    "    scores =[]\n",
    "    baseline = []\n",
    "    for fold in folds:\n",
    "        # 2 - CHRONOLOGICAL TRAIN TEST SPLIT of the current FOLD\n",
    "        (fold_train, fold_test) = train_test_split(fold = fold,\n",
    "                                                train_test_ratio = cross_val['train_test_ratio'],\n",
    "                                                input_length = cross_val['input_length'] ,\n",
    "                                                )\n",
    "        # 3 - Scanninng fold_train and fold_test for SEQUENCES\n",
    "        X_train, y_train = fold_train, fold_train[['return']].shift(1).replace(np.nan,0)\n",
    "        X_test, y_test = fold_test, fold_test[['return']].shift(1).replace(np.nan,0)\n",
    "        model.fit(X_train, y_train)\n",
    "        rmse_model = (mean_squared_error(y_test, model.predict(X_test)))**0.5\n",
    "        scores.append(rmse_model)\n",
    "        rmse_baseline = mean_squared_error(y_test.iloc[[0]], y_train.iloc[[-1]])**0.5\n",
    "        baseline.append(rmse_baseline)\n",
    "    return np.mean(scores), np.mean(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cfc3a1",
   "metadata": {},
   "source": [
    "### Script to run for model on all stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6722fad",
   "metadata": {},
   "source": [
    "### Models used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5627a6af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:44:15.680313Z",
     "start_time": "2022-12-06T09:44:15.669340Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e6b64aaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T15:41:11.669513Z",
     "start_time": "2022-12-06T15:20:19.187283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker AAPL index # 0\n",
      "done for ticker MSFT index # 1\n",
      "done for ticker GOOG index # 2\n",
      "done for ticker AMZN index # 3\n",
      "done for ticker TSLA index # 4\n",
      "done for ticker UNH index # 5\n",
      "done for ticker XOM index # 6\n",
      "done for ticker JNJ index # 7\n",
      "done for ticker WMT index # 8\n",
      "done for ticker NVDA index # 9\n",
      "done for ticker JPM index # 10\n",
      "done for ticker V index # 11\n",
      "done for ticker CVX index # 12\n",
      "done for ticker PG index # 13\n",
      "done for ticker LLY index # 14\n",
      "done for ticker MA index # 15\n",
      "done for ticker HD index # 16\n",
      "done for ticker META index # 17\n",
      "done for ticker BAC index # 18\n",
      "done for ticker ABBV index # 19\n",
      "done for ticker PFE index # 20\n",
      "done for ticker KO index # 21\n",
      "done for ticker MRK index # 22\n",
      "done for ticker PEP index # 23\n",
      "done for ticker COST index # 24\n",
      "done for ticker ORCL index # 25\n",
      "done for ticker AVGO index # 26\n",
      "done for ticker TMO index # 27\n",
      "done for ticker MCD index # 28\n",
      "done for ticker CSCO index # 29\n",
      "done for ticker ACN index # 30\n",
      "done for ticker DHR index # 31\n",
      "done for ticker TMUS index # 32\n",
      "done for ticker ABT index # 33\n",
      "done for ticker WFC index # 34\n",
      "done for ticker DIS index # 35\n",
      "done for ticker LIN index # 36\n",
      "done for ticker NEE index # 37\n",
      "done for ticker BMY index # 38\n",
      "done for ticker NKE index # 39\n",
      "done for ticker VZ index # 40\n",
      "done for ticker TXN index # 41\n",
      "done for ticker UPS index # 42\n",
      "done for ticker COP index # 43\n",
      "done for ticker ADBE index # 44\n",
      "done for ticker CMCSA index # 45\n",
      "done for ticker CRM index # 46\n",
      "done for ticker PM index # 47\n",
      "done for ticker MS index # 48\n",
      "done for ticker AMGN index # 49\n",
      "done for ticker SCHW index # 50\n",
      "done for ticker HON index # 51\n",
      "done for ticker RTX index # 52\n",
      "done for ticker QCOM index # 53\n",
      "done for ticker T index # 54\n",
      "done for ticker IBM index # 55\n",
      "done for ticker DE index # 56\n",
      "done for ticker CVS index # 57\n",
      "done for ticker LOW index # 58\n",
      "done for ticker GS index # 59\n",
      "done for ticker UNP index # 60\n",
      "done for ticker NFLX index # 61\n",
      "done for ticker LMT index # 62\n",
      "done for ticker CAT index # 63\n",
      "done for ticker AMD index # 64\n",
      "done for ticker INTC index # 65\n",
      "done for ticker ELV index # 66\n",
      "done for ticker SPGI index # 67\n",
      "done for ticker AXP index # 68\n",
      "done for ticker SBUX index # 69\n",
      "done for ticker INTU index # 70\n",
      "done for ticker BLK index # 71\n",
      "done for ticker ADP index # 72\n",
      "done for ticker GILD index # 73\n",
      "done for ticker PLD index # 74\n",
      "done for ticker MDT index # 75\n",
      "done for ticker BA index # 76\n",
      "done for ticker AMT index # 77\n",
      "done for ticker CI index # 78\n",
      "done for ticker GE index # 79\n",
      "done for ticker TJX index # 80\n",
      "done for ticker ISRG index # 81\n",
      "done for ticker C index # 82\n",
      "done for ticker AMAT index # 83\n",
      "done for ticker PYPL index # 84\n",
      "done for ticker MDLZ index # 85\n",
      "done for ticker CB index # 86\n",
      "done for ticker SYK index # 87\n",
      "done for ticker ADI index # 88\n",
      "done for ticker MMC index # 89\n",
      "done for ticker EOG index # 90\n",
      "done for ticker NOW index # 91\n",
      "done for ticker VRTX index # 92\n",
      "done for ticker MO index # 93\n",
      "done for ticker NOC index # 94\n",
      "done for ticker EL index # 95\n",
      "done for ticker REGN index # 96\n",
      "done for ticker PGR index # 97\n",
      "done for ticker BKNG index # 98\n",
      "done for ticker DUK index # 99\n",
      "done for ticker TGT index # 100\n",
      "done for ticker SLB index # 101\n",
      "done for ticker SO index # 102\n",
      "done for ticker MMM index # 103\n",
      "done for ticker ITW index # 104\n",
      "done for ticker ZTS index # 105\n",
      "done for ticker GD index # 106\n",
      "done for ticker APD index # 107\n",
      "done for ticker HUM index # 108\n",
      "done for ticker MRNA index # 109\n",
      "done for ticker BDX index # 110\n",
      "done for ticker CSX index # 111\n",
      "done for ticker WM index # 112\n",
      "done for ticker PNC index # 113\n",
      "done for ticker HCA index # 114\n",
      "done for ticker ETN index # 115\n",
      "done for ticker USB index # 116\n",
      "done for ticker FISV index # 117\n",
      "done for ticker SHW index # 118\n",
      "done for ticker OXY index # 119\n",
      "done for ticker CL index # 120\n",
      "done for ticker MU index # 121\n",
      "done for ticker CME index # 122\n",
      "done for ticker AON index # 123\n",
      "done for ticker LRCX index # 124\n",
      "done for ticker BSX index # 125\n",
      "done for ticker EQIX index # 126\n",
      "done for ticker TFC index # 127\n",
      "done for ticker PXD index # 128\n",
      "done for ticker CHTR index # 129\n",
      "done for ticker CCI index # 130\n",
      "done for ticker MET index # 131\n",
      "done for ticker ATVI index # 132\n",
      "done for ticker ICE index # 133\n",
      "done for ticker MPC index # 134\n",
      "done for ticker NSC index # 135\n",
      "done for ticker DG index # 136\n",
      "done for ticker GM index # 137\n",
      "done for ticker EMR index # 138\n",
      "done for ticker F index # 139\n",
      "done for ticker KLAC index # 140\n",
      "done for ticker MCO index # 141\n",
      "done for ticker FCX index # 142\n",
      "done for ticker KDP index # 143\n",
      "done for ticker MNST index # 144\n",
      "done for ticker MCK index # 145\n",
      "done for ticker VLO index # 146\n",
      "done for ticker ORLY index # 147\n",
      "done for ticker ADM index # 148\n",
      "done for ticker PSX index # 149\n",
      "done for ticker PSA index # 150\n",
      "done for ticker SRE index # 151\n",
      "done for ticker SNPS index # 152\n",
      "done for ticker MAR index # 153\n",
      "done for ticker D index # 154\n",
      "done for ticker GIS index # 155\n",
      "done for ticker AEP index # 156\n",
      "done for ticker AZO index # 157\n",
      "done for ticker KHC index # 158\n",
      "done for ticker APH index # 159\n",
      "done for ticker HSY index # 160\n",
      "done for ticker CNC index # 161\n",
      "done for ticker CTVA index # 162\n",
      "done for ticker EW index # 163\n",
      "done for ticker CTAS index # 164\n",
      "done for ticker A index # 165\n",
      "done for ticker ROP index # 166\n",
      "done for ticker JCI index # 167\n",
      "done for ticker CDNS index # 168\n",
      "done for ticker FDX index # 169\n",
      "done for ticker NXPI index # 170\n",
      "done for ticker AIG index # 171\n",
      "done for ticker KMB index # 172\n",
      "done for ticker AFL index # 173\n",
      "done for ticker HES index # 174\n",
      "done for ticker MSI index # 175\n",
      "done for ticker PAYX index # 176\n",
      "done for ticker DVN index # 177\n",
      "done for ticker TRV index # 178\n",
      "done for ticker BIIB index # 179\n",
      "done for ticker DXCM index # 180\n",
      "done for ticker SYY index # 181\n",
      "done for ticker LHX index # 182\n",
      "done for ticker RSG index # 183\n",
      "done for ticker ENPH index # 184\n",
      "done for ticker ECL index # 185\n",
      "done for ticker ADSK index # 186\n",
      "done for ticker MCHP index # 187\n",
      "done for ticker ANET index # 188\n",
      "done for ticker KMI index # 189\n",
      "done for ticker CMG index # 190\n",
      "done for ticker FTNT index # 191\n",
      "done for ticker AJG index # 192\n",
      "done for ticker STZ index # 193\n",
      "done for ticker TT index # 194\n",
      "done for ticker WMB index # 195\n",
      "done for ticker MSCI index # 196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tickers \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mreturn_tickers()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m tickers:\n\u001b[0;32m----> 4\u001b[0m         dict_score[ticker] \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate_ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mready_to_train_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXGBRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone for ticker \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m index # \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtickers\u001b[38;5;241m.\u001b[39mindex(ticker)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[138], line 68\u001b[0m, in \u001b[0;36mcross_validate_ml\u001b[0;34m(df, model)\u001b[0m\n\u001b[1;32m     66\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m fold_train, fold_train[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(np\u001b[38;5;241m.\u001b[39mnan,\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     67\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m fold_test, fold_test[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(np\u001b[38;5;241m.\u001b[39mnan,\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m rmse_model \u001b[38;5;241m=\u001b[39m (mean_squared_error(y_test, model\u001b[38;5;241m.\u001b[39mpredict(X_test)))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     70\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(rmse_model)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/xgboost/sklearn.py:1051\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m (\n\u001b[1;32m   1043\u001b[0m     model,\n\u001b[1;32m   1044\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1049\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1050\u001b[0m )\n\u001b[0;32m-> 1051\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dict_score = {}\n",
    "tickers = utils.return_tickers()\n",
    "for ticker in tickers:\n",
    "        dict_score[ticker] = cross_validate_ml(prep.ready_to_train_df(ticker), XGBRegressor())\n",
    "        print(f\"done for ticker {ticker} index # {tickers.index(ticker)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3fa441",
   "metadata": {},
   "source": [
    "### Cross val scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9518ae94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:31:13.692301Z",
     "start_time": "2022-12-06T10:31:13.665214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004796076311898096 0.02096529727516623\n"
     ]
    }
   ],
   "source": [
    "rmse= []\n",
    "baseline = []\n",
    "for key in dict_score.keys():\n",
    "    rmse.append(dict_score[key][0])\n",
    "    baseline.append(dict_score[key][1])\n",
    "print(np.mean(rmse), np.mean(baseline) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015156ca",
   "metadata": {},
   "source": [
    "## Custom grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493abd9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-05T15:25:40.338767Z",
     "start_time": "2022-12-05T15:25:40.312234Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depth = [2, 5, 8]\n",
    "criterion = ['friedman_mse', 'squared_error', 'mse']\n",
    "learning_rate=[0.01, 0.1, 0.2] \n",
    "n_estimators=[100, 200, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d42f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "380e8dd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:19:16.229421Z",
     "start_time": "2022-12-06T09:19:16.199027Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_gridsearch(df, model, max_depth=[2,3,4], criterion = ['friedman_mse', 'squared_error', 'mse'], n_estimator=[50, 75, 100], learning_rate=[0.08, 0.1, 0.12], loss=['squared_error', 'absolute_error', 'huber']):\n",
    "    counter = 0\n",
    "    rmse = []\n",
    "    baseline = []\n",
    "    params = []\n",
    "    for max_depth_i in max_depth:\n",
    "        for criterion_i in criterion:\n",
    "            for n_estimator_i in n_estimator:\n",
    "                for learning_rate_i in learning_rate:\n",
    "                    for loss_i in loss:\n",
    "                        test = cross_validate_ml(df = df, model = model(max_depth=max_depth_i,\n",
    "                                                                   criterion = criterion_i,\n",
    "                                                                   n_estimators = n_estimator_i,\n",
    "                                                                   learning_rate = learning_rate_i,\n",
    "                                                                   loss = loss_i))\n",
    "                        rmse.append(test[0])\n",
    "                        baseline.append(test[1])\n",
    "                        params.append((max_depth_i, criterion_i, n_estimator_i, learning_rate_i))\n",
    "                        counter += 1\n",
    "                        print(f'model {counter} done with parameters: max_depth = {max_depth_i}, criterion = {criterion_i}, estimators = {n_estimator_i}, learning rate = {learning_rate_i}, loss = {loss_i}, rmse = {test[0]}')\n",
    "    idx_min = np.argmin(rmse)\n",
    "    best_params = params[idx_min]\n",
    "    \n",
    "    return best_params, rmse, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea5f65",
   "metadata": {},
   "source": [
    "### Model results on one stock on a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b79cdd0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:44:15.668227Z",
     "start_time": "2022-12-06T09:19:29.718526Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002958185553002535\n",
      "model 2 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.006407018407676127\n",
      "model 3 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.004428068812946768\n",
      "model 4 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.002776729479836416\n",
      "model 5 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.005372760873905239\n",
      "model 6 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.003432291380610513\n",
      "model 7 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.0026067464673304175\n",
      "model 8 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.0048061652599137505\n",
      "model 9 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030851908081413497\n",
      "model 10 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0026039764209378246\n",
      "model 11 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.00477931059886155\n",
      "model 12 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.003132758828900319\n",
      "model 13 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0027137581776954262\n",
      "model 14 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0042761812198638905\n",
      "model 15 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.002879525183277261\n",
      "model 16 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.002675563304797706\n",
      "model 17 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.004136672056708204\n",
      "model 18 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0028028874014932675\n",
      "model 19 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.00263901736188292\n",
      "model 20 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.004243777202286242\n",
      "model 21 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0028285475219040047\n",
      "model 22 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0026600684459853496\n",
      "model 23 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003929768715246046\n",
      "model 24 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.002738377789027572\n",
      "model 25 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.0026049910482777846\n",
      "model 26 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0038342459402143712\n",
      "model 27 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0027842983111944423\n",
      "model 28 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002946016742644336\n",
      "model 29 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.006400529273369599\n",
      "model 30 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.004428402988000523\n",
      "model 31 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.00274092287895698\n",
      "model 32 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.005369459505880697\n",
      "model 33 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0034536502342627138\n",
      "model 34 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.0026201916948474895\n",
      "model 35 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.00481556253048331\n",
      "model 36 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030731467770259437\n",
      "model 37 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0026640675291778127\n",
      "model 38 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004781983704751462\n",
      "model 39 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0031020685649265803\n",
      "model 40 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.002641066825947065\n",
      "model 41 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.004317992084797235\n",
      "model 42 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0028619443825098193\n",
      "model 43 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0025911692981691096\n",
      "model 44 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.004111814141492845\n",
      "model 45 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.002768306658624108\n",
      "model 46 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.0026757573258833324\n",
      "model 47 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.004186948590438916\n",
      "model 48 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0028185742191962665\n",
      "model 49 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.002659309412779239\n",
      "model 50 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003949072152121796\n",
      "model 51 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0027222261058379204\n",
      "model 52 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002705998569738018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 53 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0038499484792597865\n",
      "model 54 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.002795539577918935\n",
      "model 55 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.0029099191595345383\n",
      "model 56 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.006407991680854807\n",
      "model 57 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.004428402988000523\n",
      "model 58 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0028373111145722184\n",
      "model 59 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.005372474799411161\n",
      "model 60 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0034288454838289927\n",
      "model 61 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.0025617175515441487\n",
      "model 62 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.0048133561585675414\n",
      "model 63 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030763780454744144\n",
      "model 64 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0026350705093279\n",
      "model 65 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004773342022038409\n",
      "model 66 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0031281830813940513\n",
      "model 67 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0026786321052186756\n",
      "model 68 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.004333966762045556\n",
      "model 69 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.002845647774894737\n",
      "model 70 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0026488261570874246\n",
      "model 71 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0040720219215136045\n",
      "model 72 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0027838512151926002\n",
      "model 73 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.0026434954217452755\n",
      "model 74 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.004142942835776003\n",
      "model 75 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.002837433961437816\n",
      "model 76 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0026015436713270864\n",
      "model 77 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003915021453609067\n",
      "model 78 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.002733796570629045\n",
      "model 79 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002590710250606715\n",
      "model 80 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.003836455439472309\n",
      "model 81 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0028310156316547074\n",
      "model 82 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002705103403928138\n",
      "model 83 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.005055674970022903\n",
      "model 84 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.003942692273722824\n",
      "model 85 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0025979372722968975\n",
      "model 86 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.004457668793358413\n",
      "model 87 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0032871980973670925\n",
      "model 88 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002639942667455135\n",
      "model 89 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.004204336452061385\n",
      "model 90 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030337239385201673\n",
      "model 91 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0025862176325708685\n",
      "model 92 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.00413893352920339\n",
      "model 93 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0030566455793911897\n",
      "model 94 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0025916442432565383\n",
      "model 95 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.00392530690915287\n",
      "model 96 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.002810890632321019\n",
      "model 97 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0026127529836289937\n",
      "model 98 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0038031379441063966\n",
      "model 99 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.002786888933712554\n",
      "model 100 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.002557329832721479\n",
      "model 101 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.003849198727467321\n",
      "model 102 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0028084278300910463\n",
      "model 103 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0025990298794970937\n",
      "model 104 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.0037592214174426305\n",
      "model 105 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0026895703301677075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 106 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.0025730424155033494\n",
      "model 107 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0036681061284255722\n",
      "model 108 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0027251777091245326\n",
      "model 109 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002702793151349547\n",
      "model 110 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.005058232367804183\n",
      "model 111 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.003941806328485692\n",
      "model 112 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.002588842026698095\n",
      "model 113 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.004427805540319038\n",
      "model 114 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.003268000952219227\n",
      "model 115 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002579248928301906\n",
      "model 116 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.004172891100709107\n",
      "model 117 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030140686492261992\n",
      "model 118 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.002613291387439508\n",
      "model 119 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004139953071470512\n",
      "model 120 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.003080924745809816\n",
      "model 121 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0026145555792715857\n",
      "model 122 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.003921975417612286\n",
      "model 123 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0027913629970269323\n",
      "model 124 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.002587307414146831\n",
      "model 125 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.003799259214174869\n",
      "model 126 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0027115296373352686\n",
      "model 127 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.002601336330933987\n",
      "model 128 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.0038066571748408824\n",
      "model 129 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.002808894661718752\n",
      "model 130 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0026127988262543223\n",
      "model 131 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.0037446865076462914\n",
      "model 132 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.002685108521693579\n",
      "model 133 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002634867846287722\n",
      "model 134 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.003733008379825462\n",
      "model 135 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0026192326431160044\n",
      "model 136 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002724326707987458\n",
      "model 137 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.005044860223031934\n",
      "model 138 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.0039340807041176136\n",
      "model 139 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0026069569351037753\n",
      "model 140 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.0044098313370142415\n",
      "model 141 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.003304580254359149\n",
      "model 142 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002652285999047112\n",
      "model 143 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.00422005058828855\n",
      "model 144 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.002941688447540913\n",
      "model 145 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.002650852313756527\n",
      "model 146 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.0041723361975041715\n",
      "model 147 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0030481177907805516\n",
      "model 148 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.002647907064336234\n",
      "model 149 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0039274727502588164\n",
      "model 150 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.002783644662704949\n",
      "model 151 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0026850857188837887\n",
      "model 152 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0038454450583371423\n",
      "model 153 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0027657041578346194\n",
      "model 154 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.0025795467664902786\n",
      "model 155 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.003925606315613257\n",
      "model 156 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0028148348736183842\n",
      "model 157 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0026357231847853823\n",
      "model 158 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003745041132907287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 159 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.002699105329518561\n",
      "model 160 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002601949405101594\n",
      "model 161 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0036977284985453603\n",
      "model 162 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0027447041132625722\n",
      "model 163 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002758204626328357\n",
      "model 164 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.004962315891742368\n",
      "model 165 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.003755602715526272\n",
      "model 166 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0028201793905030946\n",
      "model 167 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.004377564048505937\n",
      "model 168 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.003315656914245619\n",
      "model 169 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.0027571244856977664\n",
      "model 170 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.004088070021363021\n",
      "model 171 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.003059518646427285\n",
      "model 172 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0027493575860221735\n",
      "model 173 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004037023468753866\n",
      "model 174 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.003122135588575402\n",
      "model 175 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0027805589220566613\n",
      "model 176 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0038261411672781322\n",
      "model 177 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0028689319120608105\n",
      "model 178 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0027474853227340396\n",
      "model 179 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0037876683726842096\n",
      "model 180 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0028343406482770153\n",
      "model 181 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.0027827520136940245\n",
      "model 182 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.0038275716196665426\n",
      "model 183 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0030061031913663073\n",
      "model 184 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0028641024564482363\n",
      "model 185 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003675794027779795\n",
      "model 186 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0028617470584144803\n",
      "model 187 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.0027819085919339732\n",
      "model 188 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0037322326772521604\n",
      "model 189 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.002859199248754659\n",
      "model 190 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.0028216246715976863\n",
      "model 191 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.0049191235829356104\n",
      "model 192 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.0037387215313132395\n",
      "model 193 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.002843667545519211\n",
      "model 194 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.00439015237329631\n",
      "model 195 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0033074000213341066\n",
      "model 196 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002730438251678527\n",
      "model 197 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.0040828874531489215\n",
      "model 198 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030158147085443373\n",
      "model 199 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0027748639528769456\n",
      "model 200 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004081739919150697\n",
      "model 201 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0031098377803180006\n",
      "model 202 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0028109126274234626\n",
      "model 203 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0038678451510520445\n",
      "model 204 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0029069862124411865\n",
      "model 205 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0028351433629591177\n",
      "model 206 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.003825465067210109\n",
      "model 207 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0028548669489826795\n",
      "model 208 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.002779061332540999\n",
      "model 209 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.0037924076379733714\n",
      "model 210 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0029269825240330802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 211 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.002822949349514258\n",
      "model 212 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003671004260923169\n",
      "model 213 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0027898015141627564\n",
      "model 214 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002789152927054379\n",
      "model 215 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.003691289937380604\n",
      "model 216 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0028131115634149615\n",
      "model 217 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002799139551941181\n",
      "model 218 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.004973818289138408\n",
      "model 219 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.003748542432319123\n",
      "model 220 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0027927302306963797\n",
      "model 221 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.004335859131408847\n",
      "model 222 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0033200876306249266\n",
      "model 223 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002807844523165585\n",
      "model 224 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.004042848200822164\n",
      "model 225 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030667195280845664\n",
      "model 226 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0027740729107799005\n",
      "model 227 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004034942483539463\n",
      "model 228 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.003147541038843053\n",
      "model 229 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.002807550771254751\n",
      "model 230 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0038697693960322646\n",
      "model 231 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0028860769585803145\n",
      "model 232 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.00277620080412421\n",
      "model 233 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0038790739000200787\n",
      "model 234 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.002832208965267212\n",
      "model 235 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.002767648470364058\n",
      "model 236 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.003728316443935388\n",
      "model 237 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0029278226365026107\n",
      "model 238 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.002827328916762123\n",
      "model 239 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.0037037792842529284\n",
      "model 240 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0028500206611104984\n",
      "model 241 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.0027272080473247054\n",
      "model 242 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.00378886572935585\n",
      "model 243 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0027762487865231975\n"
     ]
    }
   ],
   "source": [
    "gs_IP = custom_gridsearch(prep.ready_to_train_df('IP'), model = GradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da915412",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:34:38.067317Z",
     "start_time": "2022-12-06T10:34:38.029878Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 'friedman_mse', 100, 0.08),\n",
       " [0.002958185553002535,\n",
       "  0.006407018407676127,\n",
       "  0.004428068812946768,\n",
       "  0.002776729479836416,\n",
       "  0.005372760873905239,\n",
       "  0.003432291380610513,\n",
       "  0.0026067464673304175,\n",
       "  0.0048061652599137505,\n",
       "  0.0030851908081413497,\n",
       "  0.0026039764209378246,\n",
       "  0.00477931059886155,\n",
       "  0.003132758828900319,\n",
       "  0.0027137581776954262,\n",
       "  0.0042761812198638905,\n",
       "  0.002879525183277261,\n",
       "  0.002675563304797706,\n",
       "  0.004136672056708204,\n",
       "  0.0028028874014932675,\n",
       "  0.00263901736188292,\n",
       "  0.004243777202286242,\n",
       "  0.0028285475219040047,\n",
       "  0.0026600684459853496,\n",
       "  0.003929768715246046,\n",
       "  0.002738377789027572,\n",
       "  0.0026049910482777846,\n",
       "  0.0038342459402143712,\n",
       "  0.0027842983111944423,\n",
       "  0.002946016742644336,\n",
       "  0.006400529273369599,\n",
       "  0.004428402988000523,\n",
       "  0.00274092287895698,\n",
       "  0.005369459505880697,\n",
       "  0.0034536502342627138,\n",
       "  0.0026201916948474895,\n",
       "  0.00481556253048331,\n",
       "  0.0030731467770259437,\n",
       "  0.0026640675291778127,\n",
       "  0.004781983704751462,\n",
       "  0.0031020685649265803,\n",
       "  0.002641066825947065,\n",
       "  0.004317992084797235,\n",
       "  0.0028619443825098193,\n",
       "  0.0025911692981691096,\n",
       "  0.004111814141492845,\n",
       "  0.002768306658624108,\n",
       "  0.0026757573258833324,\n",
       "  0.004186948590438916,\n",
       "  0.0028185742191962665,\n",
       "  0.002659309412779239,\n",
       "  0.003949072152121796,\n",
       "  0.0027222261058379204,\n",
       "  0.002705998569738018,\n",
       "  0.0038499484792597865,\n",
       "  0.002795539577918935,\n",
       "  0.0029099191595345383,\n",
       "  0.006407991680854807,\n",
       "  0.004428402988000523,\n",
       "  0.0028373111145722184,\n",
       "  0.005372474799411161,\n",
       "  0.0034288454838289927,\n",
       "  0.0025617175515441487,\n",
       "  0.0048133561585675414,\n",
       "  0.0030763780454744144,\n",
       "  0.0026350705093279,\n",
       "  0.004773342022038409,\n",
       "  0.0031281830813940513,\n",
       "  0.0026786321052186756,\n",
       "  0.004333966762045556,\n",
       "  0.002845647774894737,\n",
       "  0.0026488261570874246,\n",
       "  0.0040720219215136045,\n",
       "  0.0027838512151926002,\n",
       "  0.0026434954217452755,\n",
       "  0.004142942835776003,\n",
       "  0.002837433961437816,\n",
       "  0.0026015436713270864,\n",
       "  0.003915021453609067,\n",
       "  0.002733796570629045,\n",
       "  0.002590710250606715,\n",
       "  0.003836455439472309,\n",
       "  0.0028310156316547074,\n",
       "  0.002705103403928138,\n",
       "  0.005055674970022903,\n",
       "  0.003942692273722824,\n",
       "  0.0025979372722968975,\n",
       "  0.004457668793358413,\n",
       "  0.0032871980973670925,\n",
       "  0.002639942667455135,\n",
       "  0.004204336452061385,\n",
       "  0.0030337239385201673,\n",
       "  0.0025862176325708685,\n",
       "  0.00413893352920339,\n",
       "  0.0030566455793911897,\n",
       "  0.0025916442432565383,\n",
       "  0.00392530690915287,\n",
       "  0.002810890632321019,\n",
       "  0.0026127529836289937,\n",
       "  0.0038031379441063966,\n",
       "  0.002786888933712554,\n",
       "  0.002557329832721479,\n",
       "  0.003849198727467321,\n",
       "  0.0028084278300910463,\n",
       "  0.0025990298794970937,\n",
       "  0.0037592214174426305,\n",
       "  0.0026895703301677075,\n",
       "  0.0025730424155033494,\n",
       "  0.0036681061284255722,\n",
       "  0.0027251777091245326,\n",
       "  0.002702793151349547,\n",
       "  0.005058232367804183,\n",
       "  0.003941806328485692,\n",
       "  0.002588842026698095,\n",
       "  0.004427805540319038,\n",
       "  0.003268000952219227,\n",
       "  0.002579248928301906,\n",
       "  0.004172891100709107,\n",
       "  0.0030140686492261992,\n",
       "  0.002613291387439508,\n",
       "  0.004139953071470512,\n",
       "  0.003080924745809816,\n",
       "  0.0026145555792715857,\n",
       "  0.003921975417612286,\n",
       "  0.0027913629970269323,\n",
       "  0.002587307414146831,\n",
       "  0.003799259214174869,\n",
       "  0.0027115296373352686,\n",
       "  0.002601336330933987,\n",
       "  0.0038066571748408824,\n",
       "  0.002808894661718752,\n",
       "  0.0026127988262543223,\n",
       "  0.0037446865076462914,\n",
       "  0.002685108521693579,\n",
       "  0.002634867846287722,\n",
       "  0.003733008379825462,\n",
       "  0.0026192326431160044,\n",
       "  0.002724326707987458,\n",
       "  0.005044860223031934,\n",
       "  0.0039340807041176136,\n",
       "  0.0026069569351037753,\n",
       "  0.0044098313370142415,\n",
       "  0.003304580254359149,\n",
       "  0.002652285999047112,\n",
       "  0.00422005058828855,\n",
       "  0.002941688447540913,\n",
       "  0.002650852313756527,\n",
       "  0.0041723361975041715,\n",
       "  0.0030481177907805516,\n",
       "  0.002647907064336234,\n",
       "  0.0039274727502588164,\n",
       "  0.002783644662704949,\n",
       "  0.0026850857188837887,\n",
       "  0.0038454450583371423,\n",
       "  0.0027657041578346194,\n",
       "  0.0025795467664902786,\n",
       "  0.003925606315613257,\n",
       "  0.0028148348736183842,\n",
       "  0.0026357231847853823,\n",
       "  0.003745041132907287,\n",
       "  0.002699105329518561,\n",
       "  0.002601949405101594,\n",
       "  0.0036977284985453603,\n",
       "  0.0027447041132625722,\n",
       "  0.002758204626328357,\n",
       "  0.004962315891742368,\n",
       "  0.003755602715526272,\n",
       "  0.0028201793905030946,\n",
       "  0.004377564048505937,\n",
       "  0.003315656914245619,\n",
       "  0.0027571244856977664,\n",
       "  0.004088070021363021,\n",
       "  0.003059518646427285,\n",
       "  0.0027493575860221735,\n",
       "  0.004037023468753866,\n",
       "  0.003122135588575402,\n",
       "  0.0027805589220566613,\n",
       "  0.0038261411672781322,\n",
       "  0.0028689319120608105,\n",
       "  0.0027474853227340396,\n",
       "  0.0037876683726842096,\n",
       "  0.0028343406482770153,\n",
       "  0.0027827520136940245,\n",
       "  0.0038275716196665426,\n",
       "  0.0030061031913663073,\n",
       "  0.0028641024564482363,\n",
       "  0.003675794027779795,\n",
       "  0.0028617470584144803,\n",
       "  0.0027819085919339732,\n",
       "  0.0037322326772521604,\n",
       "  0.002859199248754659,\n",
       "  0.0028216246715976863,\n",
       "  0.0049191235829356104,\n",
       "  0.0037387215313132395,\n",
       "  0.002843667545519211,\n",
       "  0.00439015237329631,\n",
       "  0.0033074000213341066,\n",
       "  0.002730438251678527,\n",
       "  0.0040828874531489215,\n",
       "  0.0030158147085443373,\n",
       "  0.0027748639528769456,\n",
       "  0.004081739919150697,\n",
       "  0.0031098377803180006,\n",
       "  0.0028109126274234626,\n",
       "  0.0038678451510520445,\n",
       "  0.0029069862124411865,\n",
       "  0.0028351433629591177,\n",
       "  0.003825465067210109,\n",
       "  0.0028548669489826795,\n",
       "  0.002779061332540999,\n",
       "  0.0037924076379733714,\n",
       "  0.0029269825240330802,\n",
       "  0.002822949349514258,\n",
       "  0.003671004260923169,\n",
       "  0.0027898015141627564,\n",
       "  0.002789152927054379,\n",
       "  0.003691289937380604,\n",
       "  0.0028131115634149615,\n",
       "  0.002799139551941181,\n",
       "  0.004973818289138408,\n",
       "  0.003748542432319123,\n",
       "  0.0027927302306963797,\n",
       "  0.004335859131408847,\n",
       "  0.0033200876306249266,\n",
       "  0.002807844523165585,\n",
       "  0.004042848200822164,\n",
       "  0.0030667195280845664,\n",
       "  0.0027740729107799005,\n",
       "  0.004034942483539463,\n",
       "  0.003147541038843053,\n",
       "  0.002807550771254751,\n",
       "  0.0038697693960322646,\n",
       "  0.0028860769585803145,\n",
       "  0.00277620080412421,\n",
       "  0.0038790739000200787,\n",
       "  0.002832208965267212,\n",
       "  0.002767648470364058,\n",
       "  0.003728316443935388,\n",
       "  0.0029278226365026107,\n",
       "  0.002827328916762123,\n",
       "  0.0037037792842529284,\n",
       "  0.0028500206611104984,\n",
       "  0.0027272080473247054,\n",
       "  0.00378886572935585,\n",
       "  0.0027762487865231975],\n",
       " [(2, 'friedman_mse', 50, 0.08),\n",
       "  (2, 'friedman_mse', 50, 0.08),\n",
       "  (2, 'friedman_mse', 50, 0.08),\n",
       "  (2, 'friedman_mse', 50, 0.1),\n",
       "  (2, 'friedman_mse', 50, 0.1),\n",
       "  (2, 'friedman_mse', 50, 0.1),\n",
       "  (2, 'friedman_mse', 50, 0.12),\n",
       "  (2, 'friedman_mse', 50, 0.12),\n",
       "  (2, 'friedman_mse', 50, 0.12),\n",
       "  (2, 'friedman_mse', 75, 0.08),\n",
       "  (2, 'friedman_mse', 75, 0.08),\n",
       "  (2, 'friedman_mse', 75, 0.08),\n",
       "  (2, 'friedman_mse', 75, 0.1),\n",
       "  (2, 'friedman_mse', 75, 0.1),\n",
       "  (2, 'friedman_mse', 75, 0.1),\n",
       "  (2, 'friedman_mse', 75, 0.12),\n",
       "  (2, 'friedman_mse', 75, 0.12),\n",
       "  (2, 'friedman_mse', 75, 0.12),\n",
       "  (2, 'friedman_mse', 100, 0.08),\n",
       "  (2, 'friedman_mse', 100, 0.08),\n",
       "  (2, 'friedman_mse', 100, 0.08),\n",
       "  (2, 'friedman_mse', 100, 0.1),\n",
       "  (2, 'friedman_mse', 100, 0.1),\n",
       "  (2, 'friedman_mse', 100, 0.1),\n",
       "  (2, 'friedman_mse', 100, 0.12),\n",
       "  (2, 'friedman_mse', 100, 0.12),\n",
       "  (2, 'friedman_mse', 100, 0.12),\n",
       "  (2, 'squared_error', 50, 0.08),\n",
       "  (2, 'squared_error', 50, 0.08),\n",
       "  (2, 'squared_error', 50, 0.08),\n",
       "  (2, 'squared_error', 50, 0.1),\n",
       "  (2, 'squared_error', 50, 0.1),\n",
       "  (2, 'squared_error', 50, 0.1),\n",
       "  (2, 'squared_error', 50, 0.12),\n",
       "  (2, 'squared_error', 50, 0.12),\n",
       "  (2, 'squared_error', 50, 0.12),\n",
       "  (2, 'squared_error', 75, 0.08),\n",
       "  (2, 'squared_error', 75, 0.08),\n",
       "  (2, 'squared_error', 75, 0.08),\n",
       "  (2, 'squared_error', 75, 0.1),\n",
       "  (2, 'squared_error', 75, 0.1),\n",
       "  (2, 'squared_error', 75, 0.1),\n",
       "  (2, 'squared_error', 75, 0.12),\n",
       "  (2, 'squared_error', 75, 0.12),\n",
       "  (2, 'squared_error', 75, 0.12),\n",
       "  (2, 'squared_error', 100, 0.08),\n",
       "  (2, 'squared_error', 100, 0.08),\n",
       "  (2, 'squared_error', 100, 0.08),\n",
       "  (2, 'squared_error', 100, 0.1),\n",
       "  (2, 'squared_error', 100, 0.1),\n",
       "  (2, 'squared_error', 100, 0.1),\n",
       "  (2, 'squared_error', 100, 0.12),\n",
       "  (2, 'squared_error', 100, 0.12),\n",
       "  (2, 'squared_error', 100, 0.12),\n",
       "  (2, 'mse', 50, 0.08),\n",
       "  (2, 'mse', 50, 0.08),\n",
       "  (2, 'mse', 50, 0.08),\n",
       "  (2, 'mse', 50, 0.1),\n",
       "  (2, 'mse', 50, 0.1),\n",
       "  (2, 'mse', 50, 0.1),\n",
       "  (2, 'mse', 50, 0.12),\n",
       "  (2, 'mse', 50, 0.12),\n",
       "  (2, 'mse', 50, 0.12),\n",
       "  (2, 'mse', 75, 0.08),\n",
       "  (2, 'mse', 75, 0.08),\n",
       "  (2, 'mse', 75, 0.08),\n",
       "  (2, 'mse', 75, 0.1),\n",
       "  (2, 'mse', 75, 0.1),\n",
       "  (2, 'mse', 75, 0.1),\n",
       "  (2, 'mse', 75, 0.12),\n",
       "  (2, 'mse', 75, 0.12),\n",
       "  (2, 'mse', 75, 0.12),\n",
       "  (2, 'mse', 100, 0.08),\n",
       "  (2, 'mse', 100, 0.08),\n",
       "  (2, 'mse', 100, 0.08),\n",
       "  (2, 'mse', 100, 0.1),\n",
       "  (2, 'mse', 100, 0.1),\n",
       "  (2, 'mse', 100, 0.1),\n",
       "  (2, 'mse', 100, 0.12),\n",
       "  (2, 'mse', 100, 0.12),\n",
       "  (2, 'mse', 100, 0.12),\n",
       "  (3, 'friedman_mse', 50, 0.08),\n",
       "  (3, 'friedman_mse', 50, 0.08),\n",
       "  (3, 'friedman_mse', 50, 0.08),\n",
       "  (3, 'friedman_mse', 50, 0.1),\n",
       "  (3, 'friedman_mse', 50, 0.1),\n",
       "  (3, 'friedman_mse', 50, 0.1),\n",
       "  (3, 'friedman_mse', 50, 0.12),\n",
       "  (3, 'friedman_mse', 50, 0.12),\n",
       "  (3, 'friedman_mse', 50, 0.12),\n",
       "  (3, 'friedman_mse', 75, 0.08),\n",
       "  (3, 'friedman_mse', 75, 0.08),\n",
       "  (3, 'friedman_mse', 75, 0.08),\n",
       "  (3, 'friedman_mse', 75, 0.1),\n",
       "  (3, 'friedman_mse', 75, 0.1),\n",
       "  (3, 'friedman_mse', 75, 0.1),\n",
       "  (3, 'friedman_mse', 75, 0.12),\n",
       "  (3, 'friedman_mse', 75, 0.12),\n",
       "  (3, 'friedman_mse', 75, 0.12),\n",
       "  (3, 'friedman_mse', 100, 0.08),\n",
       "  (3, 'friedman_mse', 100, 0.08),\n",
       "  (3, 'friedman_mse', 100, 0.08),\n",
       "  (3, 'friedman_mse', 100, 0.1),\n",
       "  (3, 'friedman_mse', 100, 0.1),\n",
       "  (3, 'friedman_mse', 100, 0.1),\n",
       "  (3, 'friedman_mse', 100, 0.12),\n",
       "  (3, 'friedman_mse', 100, 0.12),\n",
       "  (3, 'friedman_mse', 100, 0.12),\n",
       "  (3, 'squared_error', 50, 0.08),\n",
       "  (3, 'squared_error', 50, 0.08),\n",
       "  (3, 'squared_error', 50, 0.08),\n",
       "  (3, 'squared_error', 50, 0.1),\n",
       "  (3, 'squared_error', 50, 0.1),\n",
       "  (3, 'squared_error', 50, 0.1),\n",
       "  (3, 'squared_error', 50, 0.12),\n",
       "  (3, 'squared_error', 50, 0.12),\n",
       "  (3, 'squared_error', 50, 0.12),\n",
       "  (3, 'squared_error', 75, 0.08),\n",
       "  (3, 'squared_error', 75, 0.08),\n",
       "  (3, 'squared_error', 75, 0.08),\n",
       "  (3, 'squared_error', 75, 0.1),\n",
       "  (3, 'squared_error', 75, 0.1),\n",
       "  (3, 'squared_error', 75, 0.1),\n",
       "  (3, 'squared_error', 75, 0.12),\n",
       "  (3, 'squared_error', 75, 0.12),\n",
       "  (3, 'squared_error', 75, 0.12),\n",
       "  (3, 'squared_error', 100, 0.08),\n",
       "  (3, 'squared_error', 100, 0.08),\n",
       "  (3, 'squared_error', 100, 0.08),\n",
       "  (3, 'squared_error', 100, 0.1),\n",
       "  (3, 'squared_error', 100, 0.1),\n",
       "  (3, 'squared_error', 100, 0.1),\n",
       "  (3, 'squared_error', 100, 0.12),\n",
       "  (3, 'squared_error', 100, 0.12),\n",
       "  (3, 'squared_error', 100, 0.12),\n",
       "  (3, 'mse', 50, 0.08),\n",
       "  (3, 'mse', 50, 0.08),\n",
       "  (3, 'mse', 50, 0.08),\n",
       "  (3, 'mse', 50, 0.1),\n",
       "  (3, 'mse', 50, 0.1),\n",
       "  (3, 'mse', 50, 0.1),\n",
       "  (3, 'mse', 50, 0.12),\n",
       "  (3, 'mse', 50, 0.12),\n",
       "  (3, 'mse', 50, 0.12),\n",
       "  (3, 'mse', 75, 0.08),\n",
       "  (3, 'mse', 75, 0.08),\n",
       "  (3, 'mse', 75, 0.08),\n",
       "  (3, 'mse', 75, 0.1),\n",
       "  (3, 'mse', 75, 0.1),\n",
       "  (3, 'mse', 75, 0.1),\n",
       "  (3, 'mse', 75, 0.12),\n",
       "  (3, 'mse', 75, 0.12),\n",
       "  (3, 'mse', 75, 0.12),\n",
       "  (3, 'mse', 100, 0.08),\n",
       "  (3, 'mse', 100, 0.08),\n",
       "  (3, 'mse', 100, 0.08),\n",
       "  (3, 'mse', 100, 0.1),\n",
       "  (3, 'mse', 100, 0.1),\n",
       "  (3, 'mse', 100, 0.1),\n",
       "  (3, 'mse', 100, 0.12),\n",
       "  (3, 'mse', 100, 0.12),\n",
       "  (3, 'mse', 100, 0.12),\n",
       "  (4, 'friedman_mse', 50, 0.08),\n",
       "  (4, 'friedman_mse', 50, 0.08),\n",
       "  (4, 'friedman_mse', 50, 0.08),\n",
       "  (4, 'friedman_mse', 50, 0.1),\n",
       "  (4, 'friedman_mse', 50, 0.1),\n",
       "  (4, 'friedman_mse', 50, 0.1),\n",
       "  (4, 'friedman_mse', 50, 0.12),\n",
       "  (4, 'friedman_mse', 50, 0.12),\n",
       "  (4, 'friedman_mse', 50, 0.12),\n",
       "  (4, 'friedman_mse', 75, 0.08),\n",
       "  (4, 'friedman_mse', 75, 0.08),\n",
       "  (4, 'friedman_mse', 75, 0.08),\n",
       "  (4, 'friedman_mse', 75, 0.1),\n",
       "  (4, 'friedman_mse', 75, 0.1),\n",
       "  (4, 'friedman_mse', 75, 0.1),\n",
       "  (4, 'friedman_mse', 75, 0.12),\n",
       "  (4, 'friedman_mse', 75, 0.12),\n",
       "  (4, 'friedman_mse', 75, 0.12),\n",
       "  (4, 'friedman_mse', 100, 0.08),\n",
       "  (4, 'friedman_mse', 100, 0.08),\n",
       "  (4, 'friedman_mse', 100, 0.08),\n",
       "  (4, 'friedman_mse', 100, 0.1),\n",
       "  (4, 'friedman_mse', 100, 0.1),\n",
       "  (4, 'friedman_mse', 100, 0.1),\n",
       "  (4, 'friedman_mse', 100, 0.12),\n",
       "  (4, 'friedman_mse', 100, 0.12),\n",
       "  (4, 'friedman_mse', 100, 0.12),\n",
       "  (4, 'squared_error', 50, 0.08),\n",
       "  (4, 'squared_error', 50, 0.08),\n",
       "  (4, 'squared_error', 50, 0.08),\n",
       "  (4, 'squared_error', 50, 0.1),\n",
       "  (4, 'squared_error', 50, 0.1),\n",
       "  (4, 'squared_error', 50, 0.1),\n",
       "  (4, 'squared_error', 50, 0.12),\n",
       "  (4, 'squared_error', 50, 0.12),\n",
       "  (4, 'squared_error', 50, 0.12),\n",
       "  (4, 'squared_error', 75, 0.08),\n",
       "  (4, 'squared_error', 75, 0.08),\n",
       "  (4, 'squared_error', 75, 0.08),\n",
       "  (4, 'squared_error', 75, 0.1),\n",
       "  (4, 'squared_error', 75, 0.1),\n",
       "  (4, 'squared_error', 75, 0.1),\n",
       "  (4, 'squared_error', 75, 0.12),\n",
       "  (4, 'squared_error', 75, 0.12),\n",
       "  (4, 'squared_error', 75, 0.12),\n",
       "  (4, 'squared_error', 100, 0.08),\n",
       "  (4, 'squared_error', 100, 0.08),\n",
       "  (4, 'squared_error', 100, 0.08),\n",
       "  (4, 'squared_error', 100, 0.1),\n",
       "  (4, 'squared_error', 100, 0.1),\n",
       "  (4, 'squared_error', 100, 0.1),\n",
       "  (4, 'squared_error', 100, 0.12),\n",
       "  (4, 'squared_error', 100, 0.12),\n",
       "  (4, 'squared_error', 100, 0.12),\n",
       "  (4, 'mse', 50, 0.08),\n",
       "  (4, 'mse', 50, 0.08),\n",
       "  (4, 'mse', 50, 0.08),\n",
       "  (4, 'mse', 50, 0.1),\n",
       "  (4, 'mse', 50, 0.1),\n",
       "  (4, 'mse', 50, 0.1),\n",
       "  (4, 'mse', 50, 0.12),\n",
       "  (4, 'mse', 50, 0.12),\n",
       "  (4, 'mse', 50, 0.12),\n",
       "  (4, 'mse', 75, 0.08),\n",
       "  (4, 'mse', 75, 0.08),\n",
       "  (4, 'mse', 75, 0.08),\n",
       "  (4, 'mse', 75, 0.1),\n",
       "  (4, 'mse', 75, 0.1),\n",
       "  (4, 'mse', 75, 0.1),\n",
       "  (4, 'mse', 75, 0.12),\n",
       "  (4, 'mse', 75, 0.12),\n",
       "  (4, 'mse', 75, 0.12),\n",
       "  (4, 'mse', 100, 0.08),\n",
       "  (4, 'mse', 100, 0.08),\n",
       "  (4, 'mse', 100, 0.08),\n",
       "  (4, 'mse', 100, 0.1),\n",
       "  (4, 'mse', 100, 0.1),\n",
       "  (4, 'mse', 100, 0.1),\n",
       "  (4, 'mse', 100, 0.12),\n",
       "  (4, 'mse', 100, 0.12),\n",
       "  (4, 'mse', 100, 0.12)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17e244cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:46:22.145038Z",
     "start_time": "2022-12-06T09:46:22.116797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018969723709451568"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84210e27",
   "metadata": {},
   "source": [
    "### Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e55b2a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:35:35.451616Z",
     "start_time": "2022-12-06T11:35:35.419318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=0.9)),\n",
       "                (&#x27;gradientboostingregressor&#x27;,\n",
       "                 GradientBoostingRegressor(max_depth=2))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=0.9)),\n",
       "                (&#x27;gradientboostingregressor&#x27;,\n",
       "                 GradientBoostingRegressor(max_depth=2))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=0.9)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=2)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('pca', PCA(n_components=0.9)),\n",
       "                ('gradientboostingregressor',\n",
       "                 GradientBoostingRegressor(max_depth=2))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(PCA(), GradientBoostingRegressor())\n",
    "\n",
    "params = {'pca__n_components':0.9,\n",
    "                'gradientboostingregressor__max_depth':,\n",
    "                'gradientboostingregressor__criterion':'friedman_mse',\n",
    "                'gradientboostingregressor__n_estimators':100,\n",
    "                'gradientboostingregressor__learning_rate':0.1}\n",
    "pipe.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ad6c2e11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T15:41:21.283565Z",
     "start_time": "2022-12-06T15:41:21.257663Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_pipe = make_pipeline(PCA(), XGBRegressor())\n",
    "\n",
    "params = {'pca__n_components':0.9,\n",
    "                'xgbregressor__max_depth': 5,\n",
    "                'xgbregressor__max_leaves':0,\n",
    "                'xgbregressor__n_estimators':100,\n",
    "                'xgbregressor__learning_rate':0.1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "80d27c82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T16:14:56.139564Z",
     "start_time": "2022-12-06T16:14:56.116749Z"
    }
   },
   "outputs": [],
   "source": [
    "model = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "52842b00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T16:15:07.997777Z",
     "start_time": "2022-12-06T16:15:07.969239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'reg:squarederror',\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'callbacks': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'early_stopping_rounds': None,\n",
       " 'enable_categorical': False,\n",
       " 'eval_metric': None,\n",
       " 'feature_types': None,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'grow_policy': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_bin': None,\n",
       " 'max_cat_threshold': None,\n",
       " 'max_cat_to_onehot': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'max_leaves': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'predictor': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'sampling_method': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3cac4098",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T15:41:25.260584Z",
     "start_time": "2022-12-06T15:41:25.225886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=0.9)),\n",
       "                (&#x27;xgbregressor&#x27;,\n",
       "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                              colsample_bylevel=None, colsample_bynode=None,\n",
       "                              colsample_bytree=None, early_stopping_rounds=None,\n",
       "                              enable_categorical=False, eval_metric=None,\n",
       "                              feature_types=None, gamma=None, gpu_id=None,\n",
       "                              grow_policy=None, importance_type=None,\n",
       "                              interaction_constraints=None, learning_rate=0.1,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=None,\n",
       "                              max_depth=5, max_leaves=0, min_child_weight=None,\n",
       "                              missing=nan, monotone_constraints=None,\n",
       "                              n_estimators=100, n_jobs=None,\n",
       "                              num_parallel_tree=None, predictor=None,\n",
       "                              random_state=None, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=0.9)),\n",
       "                (&#x27;xgbregressor&#x27;,\n",
       "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                              colsample_bylevel=None, colsample_bynode=None,\n",
       "                              colsample_bytree=None, early_stopping_rounds=None,\n",
       "                              enable_categorical=False, eval_metric=None,\n",
       "                              feature_types=None, gamma=None, gpu_id=None,\n",
       "                              grow_policy=None, importance_type=None,\n",
       "                              interaction_constraints=None, learning_rate=0.1,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=None,\n",
       "                              max_depth=5, max_leaves=0, min_child_weight=None,\n",
       "                              missing=nan, monotone_constraints=None,\n",
       "                              n_estimators=100, n_jobs=None,\n",
       "                              num_parallel_tree=None, predictor=None,\n",
       "                              random_state=None, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=0.9)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=5, max_leaves=0,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('pca', PCA(n_components=0.9)),\n",
       "                ('xgbregressor',\n",
       "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                              colsample_bylevel=None, colsample_bynode=None,\n",
       "                              colsample_bytree=None, early_stopping_rounds=None,\n",
       "                              enable_categorical=False, eval_metric=None,\n",
       "                              feature_types=None, gamma=None, gpu_id=None,\n",
       "                              grow_policy=None, importance_type=None,\n",
       "                              interaction_constraints=None, learning_rate=0.1,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=None,\n",
       "                              max_depth=5, max_leaves=0, min_child_weight=None,\n",
       "                              missing=nan, monotone_constraints=None,\n",
       "                              n_estimators=100, n_jobs=None,\n",
       "                              num_parallel_tree=None, predictor=None,\n",
       "                              random_state=None, ...))])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_pipe.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51877cab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:49:38.402435Z",
     "start_time": "2022-12-06T11:49:38.378925Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e99f059e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:52:55.212467Z",
     "start_time": "2022-12-06T11:49:52.225946Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model AAPL index # 0 saved\n",
      "Model MSFT index # 1 saved\n",
      "Model GOOG index # 2 saved\n",
      "Model AMZN index # 3 saved\n",
      "Model TSLA index # 4 saved\n",
      "Model UNH index # 5 saved\n",
      "Model XOM index # 6 saved\n",
      "Model JNJ index # 7 saved\n",
      "Model WMT index # 8 saved\n",
      "Model NVDA index # 9 saved\n",
      "Model JPM index # 10 saved\n",
      "Model V index # 11 saved\n",
      "Model CVX index # 12 saved\n",
      "Model PG index # 13 saved\n",
      "Model LLY index # 14 saved\n",
      "Model MA index # 15 saved\n",
      "Model HD index # 16 saved\n",
      "Model META index # 17 saved\n",
      "Model BAC index # 18 saved\n",
      "Model ABBV index # 19 saved\n",
      "Model PFE index # 20 saved\n",
      "Model KO index # 21 saved\n",
      "Model MRK index # 22 saved\n",
      "Model PEP index # 23 saved\n",
      "Model COST index # 24 saved\n",
      "Model ORCL index # 25 saved\n",
      "Model AVGO index # 26 saved\n",
      "Model TMO index # 27 saved\n",
      "Model MCD index # 28 saved\n",
      "Model CSCO index # 29 saved\n",
      "Model ACN index # 30 saved\n",
      "Model DHR index # 31 saved\n",
      "Model TMUS index # 32 saved\n",
      "Model ABT index # 33 saved\n",
      "Model WFC index # 34 saved\n",
      "Model DIS index # 35 saved\n",
      "Model LIN index # 36 saved\n",
      "Model NEE index # 37 saved\n",
      "Model BMY index # 38 saved\n",
      "Model NKE index # 39 saved\n",
      "Model VZ index # 40 saved\n",
      "Model TXN index # 41 saved\n",
      "Model UPS index # 42 saved\n",
      "Model COP index # 43 saved\n",
      "Model ADBE index # 44 saved\n",
      "Model CMCSA index # 45 saved\n",
      "Model CRM index # 46 saved\n",
      "Model PM index # 47 saved\n",
      "Model MS index # 48 saved\n",
      "Model AMGN index # 49 saved\n",
      "Model SCHW index # 50 saved\n",
      "Model HON index # 51 saved\n",
      "Model RTX index # 52 saved\n",
      "Model QCOM index # 53 saved\n",
      "Model T index # 54 saved\n",
      "Model IBM index # 55 saved\n",
      "Model DE index # 56 saved\n",
      "Model CVS index # 57 saved\n",
      "Model LOW index # 58 saved\n",
      "Model GS index # 59 saved\n",
      "Model UNP index # 60 saved\n",
      "Model NFLX index # 61 saved\n",
      "Model LMT index # 62 saved\n",
      "Model CAT index # 63 saved\n",
      "Model AMD index # 64 saved\n",
      "Model INTC index # 65 saved\n",
      "Model ELV index # 66 saved\n",
      "Model SPGI index # 67 saved\n",
      "Model AXP index # 68 saved\n",
      "Model SBUX index # 69 saved\n",
      "Model INTU index # 70 saved\n",
      "Model BLK index # 71 saved\n",
      "Model ADP index # 72 saved\n",
      "Model GILD index # 73 saved\n",
      "Model PLD index # 74 saved\n",
      "Model MDT index # 75 saved\n",
      "Model BA index # 76 saved\n",
      "Model AMT index # 77 saved\n",
      "Model CI index # 78 saved\n",
      "Model GE index # 79 saved\n",
      "Model TJX index # 80 saved\n",
      "Model ISRG index # 81 saved\n",
      "Model C index # 82 saved\n",
      "Model AMAT index # 83 saved\n",
      "Model PYPL index # 84 saved\n",
      "Model MDLZ index # 85 saved\n",
      "Model CB index # 86 saved\n",
      "Model SYK index # 87 saved\n",
      "Model ADI index # 88 saved\n",
      "Model MMC index # 89 saved\n",
      "Model EOG index # 90 saved\n",
      "Model NOW index # 91 saved\n",
      "Model VRTX index # 92 saved\n",
      "Model MO index # 93 saved\n",
      "Model NOC index # 94 saved\n",
      "Model EL index # 95 saved\n",
      "Model REGN index # 96 saved\n",
      "Model PGR index # 97 saved\n",
      "Model BKNG index # 98 saved\n",
      "Model DUK index # 99 saved\n",
      "Model TGT index # 100 saved\n",
      "Model SLB index # 101 saved\n",
      "Model SO index # 102 saved\n",
      "Model MMM index # 103 saved\n",
      "Model ITW index # 104 saved\n",
      "Model ZTS index # 105 saved\n",
      "Model GD index # 106 saved\n",
      "Model APD index # 107 saved\n",
      "Model HUM index # 108 saved\n",
      "Model MRNA index # 109 saved\n",
      "Model BDX index # 110 saved\n",
      "Model CSX index # 111 saved\n",
      "Model WM index # 112 saved\n",
      "Model PNC index # 113 saved\n",
      "Model HCA index # 114 saved\n",
      "Model ETN index # 115 saved\n",
      "Model USB index # 116 saved\n",
      "Model FISV index # 117 saved\n",
      "Model SHW index # 118 saved\n",
      "Model OXY index # 119 saved\n",
      "Model CL index # 120 saved\n",
      "Model MU index # 121 saved\n",
      "Model CME index # 122 saved\n",
      "Model AON index # 123 saved\n",
      "Model LRCX index # 124 saved\n",
      "Model BSX index # 125 saved\n",
      "Model EQIX index # 126 saved\n",
      "Model TFC index # 127 saved\n",
      "Model PXD index # 128 saved\n",
      "Model CHTR index # 129 saved\n",
      "Model CCI index # 130 saved\n",
      "Model MET index # 131 saved\n",
      "Model ATVI index # 132 saved\n",
      "Model ICE index # 133 saved\n",
      "Model MPC index # 134 saved\n",
      "Model NSC index # 135 saved\n",
      "Model DG index # 136 saved\n",
      "Model GM index # 137 saved\n",
      "Model EMR index # 138 saved\n",
      "Model F index # 139 saved\n",
      "Model KLAC index # 140 saved\n",
      "Model MCO index # 141 saved\n",
      "Model FCX index # 142 saved\n",
      "Model KDP index # 143 saved\n",
      "Model MNST index # 144 saved\n",
      "Model MCK index # 145 saved\n",
      "Model VLO index # 146 saved\n",
      "Model ORLY index # 147 saved\n",
      "Model ADM index # 148 saved\n",
      "Model PSX index # 149 saved\n",
      "Model PSA index # 150 saved\n",
      "Model SRE index # 151 saved\n",
      "Model SNPS index # 152 saved\n",
      "Model MAR index # 153 saved\n",
      "Model D index # 154 saved\n",
      "Model GIS index # 155 saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m pipe\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m prep\u001b[38;5;241m.\u001b[39mready_to_train_df(ticker)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreturn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../raw_data/models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_GradientBoostingRegressor_PCA.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m index # \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtickers\u001b[38;5;241m.\u001b[39mindex(ticker)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/pipeline.py:382\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    381\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 382\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:668\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 668\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:745\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    738\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[1;32m    739\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    740\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    741\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    742\u001b[0m     )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:247\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    244\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    246\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 247\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[1;32m    250\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[1;32m    251\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[1;32m    252\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    260\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/tree/_classes.py:1342\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \n\u001b[1;32m   1316\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1342\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/tree/_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    449\u001b[0m         splitter,\n\u001b[1;32m    450\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    456\u001b[0m     )\n\u001b[0;32m--> 458\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tickers = utils.return_tickers()\n",
    "for ticker in tickers:\n",
    "    model = pipe\n",
    "    df = prep.ready_to_train_df(ticker)\n",
    "    model.fit(df, df['return'])\n",
    "    joblib.dump(model, f\"../raw_data/models/{ticker}_GradientBoostingRegressor_PCA.joblib\")\n",
    "    print(f\"Model {ticker} index # {tickers.index(ticker)} saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0fdcdbc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T17:14:20.431751Z",
     "start_time": "2022-12-06T16:45:54.375141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error for AAPL: 0.0010755018274289084, index # 0\n",
      "error for MSFT: 0.0014009360157847585, index # 1\n",
      "error for GOOG: 0.0019409268784759998, index # 2\n",
      "error for AMZN: 0.0011770144206575716, index # 3\n",
      "error for TSLA: 0.0038774516256775086, index # 4\n",
      "error for UNH: 0.0015803208162903399, index # 5\n",
      "error for XOM: 0.002412013922014089, index # 6\n",
      "error for JNJ: 0.0015723586058631182, index # 7\n",
      "error for WMT: 0.0009193086913061908, index # 8\n",
      "error for NVDA: 0.0015347427183583305, index # 9\n",
      "error for JPM: 0.0011790385822612536, index # 10\n",
      "error for V: 0.0010331803956665515, index # 11\n",
      "error for CVX: 0.005028838707639965, index # 12\n",
      "error for PG: 0.0014305292660018222, index # 13\n",
      "error for LLY: 0.0026292035991625595, index # 14\n",
      "error for MA: 0.0013321126960574535, index # 15\n",
      "error for HD: 0.002734689182004797, index # 16\n",
      "error for META: 0.005626137611098566, index # 17\n",
      "error for BAC: 0.0016069918195044475, index # 18\n",
      "error for ABBV: 0.0009326388735273637, index # 19\n",
      "error for PFE: 0.0010640602623979184, index # 20\n",
      "error for KO: 0.0017340305717208173, index # 21\n",
      "error for MRK: 0.0007504164638777156, index # 22\n",
      "error for PEP: 0.0014919000424494823, index # 23\n",
      "error for COST: 0.0014687580368580066, index # 24\n",
      "error for ORCL: 0.0019819219045512653, index # 25\n",
      "error for AVGO: 0.0017413447082608188, index # 26\n",
      "error for TMO: 0.0011546911368784762, index # 27\n",
      "error for MCD: 0.003659189331233199, index # 28\n",
      "error for CSCO: 0.0009412365828269017, index # 29\n",
      "error for ACN: 0.0009558352295064193, index # 30\n",
      "error for DHR: 0.0011404183958774656, index # 31\n",
      "error for TMUS: 0.0026382486755537602, index # 32\n",
      "error for ABT: 0.0014260439714197462, index # 33\n",
      "error for WFC: 0.0018326652396307797, index # 34\n",
      "error for DIS: 0.0023512259726867317, index # 35\n",
      "error for LIN: 0.0020037177695567602, index # 36\n",
      "error for NEE: 0.0026000779051905654, index # 37\n",
      "error for BMY: 0.0008119000705026144, index # 38\n",
      "error for NKE: 0.002091774287828361, index # 39\n",
      "error for VZ: 0.0006450189359596275, index # 40\n",
      "error for TXN: 0.0009957393091499624, index # 41\n",
      "error for UPS: 0.0031204848013514073, index # 42\n",
      "error for COP: 0.005805354012324841, index # 43\n",
      "error for ADBE: 0.0020298757877304616, index # 44\n",
      "error for CMCSA: 0.0018128902905094188, index # 45\n",
      "error for CRM: 0.002768410437460641, index # 46\n",
      "error for PM: 0.0023220086208639613, index # 47\n",
      "error for MS: 0.0019252419931241472, index # 48\n",
      "error for AMGN: 0.00111167201636627, index # 49\n",
      "error for SCHW: 0.0023760614363580895, index # 50\n",
      "error for HON: 0.0014268582770898886, index # 51\n",
      "error for RTX: 0.0035978893744110026, index # 52\n",
      "error for QCOM: 0.0028225185443793814, index # 53\n",
      "error for T: 0.0018290584647360597, index # 54\n",
      "error for IBM: 0.001905419273443674, index # 55\n",
      "error for DE: 0.0010789854186291645, index # 56\n",
      "error for CVS: 0.001438023385761279, index # 57\n",
      "error for LOW: 0.004382667303041508, index # 58\n",
      "error for GS: 0.0023447309320128577, index # 59\n",
      "error for UNP: 0.002029851010920556, index # 60\n",
      "error for NFLX: 0.002288071131739176, index # 61\n",
      "error for LMT: 0.001564329054812243, index # 62\n",
      "error for CAT: 0.0020498826508747107, index # 63\n",
      "error for AMD: 0.002315227266127668, index # 64\n",
      "error for INTC: 0.001917945380778091, index # 65\n",
      "error for ELV: 0.003671158400408364, index # 66\n",
      "error for SPGI: 0.0016760800608328669, index # 67\n",
      "error for AXP: 0.001917246347453728, index # 68\n",
      "error for SBUX: 0.001856338232906096, index # 69\n",
      "error for INTU: 0.001105084308812084, index # 70\n",
      "error for BLK: 0.001604067469707407, index # 71\n",
      "error for ADP: 0.004252949989535096, index # 72\n",
      "error for GILD: 0.000896467121610457, index # 73\n",
      "error for PLD: 0.001618359130137314, index # 74\n",
      "error for MDT: 0.001100472952246733, index # 75\n",
      "error for BA: 0.00746002143500447, index # 76\n",
      "error for AMT: 0.0014822044534176665, index # 77\n",
      "error for CI: 0.0016528211310916057, index # 78\n",
      "error for GE: 0.002354164792076048, index # 79\n",
      "error for TJX: 0.003346230677571263, index # 80\n",
      "error for ISRG: 0.0018127058192287239, index # 81\n",
      "error for C: 0.002334606349554473, index # 82\n",
      "error for AMAT: 0.0026625538559459615, index # 83\n",
      "error for PYPL: 0.006618921960716061, index # 84\n",
      "error for MDLZ: 0.002156006053622694, index # 85\n",
      "error for CB: 0.0015096378866317327, index # 86\n",
      "error for SYK: 0.0014981773433326185, index # 87\n",
      "error for ADI: 0.0018611407933716796, index # 88\n",
      "error for MMC: 0.0011152589511035347, index # 89\n",
      "error for EOG: 0.006127031595390773, index # 90\n",
      "error for NOW: 0.0035192421105183624, index # 91\n",
      "error for VRTX: 0.001208482818701732, index # 92\n",
      "error for MO: 0.0009273527416056807, index # 93\n",
      "error for NOC: 0.001337983111647313, index # 94\n",
      "error for EL: 0.001695175712539175, index # 95\n",
      "error for REGN: 0.0011087506605553185, index # 96\n",
      "error for PGR: 0.001142733448684662, index # 97\n",
      "error for BKNG: 0.001276106949077176, index # 98\n",
      "error for DUK: 0.001515419914034176, index # 99\n",
      "error for TGT: 0.004379039042887269, index # 100\n",
      "error for SLB: 0.004696234798165475, index # 101\n",
      "error for SO: 0.004291218185404059, index # 102\n",
      "error for MMM: 0.0023434174142506646, index # 103\n",
      "error for ITW: 0.002228588596177506, index # 104\n",
      "error for ZTS: 0.0014084528925473515, index # 105\n",
      "error for GD: 0.0013387540059920891, index # 106\n",
      "error for APD: 0.0020550028446898994, index # 107\n",
      "error for HUM: 0.0017790435764759658, index # 108\n",
      "error for MRNA: 0.006778422033622207, index # 109\n",
      "error for BDX: 0.0014425682505759324, index # 110\n",
      "error for CSX: 0.001578298523680542, index # 111\n",
      "error for WM: 0.0009469169006928778, index # 112\n",
      "error for PNC: 0.002392139295759158, index # 113\n",
      "error for HCA: 0.002963912330293592, index # 114\n",
      "error for ETN: 0.004652507051542981, index # 115\n",
      "error for USB: 0.0015583495045547139, index # 116\n",
      "error for FISV: 0.0021553505049733126, index # 117\n",
      "error for SHW: 0.0015809642725970762, index # 118\n",
      "error for OXY: 0.012289025234890638, index # 119\n",
      "error for CL: 0.0014668944129641605, index # 120\n",
      "error for MU: 0.0014117903497776793, index # 121\n",
      "error for CME: 0.0014945864986616079, index # 122\n",
      "error for AON: 0.001335926004461117, index # 123\n",
      "error for LRCX: 0.0016549057669595908, index # 124\n",
      "error for BSX: 0.0012176333519961953, index # 125\n",
      "error for EQIX: 0.0011250302580737928, index # 126\n",
      "error for TFC: 0.0024364382821144785, index # 127\n",
      "error for PXD: 0.005982852155999896, index # 128\n",
      "error for CHTR: 0.0011494328925839418, index # 129\n",
      "error for CCI: 0.001519515748793994, index # 130\n",
      "error for MET: 0.0017137924839527576, index # 131\n",
      "error for ATVI: 0.0024646969143738823, index # 132\n",
      "error for ICE: 0.0014757329862792276, index # 133\n",
      "error for MPC: 0.0021786785737862454, index # 134\n",
      "error for NSC: 0.002946439765877604, index # 135\n",
      "error for DG: 0.00139322715153796, index # 136\n",
      "error for GM: 0.0017673013033827488, index # 137\n",
      "error for EMR: 0.002893192505804811, index # 138\n",
      "error for F: 0.0015343550881815045, index # 139\n",
      "error for KLAC: 0.0017283737999024568, index # 140\n",
      "error for MCO: 0.0015054548284338261, index # 141\n",
      "error for FCX: 0.0023585476085224944, index # 142\n",
      "error for KDP: 0.0046141264163413395, index # 143\n",
      "error for MNST: 0.001317427565265654, index # 144\n",
      "error for MCK: 0.0018956720673336262, index # 145\n",
      "error for VLO: 0.005346301214588482, index # 146\n",
      "error for ORLY: 0.001293985608488238, index # 147\n",
      "error for ADM: 0.0013371552272127747, index # 148\n",
      "error for PSX: 0.004171729378639571, index # 149\n",
      "error for PSA: 0.0010443441163109028, index # 150\n",
      "error for SRE: 0.0018602052246100597, index # 151\n",
      "error for SNPS: 0.0014535513608341794, index # 152\n",
      "error for MAR: 0.0028103246781501414, index # 153\n",
      "error for D: 0.002727198237999491, index # 154\n",
      "error for GIS: 0.0015004470803025147, index # 155\n",
      "error for AEP: 0.0016087106698966362, index # 156\n",
      "error for AZO: 0.0026582319565180046, index # 157\n",
      "error for KHC: 0.0012586149213472213, index # 158\n",
      "error for APH: 0.0010225795268530224, index # 159\n",
      "error for HSY: 0.0027136346035318806, index # 160\n",
      "error for CNC: 0.0018937769745441656, index # 161\n",
      "error for CTVA: 0.0026213534870486326, index # 162\n",
      "error for EW: 0.0015573579778804623, index # 163\n",
      "error for CTAS: 0.0027774163501971053, index # 164\n",
      "error for A: 0.001106667845130902, index # 165\n",
      "error for ROP: 0.001019104568304, index # 166\n",
      "error for JCI: 0.0025065503194806535, index # 167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error for CDNS: 0.0009306109222494366, index # 168\n",
      "error for FDX: 0.003141837195197346, index # 169\n",
      "error for NXPI: 0.002421501522406658, index # 170\n",
      "error for AIG: 0.0023818895623304217, index # 171\n",
      "error for KMB: 0.0011870997213360133, index # 172\n",
      "error for AFL: 0.0015386538538001131, index # 173\n",
      "error for HES: 0.0051275280423488615, index # 174\n",
      "error for MSI: 0.0010126066529906961, index # 175\n",
      "error for PAYX: 0.003564260010414954, index # 176\n",
      "error for DVN: 0.007553380266759235, index # 177\n",
      "error for TRV: 0.00233973862145062, index # 178\n",
      "error for BIIB: 0.012066896300401236, index # 179\n",
      "error for DXCM: 0.002018155665266986, index # 180\n",
      "error for SYY: 0.006622501566191941, index # 181\n",
      "error for LHX: 0.0009571668124520307, index # 182\n",
      "error for RSG: 0.0014645262585922052, index # 183\n",
      "error for ENPH: 0.0040303735406395395, index # 184\n",
      "error for ECL: 0.0035312480377572746, index # 185\n",
      "error for ADSK: 0.0013404593042538994, index # 186\n",
      "error for MCHP: 0.0036346027005981266, index # 187\n",
      "error for ANET: 0.0016956389163447184, index # 188\n",
      "error for KMI: 0.0018607424527717527, index # 189\n",
      "error for CMG: 0.0017985708518501193, index # 190\n",
      "error for FTNT: 0.0022716554887689646, index # 191\n",
      "error for AJG: 0.0015425463194652994, index # 192\n",
      "error for STZ: 0.0018033054125932868, index # 193\n",
      "error for TT: 0.0034900939582769345, index # 194\n",
      "error for WMB: 0.0010595841489739485, index # 195\n",
      "error for MSCI: 0.0024309294681453247, index # 196\n",
      "error for O: 0.004498404601623377, index # 197\n",
      "error for IQV: 0.001967601774052845, index # 198\n",
      "error for TEL: 0.0017798445205523914, index # 199\n",
      "error for ROST: 0.0057346161676133725, index # 200\n",
      "error for PRU: 0.0017476850290012668, index # 201\n",
      "error for EXC: 0.0020754703646554953, index # 202\n",
      "error for PH: 0.004205557939829201, index # 203\n",
      "error for FIS: 0.0035552968678856296, index # 204\n",
      "error for SPG: 0.0045897110442748084, index # 205\n",
      "error for COF: 0.0017105109761942176, index # 206\n",
      "error for NUE: 0.0012385347088191524, index # 207\n",
      "error for XEL: 0.0013512002163486667, index # 208\n",
      "error for HLT: 0.001390902701743235, index # 209\n",
      "error for CARR: 0.005764202374804962, index # 210\n",
      "error for PCAR: 0.0011971193653905416, index # 211\n",
      "error for BK: 0.0012032205319768576, index # 212\n",
      "error for NEM: 0.0011560001636961442, index # 213\n",
      "error for DOW: 0.0022725286004598604, index # 214\n",
      "error for EA: 0.0011016455726694393, index # 215\n",
      "error for WBA: 0.0015683222321756798, index # 216\n",
      "error for DD: 0.004172547834147868, index # 217\n",
      "error for ALL: 0.0012149080802303673, index # 218\n",
      "error for YUM: 0.0037347422625154353, index # 219\n",
      "error for AMP: 0.003337612845107429, index # 220\n",
      "error for CMI: 0.0021579128523092706, index # 221\n",
      "error for ILMN: 0.0014640704193873682, index # 222\n",
      "error for TDG: 0.006114857158813016, index # 223\n",
      "error for IDXX: 0.0013951434088097779, index # 224\n",
      "error for ED: 0.004136318251312576, index # 225\n",
      "error for KR: 0.0014942899385619803, index # 226\n",
      "error for ABC: 0.0016914647919651854, index # 227\n",
      "error for DLTR: 0.003678575269227648, index # 228\n",
      "error for RMD: 0.0021330477441653213, index # 229\n",
      "error for ALB: 0.0034503766366011175, index # 230\n",
      "error for HAL: 0.0065618954992587705, index # 231\n",
      "error for NDAQ: 0.001511006212162762, index # 232\n",
      "error for LVS: 0.007191418491601045, index # 233\n",
      "error for ODFL: 0.001670835678532282, index # 234\n",
      "error for WELL: 0.006911860475575116, index # 235\n",
      "error for AME: 0.002659683258449409, index # 236\n",
      "error for CSGP: 0.0012223724040588194, index # 237\n",
      "error for OTIS: 0.004205186466211879, index # 238\n",
      "error for MTD: 0.0008823953373402352, index # 239\n",
      "error for SBAC: 0.001272633054593077, index # 240\n",
      "error for ON: 0.0023385449953278667, index # 241\n",
      "error for VICI: 0.004376954671593775, index # 242\n",
      "error for DLR: 0.0009149615502956391, index # 243\n",
      "error for KEYS: 0.001270570766002346, index # 244\n",
      "error for PPG: 0.0011799795221621278, index # 245\n",
      "error for WEC: 0.004627534037094355, index # 246\n",
      "error for CTSH: 0.0011687427659794098, index # 247\n",
      "error for ROK: 0.002090054780349697, index # 248\n",
      "error for GWW: 0.0016899147532146485, index # 249\n",
      "error for PCG: 0.018131345076619808, index # 250\n",
      "error for HPQ: 0.0017297005312329387, index # 251\n",
      "error for FAST: 0.0021495402207442247, index # 252\n",
      "error for DFS: 0.006547477744577713, index # 253\n",
      "error for MTB: 0.0030093427372458437, index # 254\n",
      "error for PEG: 0.0011431120585941625, index # 255\n",
      "error for OKE: 0.01078171905534149, index # 256\n",
      "error for DHI: 0.0013617671685193986, index # 257\n",
      "error for APTV: 0.0030614642527018216, index # 258\n",
      "error for BKR: 0.002781114715636688, index # 259\n",
      "error for GLW: 0.0017150513564982265, index # 260\n",
      "error for LYB: 0.0024602925538892415, index # 261\n",
      "error for ES: 0.0027665255569584183, index # 262\n",
      "error for BAX: 0.0023984053732072704, index # 263\n",
      "error for STT: 0.0020302243244622632, index # 264\n",
      "error for VRSK: 0.0015346727833435055, index # 265\n",
      "error for TROW: 0.0017664058623207846, index # 266\n",
      "error for WBD: 0.005559903329205698, index # 267\n",
      "error for AWK: 0.003996943536611306, index # 268\n",
      "error for IT: 0.0015070356467765326, index # 269\n",
      "error for GPN: 0.0023622264227450463, index # 270\n",
      "error for HRL: 0.0020136680006643845, index # 271\n",
      "error for FANG: 0.003949571864620418, index # 272\n",
      "error for WTW: 0.002202035640596279, index # 273\n",
      "error for RJF: 0.0013233561258014673, index # 274\n",
      "error for GPC: 0.004715734072199767, index # 275\n",
      "error for IFF: 0.0019599339806472215, index # 276\n",
      "error for CDW: 0.0011063483823699181, index # 277\n",
      "error for TSCO: 0.0008658049948532977, index # 278\n",
      "error for FITB: 0.002361069472571982, index # 279\n",
      "error for ARE: 0.0012009864363618423, index # 280\n",
      "error for URI: 0.0028957527410517444, index # 281\n",
      "error for ZBH: 0.002045320964941109, index # 282\n",
      "error for K: 0.002331675890412132, index # 283\n",
      "error for LEN: 0.0015206247810562686, index # 284\n",
      "error for EBAY: 0.0009660050573534808, index # 285\n",
      "error for EIX: 0.001899620857441084, index # 286\n",
      "error for CBRE: 0.0018213000678355643, index # 287\n",
      "error for EFX: 0.0017395097050527373, index # 288\n",
      "error for VMC: 0.0027454132290441694, index # 289\n",
      "error for TSN: 0.0021876100258627546, index # 290\n",
      "error for HIG: 0.001865844230482611, index # 291\n",
      "error for FTV: 0.0019028046701317063, index # 292\n",
      "error for WY: 0.005836633320723687, index # 293\n",
      "error for EQR: 0.0012018309441742135, index # 294\n",
      "error for AVB: 0.0012607979331666081, index # 295\n",
      "error for MKC: 0.0028312058401354736, index # 296\n",
      "error for ETR: 0.0030367066188094244, index # 297\n",
      "error for LUV: 0.00296627341789935, index # 298\n",
      "error for ULTA: 0.008897085720961942, index # 299\n",
      "error for AEE: 0.0023544752129385045, index # 300\n",
      "error for MLM: 0.0017118666462156839, index # 301\n",
      "error for FE: 0.004450180800525417, index # 302\n",
      "error for PFG: 0.0021832324071206326, index # 303\n",
      "error for FRC: 0.0017894871909900558, index # 304\n",
      "error for DTE: 0.003219909691099814, index # 305\n",
      "error for DAL: 0.0037461023910153298, index # 306\n",
      "error for HBAN: 0.0022848408352677835, index # 307\n",
      "error for IR: 0.0021782134473816543, index # 308\n",
      "error for CTRA: 0.0011920316287817435, index # 309\n",
      "error for ANSS: 0.0013644413122198844, index # 310\n",
      "error for ACGL: 0.004602764382950972, index # 311\n",
      "error for PPL: 0.0023910095251619494, index # 312\n",
      "error for RF: 0.002283252881441657, index # 313\n",
      "error for VRSN: 0.0009321310986174742, index # 314\n",
      "error for LH: 0.0029096679788211933, index # 315\n",
      "error for EXR: 0.0013257796650754422, index # 316\n",
      "error for PWR: 0.0017296355708386676, index # 317\n",
      "error for CF: 0.0017185030015679505, index # 318\n",
      "error for CAH: 0.0023986553771086364, index # 319\n",
      "error for CFG: 0.002639192454285237, index # 320\n",
      "error for XYL: 0.001322325032865392, index # 321\n",
      "error for HPE: 0.0022716831753876203, index # 322\n",
      "error for EPAM: 0.013768497590703201, index # 323\n",
      "error for DOV: 0.002581388923084388, index # 324\n",
      "error for WAT: 0.0010296985910589881, index # 325\n",
      "error for WRB: 0.0024287836979072076, index # 326\n",
      "error for TDY: 0.0023460436879897267, index # 327\n",
      "error for PAYC: 0.0030971481938333926, index # 328\n",
      "error for ROL: 0.0011187658504221358, index # 329\n",
      "error for NTRS: 0.002108690633318021, index # 330\n",
      "error for MRO: 0.008884346280074413, index # 331\n",
      "error for CNP: 0.004432619561625587, index # 332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error for INVH: 0.0033814237591178395, index # 333\n",
      "error for CHD: 0.001960683232209993, index # 334\n",
      "error for AES: 0.0014176316315629206, index # 335\n",
      "error for MOH: 0.0015946392145316486, index # 336\n",
      "error for JBHT: 0.0014121464326489123, index # 337\n",
      "error for MAA: 0.001390090270316626, index # 338\n",
      "error for BBY: 0.0013694127052625762, index # 339\n",
      "error for CLX: 0.0028657789507882305, index # 340\n",
      "error for HOLX: 0.0011757011953517677, index # 341\n",
      "error for WAB: 0.002787997094111809, index # 342\n",
      "error for DRI: 0.006918423690012474, index # 343\n",
      "error for EXPD: 0.0007835511775661293, index # 344\n",
      "error for STE: 0.0010398237947621808, index # 345\n",
      "error for AMCR: 0.0032137765491276715, index # 346\n",
      "error for VTR: 0.005184283366371841, index # 347\n",
      "error for IEX: 0.0008662654468679736, index # 348\n",
      "error for CAG: 0.0038375226668428473, index # 349\n",
      "error for CMS: 0.0011662443809519355, index # 350\n",
      "error for KEY: 0.004181719590782902, index # 351\n",
      "error for MPWR: 0.002399626460472851, index # 352\n",
      "error for BALL: 0.002875254034475451, index # 353\n",
      "error for J: 0.0010970072035128373, index # 354\n",
      "error for BR: 0.0012807758450349533, index # 355\n",
      "error for GRMN: 0.0008847296289417427, index # 356\n",
      "error for PKI: 0.0011660066857851195, index # 357\n",
      "error for TTWO: 0.0015257203123373675, index # 358\n",
      "error for INCY: 0.0011185918030325514, index # 359\n",
      "error for FDS: 0.0014194210963032726, index # 360\n",
      "error for MOS: 0.002376640724644732, index # 361\n",
      "error for SEDG: 0.002824881999238139, index # 362\n",
      "error for CINF: 0.0033402030129404078, index # 363\n",
      "error for ABMD: 0.0068918763620017635, index # 364\n",
      "error for DGX: 0.001249836973334336, index # 365\n",
      "error for WST: 0.0013711083016842887, index # 366\n",
      "error for ATO: 0.0020124297087282, index # 367\n",
      "error for TRGP: 0.0034215087714039747, index # 368\n",
      "error for BRO: 0.002219955232564299, index # 369\n",
      "error for SYF: 0.002506998592670407, index # 370\n",
      "error for FOX: 0.002391528445324226, index # 371\n",
      "error for FOXA: 0.0029030969178855306, index # 372\n",
      "error for NTAP: 0.0014838635700019983, index # 373\n",
      "error for FMC: 0.0016040528990871141, index # 374\n",
      "error for EQT: 0.008293397023764472, index # 375\n",
      "error for OMC: 0.0021382478888360332, index # 376\n",
      "error for SJM: 0.0012519627492913836, index # 377\n",
      "error for LYV: 0.0020347995492977678, index # 378\n",
      "error for CPB: 0.001634566505538851, index # 379\n",
      "error for HWM: 0.003612813130417244, index # 380\n",
      "error for CPRT: 0.0014031778895136513, index # 381\n",
      "error for AVY: 0.003707175757383917, index # 382\n",
      "error for IRM: 0.001262568128798167, index # 383\n",
      "error for COO: 0.0011209954964414496, index # 384\n",
      "error for ALGN: 0.0025721439677244174, index # 385\n",
      "error for SWKS: 0.0017752151137904908, index # 386\n",
      "error for EXPE: 0.003997660226778953, index # 387\n",
      "error for RCL: 0.0061207959467716845, index # 388\n",
      "error for ETSY: 0.00259962890689739, index # 389\n",
      "error for APA: 0.015681853273233456, index # 390\n",
      "error for GEN: 0.000997478075448159, index # 391\n",
      "error for TXT: 0.0019925220075362295, index # 392\n",
      "error for LDOS: 0.002651681752524215, index # 393\n",
      "error for LKQ: 0.004011890360952783, index # 394\n",
      "error for TER: 0.0021390402612459147, index # 395\n",
      "error for PTC: 0.001358973116240457, index # 396\n",
      "error for TRMB: 0.0026824022716962883, index # 397\n",
      "error for AKAM: 0.003614780702534856, index # 398\n",
      "error for NVR: 0.0041670061962065545, index # 399\n",
      "error for UAL: 0.003647214589185995, index # 400\n",
      "error for LNT: 0.001162707233114363, index # 401\n",
      "error for FLT: 0.00267296654382698, index # 402\n",
      "error for KIM: 0.0026340093750884023, index # 403\n",
      "error for ZBRA: 0.0014227777926369553, index # 404\n",
      "error for TYL: 0.00081603349124964, index # 405\n",
      "error for DPZ: 0.0018008172439856144, index # 406\n",
      "error for JKHY: 0.0014260308416153688, index # 407\n",
      "error for MGM: 0.0043807681430243035, index # 408\n",
      "error for ESS: 0.0017198184017981452, index # 409\n",
      "error for L: 0.0018638309913688222, index # 410\n",
      "error for PEAK: 0.0022558296534340226, index # 411\n",
      "error for MTCH: 0.0031798472762294473, index # 412\n",
      "error for NDSN: 0.0029394330791576726, index # 413\n",
      "error for EVRG: 0.0021969682104684934, index # 414\n",
      "error for IPG: 0.0010165171984380444, index # 415\n",
      "error for BEN: 0.0013439306886205082, index # 416\n",
      "error for CBOE: 0.001454310064761056, index # 417\n",
      "error for TECH: 0.0014724495592807964, index # 418\n",
      "error for SIVB: 0.002646506088341404, index # 419\n",
      "error for VFC: 0.002054434459154474, index # 420\n",
      "error for IP: 0.0017740289108754167, index # 421\n",
      "error for HST: 0.0023112320988072513, index # 422\n",
      "error for UDR: 0.0012469548743702745, index # 423\n",
      "error for POOL: 0.002927776359083105, index # 424\n",
      "error for RE: 0.0022208324668111525, index # 425\n",
      "error for PARA: 0.006445531723559998, index # 426\n",
      "error for SNA: 0.0011772113127153407, index # 427\n",
      "error for CPT: 0.0014198051765853376, index # 428\n",
      "error for LW: 0.0018291805043247962, index # 429\n",
      "error for PKG: 0.0012304573097295306, index # 430\n",
      "error for CRL: 0.0016444072525795706, index # 431\n",
      "error for SWK: 0.006394188157930928, index # 432\n",
      "error for BIO: 0.0011809013763158826, index # 433\n",
      "error for WDC: 0.0022605362610214927, index # 434\n",
      "error for CHRW: 0.0013861264918262367, index # 435\n",
      "error for STX: 0.001529617386217618, index # 436\n",
      "error for MAS: 0.001023075189578671, index # 437\n",
      "error for GL: 0.0015452284959017037, index # 438\n",
      "error for CE: 0.0019282082746170323, index # 439\n",
      "error for REG: 0.0057448617816806494, index # 440\n",
      "error for NI: 0.002905994595758565, index # 441\n",
      "error for BXP: 0.001782401406905332, index # 442\n",
      "error for HSIC: 0.0009675607629304299, index # 443\n",
      "error for CCL: 0.012870416893901494, index # 444\n",
      "error for TFX: 0.0012712182275770123, index # 445\n",
      "error for CZR: 0.006386479791893641, index # 446\n",
      "error for NWS: 0.0012963933312975292, index # 447\n",
      "error for NWSA: 0.0010191502399016336, index # 448\n",
      "error for KMX: 0.0028224923285840556, index # 449\n",
      "error for EMN: 0.0016847745613461874, index # 450\n",
      "error for JNPR: 0.0010615263258802038, index # 451\n",
      "error for PHM: 0.002658478753288436, index # 452\n",
      "error for CDAY: 0.003486679637074853, index # 453\n",
      "error for ALLE: 0.0011551575696312595, index # 454\n",
      "error for QRVO: 0.0023424487812838506, index # 455\n",
      "error for BWA: 0.0010242327299532323, index # 456\n",
      "error for NRG: 0.00167166169817983, index # 457\n",
      "error for MKTX: 0.001844789510890823, index # 458\n",
      "error for WRK: 0.0022451658264889306, index # 459\n",
      "error for UHS: 0.005299534755197729, index # 460\n",
      "error for FFIV: 0.0010431572294000843, index # 461\n",
      "error for AOS: 0.0013378290490854055, index # 462\n",
      "error for CMA: 0.0024771032462253136, index # 463\n",
      "error for AAL: 0.0069322263443376145, index # 464\n",
      "error for BBWI: 0.009069989895439308, index # 465\n",
      "error for HII: 0.001553186342738554, index # 466\n",
      "error for AAP: 0.0019401086215458468, index # 467\n",
      "error for TPR: 0.004874612248885566, index # 468\n",
      "error for FRT: 0.005414933546189069, index # 469\n",
      "error for IVZ: 0.00225347675427485, index # 470\n",
      "error for PNW: 0.0026159277703852466, index # 471\n",
      "error for HAS: 0.005220373894322179, index # 472\n",
      "error for WYNN: 0.004286714055419146, index # 473\n",
      "error for FBHS: 0.0024200135204430345, index # 474\n",
      "error for SBNY: 0.005230023143585587, index # 475\n",
      "error for DISH: 0.002864745192448966, index # 476\n",
      "error for RHI: 0.002004958721452012, index # 477\n",
      "error for WHR: 0.004685348747853089, index # 478\n",
      "error for ZION: 0.0019384968140748766, index # 479\n",
      "error for CTLT: 0.006077071604942437, index # 480\n",
      "error for PNR: 0.003094769860206082, index # 481\n",
      "error for SEE: 0.0025661045583859903, index # 482\n",
      "error for RL: 0.0021397832326850836, index # 483\n",
      "error for NCLH: 0.003901912957788234, index # 484\n",
      "error for DXC: 0.004432693205308219, index # 485\n",
      "error for GNRC: 0.009999334970143577, index # 486\n",
      "error for AIZ: 0.0022623381156304466, index # 487\n",
      "error for XRAY: 0.004831451751447429, index # 488\n",
      "error for LNC: 0.0035038265645580707, index # 489\n",
      "error for DVA: 0.005584660622690664, index # 490\n",
      "error for MHK: 0.00698327352200612, index # 491\n",
      "error for LUMN: 0.004397292006796589, index # 492\n",
      "error for ALK: 0.0023219843762346663, index # 493\n",
      "error for NWL: 0.0021459612501385796, index # 494\n",
      "error for VNO: 0.0036244788287421556, index # 495\n",
      "error for TAP: 0.001389944803779793, index # 496\n"
     ]
    }
   ],
   "source": [
    "tickers = utils.return_tickers()\n",
    "scores = []\n",
    "for ticker in tickers:\n",
    "    model = GradientBoostingRegressor()\n",
    "    X_train = prep.ready_to_train_df(ticker)\n",
    "    y_train = prep.ready_to_train_df(ticker)['return'].shift(1).replace(np.nan,0)\n",
    "    X_test = prep.ready_to_test(ticker)\n",
    "    y_test = prep.ready_to_test(ticker)['return'].shift(1).replace(np.nan,0)\n",
    "    model.fit(X_train, y_train)\n",
    "    error = (mean_squared_error(y_test, model.predict(X_test)))**0.5\n",
    "    scores.append(error)\n",
    "    print(f\"error for {ticker}: {error}, index # {tickers.index(ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "83e72b0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T17:14:29.948984Z",
     "start_time": "2022-12-06T17:14:29.921631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0026363905812504087"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8a47dc4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T17:14:41.990617Z",
     "start_time": "2022-12-06T17:14:41.964784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0020598109161219865"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813f16f",
   "metadata": {},
   "source": [
    "## Baseline calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "679275ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T13:44:29.624815Z",
     "start_time": "2022-12-06T13:40:21.080191Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline done for AAPL index # 0\n",
      "baseline done for MSFT index # 1\n",
      "baseline done for GOOG index # 2\n",
      "baseline done for AMZN index # 3\n",
      "baseline done for TSLA index # 4\n",
      "baseline done for UNH index # 5\n",
      "baseline done for XOM index # 6\n",
      "baseline done for JNJ index # 7\n",
      "baseline done for WMT index # 8\n",
      "baseline done for NVDA index # 9\n",
      "baseline done for JPM index # 10\n",
      "baseline done for V index # 11\n",
      "baseline done for CVX index # 12\n",
      "baseline done for PG index # 13\n",
      "baseline done for LLY index # 14\n",
      "baseline done for MA index # 15\n",
      "baseline done for HD index # 16\n",
      "baseline done for META index # 17\n",
      "baseline done for BAC index # 18\n",
      "baseline done for ABBV index # 19\n",
      "baseline done for PFE index # 20\n",
      "baseline done for KO index # 21\n",
      "baseline done for MRK index # 22\n",
      "baseline done for PEP index # 23\n",
      "baseline done for COST index # 24\n",
      "baseline done for ORCL index # 25\n",
      "baseline done for AVGO index # 26\n",
      "baseline done for TMO index # 27\n",
      "baseline done for MCD index # 28\n",
      "baseline done for CSCO index # 29\n",
      "baseline done for ACN index # 30\n",
      "baseline done for DHR index # 31\n",
      "baseline done for TMUS index # 32\n",
      "baseline done for ABT index # 33\n",
      "baseline done for WFC index # 34\n",
      "baseline done for DIS index # 35\n",
      "baseline done for LIN index # 36\n",
      "baseline done for NEE index # 37\n",
      "baseline done for BMY index # 38\n",
      "baseline done for NKE index # 39\n",
      "baseline done for VZ index # 40\n",
      "baseline done for TXN index # 41\n",
      "baseline done for UPS index # 42\n",
      "baseline done for COP index # 43\n",
      "baseline done for ADBE index # 44\n",
      "baseline done for CMCSA index # 45\n",
      "baseline done for CRM index # 46\n",
      "baseline done for PM index # 47\n",
      "baseline done for MS index # 48\n",
      "baseline done for AMGN index # 49\n",
      "baseline done for SCHW index # 50\n",
      "baseline done for HON index # 51\n",
      "baseline done for RTX index # 52\n",
      "baseline done for QCOM index # 53\n",
      "baseline done for T index # 54\n",
      "baseline done for IBM index # 55\n",
      "baseline done for DE index # 56\n",
      "baseline done for CVS index # 57\n",
      "baseline done for LOW index # 58\n",
      "baseline done for GS index # 59\n",
      "baseline done for UNP index # 60\n",
      "baseline done for NFLX index # 61\n",
      "baseline done for LMT index # 62\n",
      "baseline done for CAT index # 63\n",
      "baseline done for AMD index # 64\n",
      "baseline done for INTC index # 65\n",
      "baseline done for ELV index # 66\n",
      "baseline done for SPGI index # 67\n",
      "baseline done for AXP index # 68\n",
      "baseline done for SBUX index # 69\n",
      "baseline done for INTU index # 70\n",
      "baseline done for BLK index # 71\n",
      "baseline done for ADP index # 72\n",
      "baseline done for GILD index # 73\n",
      "baseline done for PLD index # 74\n",
      "baseline done for MDT index # 75\n",
      "baseline done for BA index # 76\n",
      "baseline done for AMT index # 77\n",
      "baseline done for CI index # 78\n",
      "baseline done for GE index # 79\n",
      "baseline done for TJX index # 80\n",
      "baseline done for ISRG index # 81\n",
      "baseline done for C index # 82\n",
      "baseline done for AMAT index # 83\n",
      "baseline done for PYPL index # 84\n",
      "baseline done for MDLZ index # 85\n",
      "baseline done for CB index # 86\n",
      "baseline done for SYK index # 87\n",
      "baseline done for ADI index # 88\n",
      "baseline done for MMC index # 89\n",
      "baseline done for EOG index # 90\n",
      "baseline done for NOW index # 91\n",
      "baseline done for VRTX index # 92\n",
      "baseline done for MO index # 93\n",
      "baseline done for NOC index # 94\n",
      "baseline done for EL index # 95\n",
      "baseline done for REGN index # 96\n",
      "baseline done for PGR index # 97\n",
      "baseline done for BKNG index # 98\n",
      "baseline done for DUK index # 99\n",
      "baseline done for TGT index # 100\n",
      "baseline done for SLB index # 101\n",
      "baseline done for SO index # 102\n",
      "baseline done for MMM index # 103\n",
      "baseline done for ITW index # 104\n",
      "baseline done for ZTS index # 105\n",
      "baseline done for GD index # 106\n",
      "baseline done for APD index # 107\n",
      "baseline done for HUM index # 108\n",
      "baseline done for MRNA index # 109\n",
      "baseline done for BDX index # 110\n",
      "baseline done for CSX index # 111\n",
      "baseline done for WM index # 112\n",
      "baseline done for PNC index # 113\n",
      "baseline done for HCA index # 114\n",
      "baseline done for ETN index # 115\n",
      "baseline done for USB index # 116\n",
      "baseline done for FISV index # 117\n",
      "baseline done for SHW index # 118\n",
      "baseline done for OXY index # 119\n",
      "baseline done for CL index # 120\n",
      "baseline done for MU index # 121\n",
      "baseline done for CME index # 122\n",
      "baseline done for AON index # 123\n",
      "baseline done for LRCX index # 124\n",
      "baseline done for BSX index # 125\n",
      "baseline done for EQIX index # 126\n",
      "baseline done for TFC index # 127\n",
      "baseline done for PXD index # 128\n",
      "baseline done for CHTR index # 129\n",
      "baseline done for CCI index # 130\n",
      "baseline done for MET index # 131\n",
      "baseline done for ATVI index # 132\n",
      "baseline done for ICE index # 133\n",
      "baseline done for MPC index # 134\n",
      "baseline done for NSC index # 135\n",
      "baseline done for DG index # 136\n",
      "baseline done for GM index # 137\n",
      "baseline done for EMR index # 138\n",
      "baseline done for F index # 139\n",
      "baseline done for KLAC index # 140\n",
      "baseline done for MCO index # 141\n",
      "baseline done for FCX index # 142\n",
      "baseline done for KDP index # 143\n",
      "baseline done for MNST index # 144\n",
      "baseline done for MCK index # 145\n",
      "baseline done for VLO index # 146\n",
      "baseline done for ORLY index # 147\n",
      "baseline done for ADM index # 148\n",
      "baseline done for PSX index # 149\n",
      "baseline done for PSA index # 150\n",
      "baseline done for SRE index # 151\n",
      "baseline done for SNPS index # 152\n",
      "baseline done for MAR index # 153\n",
      "baseline done for D index # 154\n",
      "baseline done for GIS index # 155\n",
      "baseline done for AEP index # 156\n",
      "baseline done for AZO index # 157\n",
      "baseline done for KHC index # 158\n",
      "baseline done for APH index # 159\n",
      "baseline done for HSY index # 160\n",
      "baseline done for CNC index # 161\n",
      "baseline done for CTVA index # 162\n",
      "baseline done for EW index # 163\n",
      "baseline done for CTAS index # 164\n",
      "baseline done for A index # 165\n",
      "baseline done for ROP index # 166\n",
      "baseline done for JCI index # 167\n",
      "baseline done for CDNS index # 168\n",
      "baseline done for FDX index # 169\n",
      "baseline done for NXPI index # 170\n",
      "baseline done for AIG index # 171\n",
      "baseline done for KMB index # 172\n",
      "baseline done for AFL index # 173\n",
      "baseline done for HES index # 174\n",
      "baseline done for MSI index # 175\n",
      "baseline done for PAYX index # 176\n",
      "baseline done for DVN index # 177\n",
      "baseline done for TRV index # 178\n",
      "baseline done for BIIB index # 179\n",
      "baseline done for DXCM index # 180\n",
      "baseline done for SYY index # 181\n",
      "baseline done for LHX index # 182\n",
      "baseline done for RSG index # 183\n",
      "baseline done for ENPH index # 184\n",
      "baseline done for ECL index # 185\n",
      "baseline done for ADSK index # 186\n",
      "baseline done for MCHP index # 187\n",
      "baseline done for ANET index # 188\n",
      "baseline done for KMI index # 189\n",
      "baseline done for CMG index # 190\n",
      "baseline done for FTNT index # 191\n",
      "baseline done for AJG index # 192\n",
      "baseline done for STZ index # 193\n",
      "baseline done for TT index # 194\n",
      "baseline done for WMB index # 195\n",
      "baseline done for MSCI index # 196\n",
      "baseline done for O index # 197\n",
      "baseline done for IQV index # 198\n",
      "baseline done for TEL index # 199\n",
      "baseline done for ROST index # 200\n",
      "baseline done for PRU index # 201\n",
      "baseline done for EXC index # 202\n",
      "baseline done for PH index # 203\n",
      "baseline done for FIS index # 204\n",
      "baseline done for SPG index # 205\n",
      "baseline done for COF index # 206\n",
      "baseline done for NUE index # 207\n",
      "baseline done for XEL index # 208\n",
      "baseline done for HLT index # 209\n",
      "baseline done for CARR index # 210\n",
      "baseline done for PCAR index # 211\n",
      "baseline done for BK index # 212\n",
      "baseline done for NEM index # 213\n",
      "baseline done for DOW index # 214\n",
      "baseline done for EA index # 215\n",
      "baseline done for WBA index # 216\n",
      "baseline done for DD index # 217\n",
      "baseline done for ALL index # 218\n",
      "baseline done for YUM index # 219\n",
      "baseline done for AMP index # 220\n",
      "baseline done for CMI index # 221\n",
      "baseline done for ILMN index # 222\n",
      "baseline done for TDG index # 223\n",
      "baseline done for IDXX index # 224\n",
      "baseline done for ED index # 225\n",
      "baseline done for KR index # 226\n",
      "baseline done for ABC index # 227\n",
      "baseline done for DLTR index # 228\n",
      "baseline done for RMD index # 229\n",
      "baseline done for ALB index # 230\n",
      "baseline done for HAL index # 231\n",
      "baseline done for NDAQ index # 232\n",
      "baseline done for LVS index # 233\n",
      "baseline done for ODFL index # 234\n",
      "baseline done for WELL index # 235\n",
      "baseline done for AME index # 236\n",
      "baseline done for CSGP index # 237\n",
      "baseline done for OTIS index # 238\n",
      "baseline done for MTD index # 239\n",
      "baseline done for SBAC index # 240\n",
      "baseline done for ON index # 241\n",
      "baseline done for VICI index # 242\n",
      "baseline done for DLR index # 243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline done for KEYS index # 244\n",
      "baseline done for PPG index # 245\n",
      "baseline done for WEC index # 246\n",
      "baseline done for CTSH index # 247\n",
      "baseline done for ROK index # 248\n",
      "baseline done for GWW index # 249\n",
      "baseline done for PCG index # 250\n",
      "baseline done for HPQ index # 251\n",
      "baseline done for FAST index # 252\n",
      "baseline done for DFS index # 253\n",
      "baseline done for MTB index # 254\n",
      "baseline done for PEG index # 255\n",
      "baseline done for OKE index # 256\n",
      "baseline done for DHI index # 257\n",
      "baseline done for APTV index # 258\n",
      "baseline done for BKR index # 259\n",
      "baseline done for GLW index # 260\n",
      "baseline done for LYB index # 261\n",
      "baseline done for ES index # 262\n",
      "baseline done for BAX index # 263\n",
      "baseline done for STT index # 264\n",
      "baseline done for VRSK index # 265\n",
      "baseline done for TROW index # 266\n",
      "baseline done for WBD index # 267\n",
      "baseline done for AWK index # 268\n",
      "baseline done for IT index # 269\n",
      "baseline done for GPN index # 270\n",
      "baseline done for HRL index # 271\n",
      "baseline done for FANG index # 272\n",
      "baseline done for WTW index # 273\n",
      "baseline done for RJF index # 274\n",
      "baseline done for GPC index # 275\n",
      "baseline done for IFF index # 276\n",
      "baseline done for CDW index # 277\n",
      "baseline done for TSCO index # 278\n",
      "baseline done for FITB index # 279\n",
      "baseline done for ARE index # 280\n",
      "baseline done for URI index # 281\n",
      "baseline done for ZBH index # 282\n",
      "baseline done for K index # 283\n",
      "baseline done for LEN index # 284\n",
      "baseline done for EBAY index # 285\n",
      "baseline done for EIX index # 286\n",
      "baseline done for CBRE index # 287\n",
      "baseline done for EFX index # 288\n",
      "baseline done for VMC index # 289\n",
      "baseline done for TSN index # 290\n",
      "baseline done for HIG index # 291\n",
      "baseline done for FTV index # 292\n",
      "baseline done for WY index # 293\n",
      "baseline done for EQR index # 294\n",
      "baseline done for AVB index # 295\n",
      "baseline done for MKC index # 296\n",
      "baseline done for ETR index # 297\n",
      "baseline done for LUV index # 298\n",
      "baseline done for ULTA index # 299\n",
      "baseline done for AEE index # 300\n",
      "baseline done for MLM index # 301\n",
      "baseline done for FE index # 302\n",
      "baseline done for PFG index # 303\n",
      "baseline done for FRC index # 304\n",
      "baseline done for DTE index # 305\n",
      "baseline done for DAL index # 306\n",
      "baseline done for HBAN index # 307\n",
      "baseline done for IR index # 308\n",
      "baseline done for CTRA index # 309\n",
      "baseline done for ANSS index # 310\n",
      "baseline done for ACGL index # 311\n",
      "baseline done for PPL index # 312\n",
      "baseline done for RF index # 313\n",
      "baseline done for VRSN index # 314\n",
      "baseline done for LH index # 315\n",
      "baseline done for EXR index # 316\n",
      "baseline done for PWR index # 317\n",
      "baseline done for CF index # 318\n",
      "baseline done for CAH index # 319\n",
      "baseline done for CFG index # 320\n",
      "baseline done for XYL index # 321\n",
      "baseline done for HPE index # 322\n",
      "baseline done for EPAM index # 323\n",
      "baseline done for DOV index # 324\n",
      "baseline done for WAT index # 325\n",
      "baseline done for WRB index # 326\n",
      "baseline done for TDY index # 327\n",
      "baseline done for PAYC index # 328\n",
      "baseline done for ROL index # 329\n",
      "baseline done for NTRS index # 330\n",
      "baseline done for MRO index # 331\n",
      "baseline done for CNP index # 332\n",
      "baseline done for INVH index # 333\n",
      "baseline done for CHD index # 334\n",
      "baseline done for AES index # 335\n",
      "baseline done for MOH index # 336\n",
      "baseline done for JBHT index # 337\n",
      "baseline done for MAA index # 338\n",
      "baseline done for BBY index # 339\n",
      "baseline done for CLX index # 340\n",
      "baseline done for HOLX index # 341\n",
      "baseline done for WAB index # 342\n",
      "baseline done for DRI index # 343\n",
      "baseline done for EXPD index # 344\n",
      "baseline done for STE index # 345\n",
      "baseline done for AMCR index # 346\n",
      "baseline done for VTR index # 347\n",
      "baseline done for IEX index # 348\n",
      "baseline done for CAG index # 349\n",
      "baseline done for CMS index # 350\n",
      "baseline done for KEY index # 351\n",
      "baseline done for MPWR index # 352\n",
      "baseline done for BALL index # 353\n",
      "baseline done for J index # 354\n",
      "baseline done for BR index # 355\n",
      "baseline done for GRMN index # 356\n",
      "baseline done for PKI index # 357\n",
      "baseline done for TTWO index # 358\n",
      "baseline done for INCY index # 359\n",
      "baseline done for FDS index # 360\n",
      "baseline done for MOS index # 361\n",
      "baseline done for SEDG index # 362\n",
      "baseline done for CINF index # 363\n",
      "baseline done for ABMD index # 364\n",
      "baseline done for DGX index # 365\n",
      "baseline done for WST index # 366\n",
      "baseline done for ATO index # 367\n",
      "baseline done for TRGP index # 368\n",
      "baseline done for BRO index # 369\n",
      "baseline done for SYF index # 370\n",
      "baseline done for FOX index # 371\n",
      "baseline done for FOXA index # 372\n",
      "baseline done for NTAP index # 373\n",
      "baseline done for FMC index # 374\n",
      "baseline done for EQT index # 375\n",
      "baseline done for OMC index # 376\n",
      "baseline done for SJM index # 377\n",
      "baseline done for LYV index # 378\n",
      "baseline done for CPB index # 379\n",
      "baseline done for HWM index # 380\n",
      "baseline done for CPRT index # 381\n",
      "baseline done for AVY index # 382\n",
      "baseline done for IRM index # 383\n",
      "baseline done for COO index # 384\n",
      "baseline done for ALGN index # 385\n",
      "baseline done for SWKS index # 386\n",
      "baseline done for EXPE index # 387\n",
      "baseline done for RCL index # 388\n",
      "baseline done for ETSY index # 389\n",
      "baseline done for APA index # 390\n",
      "baseline done for GEN index # 391\n",
      "baseline done for TXT index # 392\n",
      "baseline done for LDOS index # 393\n",
      "baseline done for LKQ index # 394\n",
      "baseline done for TER index # 395\n",
      "baseline done for PTC index # 396\n",
      "baseline done for TRMB index # 397\n",
      "baseline done for AKAM index # 398\n",
      "baseline done for NVR index # 399\n",
      "baseline done for UAL index # 400\n",
      "baseline done for LNT index # 401\n",
      "baseline done for FLT index # 402\n",
      "baseline done for KIM index # 403\n",
      "baseline done for ZBRA index # 404\n",
      "baseline done for TYL index # 405\n",
      "baseline done for DPZ index # 406\n",
      "baseline done for JKHY index # 407\n",
      "baseline done for MGM index # 408\n",
      "baseline done for ESS index # 409\n",
      "baseline done for L index # 410\n",
      "baseline done for PEAK index # 411\n",
      "baseline done for MTCH index # 412\n",
      "baseline done for NDSN index # 413\n",
      "baseline done for EVRG index # 414\n",
      "baseline done for IPG index # 415\n",
      "baseline done for BEN index # 416\n",
      "baseline done for CBOE index # 417\n",
      "baseline done for TECH index # 418\n",
      "baseline done for SIVB index # 419\n",
      "baseline done for VFC index # 420\n",
      "baseline done for IP index # 421\n",
      "baseline done for HST index # 422\n",
      "baseline done for UDR index # 423\n",
      "baseline done for POOL index # 424\n",
      "baseline done for RE index # 425\n",
      "baseline done for PARA index # 426\n",
      "baseline done for SNA index # 427\n",
      "baseline done for CPT index # 428\n",
      "baseline done for LW index # 429\n",
      "baseline done for PKG index # 430\n",
      "baseline done for CRL index # 431\n",
      "baseline done for SWK index # 432\n",
      "baseline done for BIO index # 433\n",
      "baseline done for WDC index # 434\n",
      "baseline done for CHRW index # 435\n",
      "baseline done for STX index # 436\n",
      "baseline done for MAS index # 437\n",
      "baseline done for GL index # 438\n",
      "baseline done for CE index # 439\n",
      "baseline done for REG index # 440\n",
      "baseline done for NI index # 441\n",
      "baseline done for BXP index # 442\n",
      "baseline done for HSIC index # 443\n",
      "baseline done for CCL index # 444\n",
      "baseline done for TFX index # 445\n",
      "baseline done for CZR index # 446\n",
      "baseline done for NWS index # 447\n",
      "baseline done for NWSA index # 448\n",
      "baseline done for KMX index # 449\n",
      "baseline done for EMN index # 450\n",
      "baseline done for JNPR index # 451\n",
      "baseline done for PHM index # 452\n",
      "baseline done for CDAY index # 453\n",
      "baseline done for ALLE index # 454\n",
      "baseline done for QRVO index # 455\n",
      "baseline done for BWA index # 456\n",
      "baseline done for NRG index # 457\n",
      "baseline done for MKTX index # 458\n",
      "baseline done for WRK index # 459\n",
      "baseline done for UHS index # 460\n",
      "baseline done for FFIV index # 461\n",
      "baseline done for AOS index # 462\n",
      "baseline done for CMA index # 463\n",
      "baseline done for AAL index # 464\n",
      "baseline done for BBWI index # 465\n",
      "baseline done for HII index # 466\n",
      "baseline done for AAP index # 467\n",
      "baseline done for TPR index # 468\n",
      "baseline done for FRT index # 469\n",
      "baseline done for IVZ index # 470\n",
      "baseline done for PNW index # 471\n",
      "baseline done for HAS index # 472\n",
      "baseline done for WYNN index # 473\n",
      "baseline done for FBHS index # 474\n",
      "baseline done for SBNY index # 475\n",
      "baseline done for DISH index # 476\n",
      "baseline done for RHI index # 477\n",
      "baseline done for WHR index # 478\n",
      "baseline done for ZION index # 479\n",
      "baseline done for CTLT index # 480\n",
      "baseline done for PNR index # 481\n",
      "baseline done for SEE index # 482\n",
      "baseline done for RL index # 483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline done for NCLH index # 484\n",
      "baseline done for DXC index # 485\n",
      "baseline done for GNRC index # 486\n",
      "baseline done for AIZ index # 487\n",
      "baseline done for XRAY index # 488\n",
      "baseline done for LNC index # 489\n",
      "baseline done for DVA index # 490\n",
      "baseline done for MHK index # 491\n",
      "baseline done for LUMN index # 492\n",
      "baseline done for ALK index # 493\n",
      "baseline done for NWL index # 494\n",
      "baseline done for VNO index # 495\n",
      "baseline done for TAP index # 496\n"
     ]
    }
   ],
   "source": [
    "tickers = utils.return_tickers()\n",
    "baseline = []\n",
    "for ticker in tickers:\n",
    "    df = prep.ready_to_test(ticker)\n",
    "    for i in range(1, len(df), 1):\n",
    "        error = (mean_squared_error(df.loc[i,['return']], df.loc[i-1,['return']]))**0.5\n",
    "        baseline.append(error)\n",
    "    print(f\"baseline done for {ticker} index # {tickers.index(ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9929fe44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T13:46:25.738518Z",
     "start_time": "2022-12-06T13:46:25.695921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02256406773595633"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(baseline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b8cba54f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T13:46:42.744415Z",
     "start_time": "2022-12-06T13:46:42.697615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026165766170841074"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b0608b2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T12:03:01.191322Z",
     "start_time": "2022-12-06T12:03:01.158597Z"
    }
   },
   "outputs": [],
   "source": [
    "model = joblib.load(f\"../raw_data/models/QCOM_GradientBoostingRegressor_PCA.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = utils.return_tickers()\n",
    "y_pred = []\n",
    "for ticker in tickers:\n",
    "    model = joblib.load(model, f\"model/{ticker}_GradientBoostingRegressor_PCA.joblib\")\n",
    "    y_pred.append(model.predict(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d5baa074",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T17:15:10.420544Z",
     "start_time": "2022-12-06T17:15:09.722826Z"
    }
   },
   "outputs": [],
   "source": [
    "ticker = 'AAPL'\n",
    "X_train = prep.ready_to_train_df(ticker)\n",
    "y_train = prep.ready_to_train_df(ticker)['return'].shift(1).replace(np.nan,0)\n",
    "X_test = prep.ready_to_test(ticker)\n",
    "y_test = prep.ready_to_test(ticker)['return'].shift(1).replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "46157be4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T17:15:33.882320Z",
     "start_time": "2022-12-06T17:15:30.629654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" checked><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "125c850d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T17:17:51.948292Z",
     "start_time": "2022-12-06T17:17:51.917074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.012839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>0.011724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>-0.007757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>0.013579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>0.003698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>-0.021384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1086 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          AAPL\n",
       "0     0.013787\n",
       "1     0.013746\n",
       "2    -0.001507\n",
       "3    -0.012839\n",
       "4     0.016576\n",
       "...        ...\n",
       "1081  0.011724\n",
       "1082 -0.007757\n",
       "1083  0.013579\n",
       "1084  0.003698\n",
       "1085 -0.021384\n",
       "\n",
       "[1086 rows x 1 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model.predict(X_test)).rename(columns={0:f\"{ticker}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e965655b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T17:18:27.601308Z",
     "start_time": "2022-12-06T17:18:27.576968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b57ba2b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T17:16:49.231178Z",
     "start_time": "2022-12-06T17:16:49.202612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.000000\n",
       "1       0.013885\n",
       "2      -0.001207\n",
       "3      -0.012976\n",
       "4       0.016766\n",
       "          ...   \n",
       "1081    0.011869\n",
       "1082   -0.008331\n",
       "1083    0.012971\n",
       "1084    0.003782\n",
       "1085   -0.021680\n",
       "Name: return, Length: 1086, dtype: float64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca448a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = utils.return_tickers()\n",
    "for ticker in tickers:\n",
    "    model = XGBRegressor(n_jobs=-1)\n",
    "    X_train = prep.ready_to_train_df(ticker)\n",
    "    y_train = prep.ready_to_train_df(ticker)['return'].shift(1).replace(np.nan,0)\n",
    "    X_test = prep.ready_to_test(ticker)\n",
    "    y_test = prep.ready_to_test(ticker)['return'].shift(1).replace(np.nan,0)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_ticker = pd.DataFrame(model.predict(X_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba77073b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dcfd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
