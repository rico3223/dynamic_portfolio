{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e0f1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:19:27.106744Z",
     "start_time": "2022-12-02T08:19:26.858703Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d1bd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:19:27.179623Z",
     "start_time": "2022-12-02T08:19:27.177831Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41a4a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:19:28.868554Z",
     "start_time": "2022-12-02T08:19:28.537505Z"
    }
   },
   "outputs": [],
   "source": [
    "from dynamic_portfolio.utils import load_csv\n",
    "from dynamic_portfolio.utils import features_creation, clean_data\n",
    "from dynamic_portfolio.preprocess import scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d01300",
   "metadata": {},
   "source": [
    "# Loading data and creating clean dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9420e32e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:52:46.475428Z",
     "start_time": "2022-12-02T08:52:46.368828Z"
    }
   },
   "outputs": [],
   "source": [
    "df = features_creation('META')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6363374",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:52:48.330587Z",
     "start_time": "2022-12-02T08:52:48.324007Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaebcbd",
   "metadata": {},
   "source": [
    "# Cross vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc31db2",
   "metadata": {},
   "source": [
    "We use sklearn's time series split to break up the data in different folds\n",
    "We use a sklearn example to run the first tests.\n",
    "\n",
    "Metrics used : \n",
    " - rmse\n",
    " - mae\n",
    " - R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c719f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:19:59.345931Z",
     "start_time": "2022-12-02T08:19:59.326127Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aff74a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:55:44.151243Z",
     "start_time": "2022-12-02T08:55:44.141942Z"
    }
   },
   "outputs": [],
   "source": [
    "ts_cv = TimeSeriesSplit(\n",
    "    n_splits=20,\n",
    "    gap=0,\n",
    "    max_train_size=252,\n",
    "    test_size=45,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc575dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = list(ts_cv.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcdc4ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:29:09.695999Z",
     "start_time": "2022-12-02T08:29:09.687968Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286365a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:21:46.267703Z",
     "start_time": "2022-12-02T08:21:46.259434Z"
    }
   },
   "outputs": [],
   "source": [
    "model = HistGradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f16d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:36:25.880376Z",
     "start_time": "2022-12-02T08:31:50.247799Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse = []\n",
    "for train_index, test_index in ts_cv.split(X):\n",
    "    cv_train, cv_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    model = model.fit(X,y)\n",
    "    predictions = model.predict(cv_test)\n",
    "    true_values = cv_test['return']\n",
    "    rmse.append(np.sqrt(mean_squared_error(true_values, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d38cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:37:48.457651Z",
     "start_time": "2022-12-02T08:37:48.449394Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(rmse)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f2a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:54:01.669818Z",
     "start_time": "2022-12-02T08:54:01.661065Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, X, y, cv):\n",
    "    cv_results = cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n",
    "    )\n",
    "    mae = -cv_results[\"test_neg_mean_absolute_error\"]\n",
    "    rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\n",
    "    print(\n",
    "        f\"Mean Absolute Error:     {mae.mean():.6f} +/- {mae.std():.6f}\\n\"\n",
    "        f\"Root Mean Squared Error: {rmse.mean():.6f} +/- {rmse.std():.6f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b4b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7200e5be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:56:07.338741Z",
     "start_time": "2022-12-02T08:55:50.048380Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(model, X, y, ts_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d32eb",
   "metadata": {},
   "source": [
    "# PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e2fe0e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:37:24.095118Z",
     "start_time": "2022-12-06T10:37:24.069698Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe089e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-05T16:13:09.647733Z",
     "start_time": "2022-12-05T16:13:09.417555Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.9)\n",
    "pca.fit(prep.ready_to_train_df('IBM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae8741",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-05T16:13:24.046816Z",
     "start_time": "2022-12-05T16:13:24.021951Z"
    }
   },
   "outputs": [],
   "source": [
    "n_pcs= pca.n_components_ # get number of component\n",
    "# get the index of the most important feature on EACH component\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "initial_feature_names = ibm.columns\n",
    "# get the most important feature names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92454e",
   "metadata": {},
   "source": [
    "## Using PCA analysis on all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "392f1e5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:37:12.647715Z",
     "start_time": "2022-12-06T10:37:12.621135Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42b60026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:38:17.401120Z",
     "start_time": "2022-12-06T10:38:17.380088Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('pca', PCA()),\n",
       "  ('gradientboostingregressor', GradientBoostingRegressor())],\n",
       " 'verbose': False,\n",
       " 'pca': PCA(),\n",
       " 'gradientboostingregressor': GradientBoostingRegressor(),\n",
       " 'pca__copy': True,\n",
       " 'pca__iterated_power': 'auto',\n",
       " 'pca__n_components': None,\n",
       " 'pca__n_oversamples': 10,\n",
       " 'pca__power_iteration_normalizer': 'auto',\n",
       " 'pca__random_state': None,\n",
       " 'pca__svd_solver': 'auto',\n",
       " 'pca__tol': 0.0,\n",
       " 'pca__whiten': False,\n",
       " 'gradientboostingregressor__alpha': 0.9,\n",
       " 'gradientboostingregressor__ccp_alpha': 0.0,\n",
       " 'gradientboostingregressor__criterion': 'friedman_mse',\n",
       " 'gradientboostingregressor__init': None,\n",
       " 'gradientboostingregressor__learning_rate': 0.1,\n",
       " 'gradientboostingregressor__loss': 'squared_error',\n",
       " 'gradientboostingregressor__max_depth': 3,\n",
       " 'gradientboostingregressor__max_features': None,\n",
       " 'gradientboostingregressor__max_leaf_nodes': None,\n",
       " 'gradientboostingregressor__min_impurity_decrease': 0.0,\n",
       " 'gradientboostingregressor__min_samples_leaf': 1,\n",
       " 'gradientboostingregressor__min_samples_split': 2,\n",
       " 'gradientboostingregressor__min_weight_fraction_leaf': 0.0,\n",
       " 'gradientboostingregressor__n_estimators': 100,\n",
       " 'gradientboostingregressor__n_iter_no_change': None,\n",
       " 'gradientboostingregressor__random_state': None,\n",
       " 'gradientboostingregressor__subsample': 1.0,\n",
       " 'gradientboostingregressor__tol': 0.0001,\n",
       " 'gradientboostingregressor__validation_fraction': 0.1,\n",
       " 'gradientboostingregressor__verbose': 0,\n",
       " 'gradientboostingregressor__warm_start': False}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17fde726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:39:58.699205Z",
     "start_time": "2022-12-06T10:39:58.666792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=0.9)),\n",
       "                (&#x27;gradientboostingregressor&#x27;,\n",
       "                 GradientBoostingRegressor(learning_rate=0.08))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=0.9)),\n",
       "                (&#x27;gradientboostingregressor&#x27;,\n",
       "                 GradientBoostingRegressor(learning_rate=0.08))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=0.9)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.08)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('pca', PCA(n_components=0.9)),\n",
       "                ('gradientboostingregressor',\n",
       "                 GradientBoostingRegressor(learning_rate=0.08))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(PCA(), GradientBoostingRegressor())\n",
    "\n",
    "params = {'pca__n_components':0.9,\n",
    "                'gradientboostingregressor__max_depth':3,\n",
    "                'gradientboostingregressor__criterion':'friedman_mse',\n",
    "                'gradientboostingregressor__n_estimators':100,\n",
    "                'gradientboostingregressor__learning_rate':0.08}\n",
    "pipe.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5064987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:03:57.855604Z",
     "start_time": "2022-12-06T10:42:04.427027Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker AAPL index # 0\n",
      "done for ticker MSFT index # 1\n",
      "done for ticker GOOG index # 2\n",
      "done for ticker AMZN index # 3\n",
      "done for ticker TSLA index # 4\n",
      "done for ticker UNH index # 5\n",
      "done for ticker XOM index # 6\n",
      "done for ticker JNJ index # 7\n",
      "done for ticker WMT index # 8\n",
      "done for ticker NVDA index # 9\n",
      "done for ticker JPM index # 10\n",
      "done for ticker V index # 11\n",
      "done for ticker CVX index # 12\n",
      "done for ticker PG index # 13\n",
      "done for ticker LLY index # 14\n",
      "done for ticker MA index # 15\n",
      "done for ticker HD index # 16\n",
      "done for ticker META index # 17\n",
      "done for ticker BAC index # 18\n",
      "done for ticker ABBV index # 19\n",
      "done for ticker PFE index # 20\n",
      "done for ticker KO index # 21\n",
      "done for ticker MRK index # 22\n",
      "done for ticker PEP index # 23\n",
      "done for ticker COST index # 24\n",
      "done for ticker ORCL index # 25\n",
      "done for ticker AVGO index # 26\n",
      "done for ticker TMO index # 27\n",
      "done for ticker MCD index # 28\n",
      "done for ticker CSCO index # 29\n",
      "done for ticker ACN index # 30\n",
      "done for ticker DHR index # 31\n",
      "done for ticker TMUS index # 32\n",
      "done for ticker ABT index # 33\n",
      "done for ticker WFC index # 34\n",
      "done for ticker DIS index # 35\n",
      "done for ticker LIN index # 36\n",
      "done for ticker NEE index # 37\n",
      "done for ticker BMY index # 38\n",
      "done for ticker NKE index # 39\n",
      "done for ticker VZ index # 40\n",
      "done for ticker TXN index # 41\n",
      "done for ticker UPS index # 42\n",
      "done for ticker COP index # 43\n",
      "done for ticker ADBE index # 44\n",
      "done for ticker CMCSA index # 45\n",
      "done for ticker CRM index # 46\n",
      "done for ticker PM index # 47\n",
      "done for ticker MS index # 48\n",
      "done for ticker AMGN index # 49\n",
      "done for ticker SCHW index # 50\n",
      "done for ticker HON index # 51\n",
      "done for ticker RTX index # 52\n",
      "done for ticker QCOM index # 53\n",
      "done for ticker T index # 54\n",
      "done for ticker IBM index # 55\n",
      "done for ticker DE index # 56\n",
      "done for ticker CVS index # 57\n",
      "done for ticker LOW index # 58\n",
      "done for ticker GS index # 59\n",
      "done for ticker UNP index # 60\n",
      "done for ticker NFLX index # 61\n",
      "done for ticker LMT index # 62\n",
      "done for ticker CAT index # 63\n",
      "done for ticker AMD index # 64\n",
      "done for ticker INTC index # 65\n",
      "done for ticker ELV index # 66\n",
      "done for ticker SPGI index # 67\n",
      "done for ticker AXP index # 68\n",
      "done for ticker SBUX index # 69\n",
      "done for ticker INTU index # 70\n",
      "done for ticker BLK index # 71\n",
      "done for ticker ADP index # 72\n",
      "done for ticker GILD index # 73\n",
      "done for ticker PLD index # 74\n",
      "done for ticker MDT index # 75\n",
      "done for ticker BA index # 76\n",
      "done for ticker AMT index # 77\n",
      "done for ticker CI index # 78\n",
      "done for ticker GE index # 79\n",
      "done for ticker TJX index # 80\n",
      "done for ticker ISRG index # 81\n",
      "done for ticker C index # 82\n",
      "done for ticker AMAT index # 83\n",
      "done for ticker PYPL index # 84\n",
      "done for ticker MDLZ index # 85\n",
      "done for ticker CB index # 86\n",
      "done for ticker SYK index # 87\n",
      "done for ticker ADI index # 88\n",
      "done for ticker MMC index # 89\n",
      "done for ticker EOG index # 90\n",
      "done for ticker NOW index # 91\n",
      "done for ticker VRTX index # 92\n",
      "done for ticker MO index # 93\n",
      "done for ticker NOC index # 94\n",
      "done for ticker EL index # 95\n",
      "done for ticker REGN index # 96\n",
      "done for ticker PGR index # 97\n",
      "done for ticker BKNG index # 98\n",
      "done for ticker DUK index # 99\n",
      "done for ticker TGT index # 100\n",
      "done for ticker SLB index # 101\n",
      "done for ticker SO index # 102\n",
      "done for ticker MMM index # 103\n",
      "done for ticker ITW index # 104\n",
      "done for ticker ZTS index # 105\n",
      "done for ticker GD index # 106\n",
      "done for ticker APD index # 107\n",
      "done for ticker HUM index # 108\n",
      "done for ticker MRNA index # 109\n",
      "done for ticker BDX index # 110\n",
      "done for ticker CSX index # 111\n",
      "done for ticker WM index # 112\n",
      "done for ticker PNC index # 113\n",
      "done for ticker HCA index # 114\n",
      "done for ticker ETN index # 115\n",
      "done for ticker USB index # 116\n",
      "done for ticker FISV index # 117\n",
      "done for ticker SHW index # 118\n",
      "done for ticker OXY index # 119\n",
      "done for ticker CL index # 120\n",
      "done for ticker MU index # 121\n",
      "done for ticker CME index # 122\n",
      "done for ticker AON index # 123\n",
      "done for ticker LRCX index # 124\n",
      "done for ticker BSX index # 125\n",
      "done for ticker EQIX index # 126\n",
      "done for ticker TFC index # 127\n",
      "done for ticker PXD index # 128\n",
      "done for ticker CHTR index # 129\n",
      "done for ticker CCI index # 130\n",
      "done for ticker MET index # 131\n",
      "done for ticker ATVI index # 132\n",
      "done for ticker ICE index # 133\n",
      "done for ticker MPC index # 134\n",
      "done for ticker NSC index # 135\n",
      "done for ticker DG index # 136\n",
      "done for ticker GM index # 137\n",
      "done for ticker EMR index # 138\n",
      "done for ticker F index # 139\n",
      "done for ticker KLAC index # 140\n",
      "done for ticker MCO index # 141\n",
      "done for ticker FCX index # 142\n",
      "done for ticker KDP index # 143\n",
      "done for ticker MNST index # 144\n",
      "done for ticker MCK index # 145\n",
      "done for ticker VLO index # 146\n",
      "done for ticker ORLY index # 147\n",
      "done for ticker ADM index # 148\n",
      "done for ticker PSX index # 149\n",
      "done for ticker PSA index # 150\n",
      "done for ticker SRE index # 151\n",
      "done for ticker SNPS index # 152\n",
      "done for ticker MAR index # 153\n",
      "done for ticker D index # 154\n",
      "done for ticker GIS index # 155\n",
      "done for ticker AEP index # 156\n",
      "done for ticker AZO index # 157\n",
      "done for ticker KHC index # 158\n",
      "done for ticker APH index # 159\n",
      "done for ticker HSY index # 160\n",
      "done for ticker CNC index # 161\n",
      "done for ticker CTVA index # 162\n",
      "done for ticker EW index # 163\n",
      "done for ticker CTAS index # 164\n",
      "done for ticker A index # 165\n",
      "done for ticker ROP index # 166\n",
      "done for ticker JCI index # 167\n",
      "done for ticker CDNS index # 168\n",
      "done for ticker FDX index # 169\n",
      "done for ticker NXPI index # 170\n",
      "done for ticker AIG index # 171\n",
      "done for ticker KMB index # 172\n",
      "done for ticker AFL index # 173\n",
      "done for ticker HES index # 174\n",
      "done for ticker MSI index # 175\n",
      "done for ticker PAYX index # 176\n",
      "done for ticker DVN index # 177\n",
      "done for ticker TRV index # 178\n",
      "done for ticker BIIB index # 179\n",
      "done for ticker DXCM index # 180\n",
      "done for ticker SYY index # 181\n",
      "done for ticker LHX index # 182\n",
      "done for ticker RSG index # 183\n",
      "done for ticker ENPH index # 184\n",
      "done for ticker ECL index # 185\n",
      "done for ticker ADSK index # 186\n",
      "done for ticker MCHP index # 187\n",
      "done for ticker ANET index # 188\n",
      "done for ticker KMI index # 189\n",
      "done for ticker CMG index # 190\n",
      "done for ticker FTNT index # 191\n",
      "done for ticker AJG index # 192\n",
      "done for ticker STZ index # 193\n",
      "done for ticker TT index # 194\n",
      "done for ticker WMB index # 195\n",
      "done for ticker MSCI index # 196\n",
      "done for ticker O index # 197\n",
      "done for ticker IQV index # 198\n",
      "done for ticker TEL index # 199\n",
      "done for ticker ROST index # 200\n",
      "done for ticker PRU index # 201\n",
      "done for ticker EXC index # 202\n",
      "done for ticker PH index # 203\n",
      "done for ticker FIS index # 204\n",
      "done for ticker SPG index # 205\n",
      "done for ticker COF index # 206\n",
      "done for ticker NUE index # 207\n",
      "done for ticker XEL index # 208\n",
      "done for ticker HLT index # 209\n",
      "done for ticker CARR index # 210\n",
      "done for ticker PCAR index # 211\n",
      "done for ticker BK index # 212\n",
      "done for ticker NEM index # 213\n",
      "done for ticker DOW index # 214\n",
      "done for ticker EA index # 215\n",
      "done for ticker WBA index # 216\n",
      "done for ticker DD index # 217\n",
      "done for ticker ALL index # 218\n",
      "done for ticker YUM index # 219\n",
      "done for ticker AMP index # 220\n",
      "done for ticker CMI index # 221\n",
      "done for ticker ILMN index # 222\n",
      "done for ticker TDG index # 223\n",
      "done for ticker IDXX index # 224\n",
      "done for ticker ED index # 225\n",
      "done for ticker KR index # 226\n",
      "done for ticker ABC index # 227\n",
      "done for ticker DLTR index # 228\n",
      "done for ticker RMD index # 229\n",
      "done for ticker ALB index # 230\n",
      "done for ticker HAL index # 231\n",
      "done for ticker NDAQ index # 232\n",
      "done for ticker LVS index # 233\n",
      "done for ticker ODFL index # 234\n",
      "done for ticker WELL index # 235\n",
      "done for ticker AME index # 236\n",
      "done for ticker CSGP index # 237\n",
      "done for ticker OTIS index # 238\n",
      "done for ticker MTD index # 239\n",
      "done for ticker SBAC index # 240\n",
      "done for ticker ON index # 241\n",
      "done for ticker VICI index # 242\n",
      "done for ticker DLR index # 243\n",
      "done for ticker KEYS index # 244\n",
      "done for ticker PPG index # 245\n",
      "done for ticker WEC index # 246\n",
      "done for ticker CTSH index # 247\n",
      "done for ticker ROK index # 248\n",
      "done for ticker GWW index # 249\n",
      "done for ticker PCG index # 250\n",
      "done for ticker HPQ index # 251\n",
      "done for ticker FAST index # 252\n",
      "done for ticker DFS index # 253\n",
      "done for ticker MTB index # 254\n",
      "done for ticker PEG index # 255\n",
      "done for ticker OKE index # 256\n",
      "done for ticker DHI index # 257\n",
      "done for ticker APTV index # 258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker BKR index # 259\n",
      "done for ticker GLW index # 260\n",
      "done for ticker LYB index # 261\n",
      "done for ticker ES index # 262\n",
      "done for ticker BAX index # 263\n",
      "done for ticker STT index # 264\n",
      "done for ticker VRSK index # 265\n",
      "done for ticker TROW index # 266\n",
      "done for ticker WBD index # 267\n",
      "done for ticker AWK index # 268\n",
      "done for ticker IT index # 269\n",
      "done for ticker GPN index # 270\n",
      "done for ticker HRL index # 271\n",
      "done for ticker FANG index # 272\n",
      "done for ticker WTW index # 273\n",
      "done for ticker RJF index # 274\n",
      "done for ticker GPC index # 275\n",
      "done for ticker IFF index # 276\n",
      "done for ticker CDW index # 277\n",
      "done for ticker TSCO index # 278\n",
      "done for ticker FITB index # 279\n",
      "done for ticker ARE index # 280\n",
      "done for ticker URI index # 281\n",
      "done for ticker ZBH index # 282\n",
      "done for ticker K index # 283\n",
      "done for ticker LEN index # 284\n",
      "done for ticker EBAY index # 285\n",
      "done for ticker EIX index # 286\n",
      "done for ticker CBRE index # 287\n",
      "done for ticker EFX index # 288\n",
      "done for ticker VMC index # 289\n",
      "done for ticker TSN index # 290\n",
      "done for ticker HIG index # 291\n",
      "done for ticker FTV index # 292\n",
      "done for ticker WY index # 293\n",
      "done for ticker EQR index # 294\n",
      "done for ticker AVB index # 295\n",
      "done for ticker MKC index # 296\n",
      "done for ticker ETR index # 297\n",
      "done for ticker LUV index # 298\n",
      "done for ticker ULTA index # 299\n",
      "done for ticker AEE index # 300\n",
      "done for ticker MLM index # 301\n",
      "done for ticker FE index # 302\n",
      "done for ticker PFG index # 303\n",
      "done for ticker FRC index # 304\n",
      "done for ticker DTE index # 305\n",
      "done for ticker DAL index # 306\n",
      "done for ticker HBAN index # 307\n",
      "done for ticker IR index # 308\n",
      "done for ticker CTRA index # 309\n",
      "done for ticker ANSS index # 310\n",
      "done for ticker ACGL index # 311\n",
      "done for ticker PPL index # 312\n",
      "done for ticker RF index # 313\n",
      "done for ticker VRSN index # 314\n",
      "done for ticker LH index # 315\n",
      "done for ticker EXR index # 316\n",
      "done for ticker PWR index # 317\n",
      "done for ticker CF index # 318\n",
      "done for ticker CAH index # 319\n",
      "done for ticker CFG index # 320\n",
      "done for ticker XYL index # 321\n",
      "done for ticker HPE index # 322\n",
      "done for ticker EPAM index # 323\n",
      "done for ticker DOV index # 324\n",
      "done for ticker WAT index # 325\n",
      "done for ticker WRB index # 326\n",
      "done for ticker TDY index # 327\n",
      "done for ticker PAYC index # 328\n",
      "done for ticker ROL index # 329\n",
      "done for ticker NTRS index # 330\n",
      "done for ticker MRO index # 331\n",
      "done for ticker CNP index # 332\n",
      "done for ticker INVH index # 333\n",
      "done for ticker CHD index # 334\n",
      "done for ticker AES index # 335\n",
      "done for ticker MOH index # 336\n",
      "done for ticker JBHT index # 337\n",
      "done for ticker MAA index # 338\n",
      "done for ticker BBY index # 339\n",
      "done for ticker CLX index # 340\n",
      "done for ticker HOLX index # 341\n",
      "done for ticker WAB index # 342\n",
      "done for ticker DRI index # 343\n",
      "done for ticker EXPD index # 344\n",
      "done for ticker STE index # 345\n",
      "done for ticker AMCR index # 346\n",
      "done for ticker VTR index # 347\n",
      "done for ticker IEX index # 348\n",
      "done for ticker CAG index # 349\n",
      "done for ticker CMS index # 350\n",
      "done for ticker KEY index # 351\n",
      "done for ticker MPWR index # 352\n",
      "done for ticker BALL index # 353\n",
      "done for ticker J index # 354\n",
      "done for ticker BR index # 355\n",
      "done for ticker GRMN index # 356\n",
      "done for ticker PKI index # 357\n",
      "done for ticker TTWO index # 358\n",
      "done for ticker INCY index # 359\n",
      "done for ticker FDS index # 360\n",
      "done for ticker MOS index # 361\n",
      "done for ticker SEDG index # 362\n",
      "done for ticker CINF index # 363\n",
      "done for ticker ABMD index # 364\n",
      "done for ticker DGX index # 365\n",
      "done for ticker WST index # 366\n",
      "done for ticker ATO index # 367\n",
      "done for ticker TRGP index # 368\n",
      "done for ticker BRO index # 369\n",
      "done for ticker SYF index # 370\n",
      "done for ticker FOX index # 371\n",
      "done for ticker FOXA index # 372\n",
      "done for ticker NTAP index # 373\n",
      "done for ticker FMC index # 374\n",
      "done for ticker EQT index # 375\n",
      "done for ticker OMC index # 376\n",
      "done for ticker SJM index # 377\n",
      "done for ticker LYV index # 378\n",
      "done for ticker CPB index # 379\n",
      "done for ticker HWM index # 380\n",
      "done for ticker CPRT index # 381\n",
      "done for ticker AVY index # 382\n",
      "done for ticker IRM index # 383\n",
      "done for ticker COO index # 384\n",
      "done for ticker ALGN index # 385\n",
      "done for ticker SWKS index # 386\n",
      "done for ticker EXPE index # 387\n",
      "done for ticker RCL index # 388\n",
      "done for ticker ETSY index # 389\n",
      "done for ticker APA index # 390\n",
      "done for ticker GEN index # 391\n",
      "done for ticker TXT index # 392\n",
      "done for ticker LDOS index # 393\n",
      "done for ticker LKQ index # 394\n",
      "done for ticker TER index # 395\n",
      "done for ticker PTC index # 396\n",
      "done for ticker TRMB index # 397\n",
      "done for ticker AKAM index # 398\n",
      "done for ticker NVR index # 399\n",
      "done for ticker UAL index # 400\n",
      "done for ticker LNT index # 401\n",
      "done for ticker FLT index # 402\n",
      "done for ticker KIM index # 403\n",
      "done for ticker ZBRA index # 404\n",
      "done for ticker TYL index # 405\n",
      "done for ticker DPZ index # 406\n",
      "done for ticker JKHY index # 407\n",
      "done for ticker MGM index # 408\n",
      "done for ticker ESS index # 409\n",
      "done for ticker L index # 410\n",
      "done for ticker PEAK index # 411\n",
      "done for ticker MTCH index # 412\n",
      "done for ticker NDSN index # 413\n",
      "done for ticker EVRG index # 414\n",
      "done for ticker IPG index # 415\n",
      "done for ticker BEN index # 416\n",
      "done for ticker CBOE index # 417\n",
      "done for ticker TECH index # 418\n",
      "done for ticker SIVB index # 419\n",
      "done for ticker VFC index # 420\n",
      "done for ticker IP index # 421\n",
      "done for ticker HST index # 422\n",
      "done for ticker UDR index # 423\n",
      "done for ticker POOL index # 424\n",
      "done for ticker RE index # 425\n",
      "done for ticker PARA index # 426\n",
      "done for ticker SNA index # 427\n",
      "done for ticker CPT index # 428\n",
      "done for ticker LW index # 429\n",
      "done for ticker PKG index # 430\n",
      "done for ticker CRL index # 431\n",
      "done for ticker SWK index # 432\n",
      "done for ticker BIO index # 433\n",
      "done for ticker WDC index # 434\n",
      "done for ticker CHRW index # 435\n",
      "done for ticker STX index # 436\n",
      "done for ticker MAS index # 437\n",
      "done for ticker GL index # 438\n",
      "done for ticker CE index # 439\n",
      "done for ticker REG index # 440\n",
      "done for ticker NI index # 441\n",
      "done for ticker BXP index # 442\n",
      "done for ticker HSIC index # 443\n",
      "done for ticker CCL index # 444\n",
      "done for ticker TFX index # 445\n",
      "done for ticker CZR index # 446\n",
      "done for ticker NWS index # 447\n",
      "done for ticker NWSA index # 448\n",
      "done for ticker KMX index # 449\n",
      "done for ticker EMN index # 450\n",
      "done for ticker JNPR index # 451\n",
      "done for ticker PHM index # 452\n",
      "done for ticker CDAY index # 453\n",
      "done for ticker ALLE index # 454\n",
      "done for ticker QRVO index # 455\n",
      "done for ticker BWA index # 456\n",
      "done for ticker NRG index # 457\n",
      "done for ticker MKTX index # 458\n",
      "done for ticker WRK index # 459\n",
      "done for ticker UHS index # 460\n",
      "done for ticker FFIV index # 461\n",
      "done for ticker AOS index # 462\n",
      "done for ticker CMA index # 463\n",
      "done for ticker AAL index # 464\n",
      "done for ticker BBWI index # 465\n",
      "done for ticker HII index # 466\n",
      "done for ticker AAP index # 467\n",
      "done for ticker TPR index # 468\n",
      "done for ticker FRT index # 469\n",
      "done for ticker IVZ index # 470\n",
      "done for ticker PNW index # 471\n",
      "done for ticker HAS index # 472\n",
      "done for ticker WYNN index # 473\n",
      "done for ticker FBHS index # 474\n",
      "done for ticker SBNY index # 475\n",
      "done for ticker DISH index # 476\n",
      "done for ticker RHI index # 477\n",
      "done for ticker WHR index # 478\n",
      "done for ticker ZION index # 479\n",
      "done for ticker CTLT index # 480\n",
      "done for ticker PNR index # 481\n",
      "done for ticker SEE index # 482\n",
      "done for ticker RL index # 483\n",
      "done for ticker NCLH index # 484\n",
      "done for ticker DXC index # 485\n",
      "done for ticker GNRC index # 486\n",
      "done for ticker AIZ index # 487\n",
      "done for ticker XRAY index # 488\n",
      "done for ticker LNC index # 489\n",
      "done for ticker DVA index # 490\n",
      "done for ticker MHK index # 491\n",
      "done for ticker LUMN index # 492\n",
      "done for ticker ALK index # 493\n",
      "done for ticker NWL index # 494\n",
      "done for ticker VNO index # 495\n",
      "done for ticker TAP index # 496\n"
     ]
    }
   ],
   "source": [
    "pca_dict_score = {}\n",
    "tickers = utils.return_tickers()\n",
    "for ticker in tickers:\n",
    "        dict_pca_score[ticker] = cross_validate_ml(prep.ready_to_train_df(ticker), pipe)\n",
    "        print(f\"done for ticker {ticker} index # {tickers.index(ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fef81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fdfc2c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:26:43.265897Z",
     "start_time": "2022-12-06T11:06:29.287564Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker AAPL index # 0\n",
      "done for ticker MSFT index # 1\n",
      "done for ticker GOOG index # 2\n",
      "done for ticker AMZN index # 3\n",
      "done for ticker TSLA index # 4\n",
      "done for ticker UNH index # 5\n",
      "done for ticker XOM index # 6\n",
      "done for ticker JNJ index # 7\n",
      "done for ticker WMT index # 8\n",
      "done for ticker NVDA index # 9\n",
      "done for ticker JPM index # 10\n",
      "done for ticker V index # 11\n",
      "done for ticker CVX index # 12\n",
      "done for ticker PG index # 13\n",
      "done for ticker LLY index # 14\n",
      "done for ticker MA index # 15\n",
      "done for ticker HD index # 16\n",
      "done for ticker META index # 17\n",
      "done for ticker BAC index # 18\n",
      "done for ticker ABBV index # 19\n",
      "done for ticker PFE index # 20\n",
      "done for ticker KO index # 21\n",
      "done for ticker MRK index # 22\n",
      "done for ticker PEP index # 23\n",
      "done for ticker COST index # 24\n",
      "done for ticker ORCL index # 25\n",
      "done for ticker AVGO index # 26\n",
      "done for ticker TMO index # 27\n",
      "done for ticker MCD index # 28\n",
      "done for ticker CSCO index # 29\n",
      "done for ticker ACN index # 30\n",
      "done for ticker DHR index # 31\n",
      "done for ticker TMUS index # 32\n",
      "done for ticker ABT index # 33\n",
      "done for ticker WFC index # 34\n",
      "done for ticker DIS index # 35\n",
      "done for ticker LIN index # 36\n",
      "done for ticker NEE index # 37\n",
      "done for ticker BMY index # 38\n",
      "done for ticker NKE index # 39\n",
      "done for ticker VZ index # 40\n",
      "done for ticker TXN index # 41\n",
      "done for ticker UPS index # 42\n",
      "done for ticker COP index # 43\n",
      "done for ticker ADBE index # 44\n",
      "done for ticker CMCSA index # 45\n",
      "done for ticker CRM index # 46\n",
      "done for ticker PM index # 47\n",
      "done for ticker MS index # 48\n",
      "done for ticker AMGN index # 49\n",
      "done for ticker SCHW index # 50\n",
      "done for ticker HON index # 51\n",
      "done for ticker RTX index # 52\n",
      "done for ticker QCOM index # 53\n",
      "done for ticker T index # 54\n",
      "done for ticker IBM index # 55\n",
      "done for ticker DE index # 56\n",
      "done for ticker CVS index # 57\n",
      "done for ticker LOW index # 58\n",
      "done for ticker GS index # 59\n",
      "done for ticker UNP index # 60\n",
      "done for ticker NFLX index # 61\n",
      "done for ticker LMT index # 62\n",
      "done for ticker CAT index # 63\n",
      "done for ticker AMD index # 64\n",
      "done for ticker INTC index # 65\n",
      "done for ticker ELV index # 66\n",
      "done for ticker SPGI index # 67\n",
      "done for ticker AXP index # 68\n",
      "done for ticker SBUX index # 69\n",
      "done for ticker INTU index # 70\n",
      "done for ticker BLK index # 71\n",
      "done for ticker ADP index # 72\n",
      "done for ticker GILD index # 73\n",
      "done for ticker PLD index # 74\n",
      "done for ticker MDT index # 75\n",
      "done for ticker BA index # 76\n",
      "done for ticker AMT index # 77\n",
      "done for ticker CI index # 78\n",
      "done for ticker GE index # 79\n",
      "done for ticker TJX index # 80\n",
      "done for ticker ISRG index # 81\n",
      "done for ticker C index # 82\n",
      "done for ticker AMAT index # 83\n",
      "done for ticker PYPL index # 84\n",
      "done for ticker MDLZ index # 85\n",
      "done for ticker CB index # 86\n",
      "done for ticker SYK index # 87\n",
      "done for ticker ADI index # 88\n",
      "done for ticker MMC index # 89\n",
      "done for ticker EOG index # 90\n",
      "done for ticker NOW index # 91\n",
      "done for ticker VRTX index # 92\n",
      "done for ticker MO index # 93\n",
      "done for ticker NOC index # 94\n",
      "done for ticker EL index # 95\n",
      "done for ticker REGN index # 96\n",
      "done for ticker PGR index # 97\n",
      "done for ticker BKNG index # 98\n",
      "done for ticker DUK index # 99\n",
      "done for ticker TGT index # 100\n",
      "done for ticker SLB index # 101\n",
      "done for ticker SO index # 102\n",
      "done for ticker MMM index # 103\n",
      "done for ticker ITW index # 104\n",
      "done for ticker ZTS index # 105\n",
      "done for ticker GD index # 106\n",
      "done for ticker APD index # 107\n",
      "done for ticker HUM index # 108\n",
      "done for ticker MRNA index # 109\n",
      "done for ticker BDX index # 110\n",
      "done for ticker CSX index # 111\n",
      "done for ticker WM index # 112\n",
      "done for ticker PNC index # 113\n",
      "done for ticker HCA index # 114\n",
      "done for ticker ETN index # 115\n",
      "done for ticker USB index # 116\n",
      "done for ticker FISV index # 117\n",
      "done for ticker SHW index # 118\n",
      "done for ticker OXY index # 119\n",
      "done for ticker CL index # 120\n",
      "done for ticker MU index # 121\n",
      "done for ticker CME index # 122\n",
      "done for ticker AON index # 123\n",
      "done for ticker LRCX index # 124\n",
      "done for ticker BSX index # 125\n",
      "done for ticker EQIX index # 126\n",
      "done for ticker TFC index # 127\n",
      "done for ticker PXD index # 128\n",
      "done for ticker CHTR index # 129\n",
      "done for ticker CCI index # 130\n",
      "done for ticker MET index # 131\n",
      "done for ticker ATVI index # 132\n",
      "done for ticker ICE index # 133\n",
      "done for ticker MPC index # 134\n",
      "done for ticker NSC index # 135\n",
      "done for ticker DG index # 136\n",
      "done for ticker GM index # 137\n",
      "done for ticker EMR index # 138\n",
      "done for ticker F index # 139\n",
      "done for ticker KLAC index # 140\n",
      "done for ticker MCO index # 141\n",
      "done for ticker FCX index # 142\n",
      "done for ticker KDP index # 143\n",
      "done for ticker MNST index # 144\n",
      "done for ticker MCK index # 145\n",
      "done for ticker VLO index # 146\n",
      "done for ticker ORLY index # 147\n",
      "done for ticker ADM index # 148\n",
      "done for ticker PSX index # 149\n",
      "done for ticker PSA index # 150\n",
      "done for ticker SRE index # 151\n",
      "done for ticker SNPS index # 152\n",
      "done for ticker MAR index # 153\n",
      "done for ticker D index # 154\n",
      "done for ticker GIS index # 155\n",
      "done for ticker AEP index # 156\n",
      "done for ticker AZO index # 157\n",
      "done for ticker KHC index # 158\n",
      "done for ticker APH index # 159\n",
      "done for ticker HSY index # 160\n",
      "done for ticker CNC index # 161\n",
      "done for ticker CTVA index # 162\n",
      "done for ticker EW index # 163\n",
      "done for ticker CTAS index # 164\n",
      "done for ticker A index # 165\n",
      "done for ticker ROP index # 166\n",
      "done for ticker JCI index # 167\n",
      "done for ticker CDNS index # 168\n",
      "done for ticker FDX index # 169\n",
      "done for ticker NXPI index # 170\n",
      "done for ticker AIG index # 171\n",
      "done for ticker KMB index # 172\n",
      "done for ticker AFL index # 173\n",
      "done for ticker HES index # 174\n",
      "done for ticker MSI index # 175\n",
      "done for ticker PAYX index # 176\n",
      "done for ticker DVN index # 177\n",
      "done for ticker TRV index # 178\n",
      "done for ticker BIIB index # 179\n",
      "done for ticker DXCM index # 180\n",
      "done for ticker SYY index # 181\n",
      "done for ticker LHX index # 182\n",
      "done for ticker RSG index # 183\n",
      "done for ticker ENPH index # 184\n",
      "done for ticker ECL index # 185\n",
      "done for ticker ADSK index # 186\n",
      "done for ticker MCHP index # 187\n",
      "done for ticker ANET index # 188\n",
      "done for ticker KMI index # 189\n",
      "done for ticker CMG index # 190\n",
      "done for ticker FTNT index # 191\n",
      "done for ticker AJG index # 192\n",
      "done for ticker STZ index # 193\n",
      "done for ticker TT index # 194\n",
      "done for ticker WMB index # 195\n",
      "done for ticker MSCI index # 196\n",
      "done for ticker O index # 197\n",
      "done for ticker IQV index # 198\n",
      "done for ticker TEL index # 199\n",
      "done for ticker ROST index # 200\n",
      "done for ticker PRU index # 201\n",
      "done for ticker EXC index # 202\n",
      "done for ticker PH index # 203\n",
      "done for ticker FIS index # 204\n",
      "done for ticker SPG index # 205\n",
      "done for ticker COF index # 206\n",
      "done for ticker NUE index # 207\n",
      "done for ticker XEL index # 208\n",
      "done for ticker HLT index # 209\n",
      "done for ticker CARR index # 210\n",
      "done for ticker PCAR index # 211\n",
      "done for ticker BK index # 212\n",
      "done for ticker NEM index # 213\n",
      "done for ticker DOW index # 214\n",
      "done for ticker EA index # 215\n",
      "done for ticker WBA index # 216\n",
      "done for ticker DD index # 217\n",
      "done for ticker ALL index # 218\n",
      "done for ticker YUM index # 219\n",
      "done for ticker AMP index # 220\n",
      "done for ticker CMI index # 221\n",
      "done for ticker ILMN index # 222\n",
      "done for ticker TDG index # 223\n",
      "done for ticker IDXX index # 224\n",
      "done for ticker ED index # 225\n",
      "done for ticker KR index # 226\n",
      "done for ticker ABC index # 227\n",
      "done for ticker DLTR index # 228\n",
      "done for ticker RMD index # 229\n",
      "done for ticker ALB index # 230\n",
      "done for ticker HAL index # 231\n",
      "done for ticker NDAQ index # 232\n",
      "done for ticker LVS index # 233\n",
      "done for ticker ODFL index # 234\n",
      "done for ticker WELL index # 235\n",
      "done for ticker AME index # 236\n",
      "done for ticker CSGP index # 237\n",
      "done for ticker OTIS index # 238\n",
      "done for ticker MTD index # 239\n",
      "done for ticker SBAC index # 240\n",
      "done for ticker ON index # 241\n",
      "done for ticker VICI index # 242\n",
      "done for ticker DLR index # 243\n",
      "done for ticker KEYS index # 244\n",
      "done for ticker PPG index # 245\n",
      "done for ticker WEC index # 246\n",
      "done for ticker CTSH index # 247\n",
      "done for ticker ROK index # 248\n",
      "done for ticker GWW index # 249\n",
      "done for ticker PCG index # 250\n",
      "done for ticker HPQ index # 251\n",
      "done for ticker FAST index # 252\n",
      "done for ticker DFS index # 253\n",
      "done for ticker MTB index # 254\n",
      "done for ticker PEG index # 255\n",
      "done for ticker OKE index # 256\n",
      "done for ticker DHI index # 257\n",
      "done for ticker APTV index # 258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker BKR index # 259\n",
      "done for ticker GLW index # 260\n",
      "done for ticker LYB index # 261\n",
      "done for ticker ES index # 262\n",
      "done for ticker BAX index # 263\n",
      "done for ticker STT index # 264\n",
      "done for ticker VRSK index # 265\n",
      "done for ticker TROW index # 266\n",
      "done for ticker WBD index # 267\n",
      "done for ticker AWK index # 268\n",
      "done for ticker IT index # 269\n",
      "done for ticker GPN index # 270\n",
      "done for ticker HRL index # 271\n",
      "done for ticker FANG index # 272\n",
      "done for ticker WTW index # 273\n",
      "done for ticker RJF index # 274\n",
      "done for ticker GPC index # 275\n",
      "done for ticker IFF index # 276\n",
      "done for ticker CDW index # 277\n",
      "done for ticker TSCO index # 278\n",
      "done for ticker FITB index # 279\n",
      "done for ticker ARE index # 280\n",
      "done for ticker URI index # 281\n",
      "done for ticker ZBH index # 282\n",
      "done for ticker K index # 283\n",
      "done for ticker LEN index # 284\n",
      "done for ticker EBAY index # 285\n",
      "done for ticker EIX index # 286\n",
      "done for ticker CBRE index # 287\n",
      "done for ticker EFX index # 288\n",
      "done for ticker VMC index # 289\n",
      "done for ticker TSN index # 290\n",
      "done for ticker HIG index # 291\n",
      "done for ticker FTV index # 292\n",
      "done for ticker WY index # 293\n",
      "done for ticker EQR index # 294\n",
      "done for ticker AVB index # 295\n",
      "done for ticker MKC index # 296\n",
      "done for ticker ETR index # 297\n",
      "done for ticker LUV index # 298\n",
      "done for ticker ULTA index # 299\n",
      "done for ticker AEE index # 300\n",
      "done for ticker MLM index # 301\n",
      "done for ticker FE index # 302\n",
      "done for ticker PFG index # 303\n",
      "done for ticker FRC index # 304\n",
      "done for ticker DTE index # 305\n",
      "done for ticker DAL index # 306\n",
      "done for ticker HBAN index # 307\n",
      "done for ticker IR index # 308\n",
      "done for ticker CTRA index # 309\n",
      "done for ticker ANSS index # 310\n",
      "done for ticker ACGL index # 311\n",
      "done for ticker PPL index # 312\n",
      "done for ticker RF index # 313\n",
      "done for ticker VRSN index # 314\n",
      "done for ticker LH index # 315\n",
      "done for ticker EXR index # 316\n",
      "done for ticker PWR index # 317\n",
      "done for ticker CF index # 318\n",
      "done for ticker CAH index # 319\n",
      "done for ticker CFG index # 320\n",
      "done for ticker XYL index # 321\n",
      "done for ticker HPE index # 322\n",
      "done for ticker EPAM index # 323\n",
      "done for ticker DOV index # 324\n",
      "done for ticker WAT index # 325\n",
      "done for ticker WRB index # 326\n",
      "done for ticker TDY index # 327\n",
      "done for ticker PAYC index # 328\n",
      "done for ticker ROL index # 329\n",
      "done for ticker NTRS index # 330\n",
      "done for ticker MRO index # 331\n",
      "done for ticker CNP index # 332\n",
      "done for ticker INVH index # 333\n",
      "done for ticker CHD index # 334\n",
      "done for ticker AES index # 335\n",
      "done for ticker MOH index # 336\n",
      "done for ticker JBHT index # 337\n",
      "done for ticker MAA index # 338\n",
      "done for ticker BBY index # 339\n",
      "done for ticker CLX index # 340\n",
      "done for ticker HOLX index # 341\n",
      "done for ticker WAB index # 342\n",
      "done for ticker DRI index # 343\n",
      "done for ticker EXPD index # 344\n",
      "done for ticker STE index # 345\n",
      "done for ticker AMCR index # 346\n",
      "done for ticker VTR index # 347\n",
      "done for ticker IEX index # 348\n",
      "done for ticker CAG index # 349\n",
      "done for ticker CMS index # 350\n",
      "done for ticker KEY index # 351\n",
      "done for ticker MPWR index # 352\n",
      "done for ticker BALL index # 353\n",
      "done for ticker J index # 354\n",
      "done for ticker BR index # 355\n",
      "done for ticker GRMN index # 356\n",
      "done for ticker PKI index # 357\n",
      "done for ticker TTWO index # 358\n",
      "done for ticker INCY index # 359\n",
      "done for ticker FDS index # 360\n",
      "done for ticker MOS index # 361\n",
      "done for ticker SEDG index # 362\n",
      "done for ticker CINF index # 363\n",
      "done for ticker ABMD index # 364\n",
      "done for ticker DGX index # 365\n",
      "done for ticker WST index # 366\n",
      "done for ticker ATO index # 367\n",
      "done for ticker TRGP index # 368\n",
      "done for ticker BRO index # 369\n",
      "done for ticker SYF index # 370\n",
      "done for ticker FOX index # 371\n",
      "done for ticker FOXA index # 372\n",
      "done for ticker NTAP index # 373\n",
      "done for ticker FMC index # 374\n",
      "done for ticker EQT index # 375\n",
      "done for ticker OMC index # 376\n",
      "done for ticker SJM index # 377\n",
      "done for ticker LYV index # 378\n",
      "done for ticker CPB index # 379\n",
      "done for ticker HWM index # 380\n",
      "done for ticker CPRT index # 381\n",
      "done for ticker AVY index # 382\n",
      "done for ticker IRM index # 383\n",
      "done for ticker COO index # 384\n",
      "done for ticker ALGN index # 385\n",
      "done for ticker SWKS index # 386\n",
      "done for ticker EXPE index # 387\n",
      "done for ticker RCL index # 388\n",
      "done for ticker ETSY index # 389\n",
      "done for ticker APA index # 390\n",
      "done for ticker GEN index # 391\n",
      "done for ticker TXT index # 392\n",
      "done for ticker LDOS index # 393\n",
      "done for ticker LKQ index # 394\n",
      "done for ticker TER index # 395\n",
      "done for ticker PTC index # 396\n",
      "done for ticker TRMB index # 397\n",
      "done for ticker AKAM index # 398\n",
      "done for ticker NVR index # 399\n",
      "done for ticker UAL index # 400\n",
      "done for ticker LNT index # 401\n",
      "done for ticker FLT index # 402\n",
      "done for ticker KIM index # 403\n",
      "done for ticker ZBRA index # 404\n",
      "done for ticker TYL index # 405\n",
      "done for ticker DPZ index # 406\n",
      "done for ticker JKHY index # 407\n",
      "done for ticker MGM index # 408\n",
      "done for ticker ESS index # 409\n",
      "done for ticker L index # 410\n",
      "done for ticker PEAK index # 411\n",
      "done for ticker MTCH index # 412\n",
      "done for ticker NDSN index # 413\n",
      "done for ticker EVRG index # 414\n",
      "done for ticker IPG index # 415\n",
      "done for ticker BEN index # 416\n",
      "done for ticker CBOE index # 417\n",
      "done for ticker TECH index # 418\n",
      "done for ticker SIVB index # 419\n",
      "done for ticker VFC index # 420\n",
      "done for ticker IP index # 421\n",
      "done for ticker HST index # 422\n",
      "done for ticker UDR index # 423\n",
      "done for ticker POOL index # 424\n",
      "done for ticker RE index # 425\n",
      "done for ticker PARA index # 426\n",
      "done for ticker SNA index # 427\n",
      "done for ticker CPT index # 428\n",
      "done for ticker LW index # 429\n",
      "done for ticker PKG index # 430\n",
      "done for ticker CRL index # 431\n",
      "done for ticker SWK index # 432\n",
      "done for ticker BIO index # 433\n",
      "done for ticker WDC index # 434\n",
      "done for ticker CHRW index # 435\n",
      "done for ticker STX index # 436\n",
      "done for ticker MAS index # 437\n",
      "done for ticker GL index # 438\n",
      "done for ticker CE index # 439\n",
      "done for ticker REG index # 440\n",
      "done for ticker NI index # 441\n",
      "done for ticker BXP index # 442\n",
      "done for ticker HSIC index # 443\n",
      "done for ticker CCL index # 444\n",
      "done for ticker TFX index # 445\n",
      "done for ticker CZR index # 446\n",
      "done for ticker NWS index # 447\n",
      "done for ticker NWSA index # 448\n",
      "done for ticker KMX index # 449\n",
      "done for ticker EMN index # 450\n",
      "done for ticker JNPR index # 451\n",
      "done for ticker PHM index # 452\n",
      "done for ticker CDAY index # 453\n",
      "done for ticker ALLE index # 454\n",
      "done for ticker QRVO index # 455\n",
      "done for ticker BWA index # 456\n",
      "done for ticker NRG index # 457\n",
      "done for ticker MKTX index # 458\n",
      "done for ticker WRK index # 459\n",
      "done for ticker UHS index # 460\n",
      "done for ticker FFIV index # 461\n",
      "done for ticker AOS index # 462\n",
      "done for ticker CMA index # 463\n",
      "done for ticker AAL index # 464\n",
      "done for ticker BBWI index # 465\n",
      "done for ticker HII index # 466\n",
      "done for ticker AAP index # 467\n",
      "done for ticker TPR index # 468\n",
      "done for ticker FRT index # 469\n",
      "done for ticker IVZ index # 470\n",
      "done for ticker PNW index # 471\n",
      "done for ticker HAS index # 472\n",
      "done for ticker WYNN index # 473\n",
      "done for ticker FBHS index # 474\n",
      "done for ticker SBNY index # 475\n",
      "done for ticker DISH index # 476\n",
      "done for ticker RHI index # 477\n",
      "done for ticker WHR index # 478\n",
      "done for ticker ZION index # 479\n",
      "done for ticker CTLT index # 480\n",
      "done for ticker PNR index # 481\n",
      "done for ticker SEE index # 482\n",
      "done for ticker RL index # 483\n",
      "done for ticker NCLH index # 484\n",
      "done for ticker DXC index # 485\n",
      "done for ticker GNRC index # 486\n",
      "done for ticker AIZ index # 487\n",
      "done for ticker XRAY index # 488\n",
      "done for ticker LNC index # 489\n",
      "done for ticker DVA index # 490\n",
      "done for ticker MHK index # 491\n",
      "done for ticker LUMN index # 492\n",
      "done for ticker ALK index # 493\n",
      "done for ticker NWL index # 494\n",
      "done for ticker VNO index # 495\n",
      "done for ticker TAP index # 496\n"
     ]
    }
   ],
   "source": [
    "pca_dict_score = {}\n",
    "tickers = utils.return_tickers()\n",
    "for ticker in tickers:\n",
    "    pca = PCA(n_components=0.9)\n",
    "    ticker_df = prep.ready_to_train_df(ticker)\n",
    "    pca.fit(ticker_df)\n",
    "    n_pcs= pca.n_components_\n",
    "    most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "    initial_feature_names = ticker_df.columns\n",
    "    most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "    pca_df = ticker_df[list(np.unique(most_important_names))] \n",
    "    returns = ticker_df[['return']]\n",
    "    final_pca = pd.merge(pca_df, returns, how='outer', left_index=True, right_index=True)\n",
    "    pca_dict_score[ticker] = cross_validate_ml(final_pca, GradientBoostingRegressor(max_depth = 3, \n",
    "                                                                                    criterion='friedman_mse',\n",
    "                                                                                    learning_rate=0.08,\n",
    "                                                                                    n_estimators = 100))\n",
    "    print(f\"done for ticker {ticker} index # {tickers.index(ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f166c487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:31:28.459421Z",
     "start_time": "2022-12-06T11:31:28.426417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0033498792960944923 0.02096529727516623\n"
     ]
    }
   ],
   "source": [
    "rmse_pca= []\n",
    "baseline = []\n",
    "for key in pca_dict_score.keys():\n",
    "    rmse_pca.append(pca_dict_score[key][0])\n",
    "    baseline.append(pca_dict_score[key][1])\n",
    "print(np.mean(rmse_pca), np.mean(baseline) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c29d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:44:19.570067Z",
     "start_time": "2022-12-02T08:44:19.401956Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f203d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:44:22.236582Z",
     "start_time": "2022-12-02T08:44:22.123704Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.ylim(ymin=0)\n",
    "plt.title('cumulated share of explained variance')\n",
    "plt.xlabel('# of principal component used');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559acad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T08:21:43.359186Z",
     "start_time": "2022-12-06T08:21:42.741327Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dynamic_portfolio.preprocess as prep\n",
    "import dynamic_portfolio.utils as utils\n",
    "import dynamic_portfolio.cross_validate as cv\n",
    "import dynamic_portfolio.models as model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import warnings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings(action='ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd085a7",
   "metadata": {},
   "source": [
    "### Cross validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c135dd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T08:21:49.050523Z",
     "start_time": "2022-12-06T08:21:49.017552Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "cross_val = {\n",
    "    'fold_length' : 252, # Working days for 1 year\n",
    "    'fold_stride' : 60, # Step between folds, here one quarter\n",
    "    'train_test_ratio' : 0.7, # Split in fold\n",
    "    'input_length' : 0, # Number of days to move back from last train_index, here 0\n",
    "    'horizon' : 1, # Number of days ahead to make prediction, here 1\n",
    "    'output_length' : 1, # Number of targets wanted\n",
    "}\n",
    "#Split the dataset by FOLDS\n",
    "def get_folds(\n",
    "    df: pd.DataFrame,\n",
    "    fold_length = cross_val['fold_length'],\n",
    "    fold_stride = cross_val['fold_stride']):\n",
    "    '''\n",
    "    This function slides through the Time Series dataframe of shape (n_timesteps, n_features) to create folds\n",
    "    - of equal `fold_length`\n",
    "    - using `fold_stride` between each fold\n",
    "    Returns a list of folds, each as a DataFrame\n",
    "    '''\n",
    "    folds = []\n",
    "    for idx in range(0, len(df), fold_stride):\n",
    "        # Exits the loop as soon as the last fold index would exceed the last index\n",
    "        if (idx + fold_length) > len(df):\n",
    "            break\n",
    "        fold = df.iloc[idx:idx + fold_length, :]\n",
    "        folds.append(fold)\n",
    "    return folds\n",
    "#Split FOLDS by Train et Test\n",
    "#### FOR ONE FOLD !!!!!\n",
    "def train_test_split(fold: pd.DataFrame,\n",
    "                     train_test_ratio = cross_val['train_test_ratio'],\n",
    "                     input_length = cross_val['input_length']):\n",
    "    '''\n",
    "    Returns a train dataframe and a test dataframe (fold_train, fold_test)\n",
    "    from which one can sample (X,y) sequences.\n",
    "    df_train should contain all the timesteps until round(train_test_ratio * len(fold))\n",
    "    '''\n",
    "    # TRAIN SET\n",
    "    # ======================\n",
    "    last_train_idx = round(train_test_ratio * len(fold))\n",
    "    fold_train = fold.iloc[0:last_train_idx, :]\n",
    "    # TEST SET\n",
    "    # ======================\n",
    "    first_test_idx = last_train_idx - input_length \n",
    "    fold_test = fold.iloc[first_test_idx:, :]\n",
    "    return (fold_train, fold_test)\n",
    "def cross_validate_ml(df, model) :\n",
    "    '''\n",
    "    get_folds() create many FOLDS, train_test_split() create a split on ONE FOLDS.\n",
    "    The goal of this function is to make splits and sequences on each FOLDS.\n",
    "    Then, apply a model.\n",
    "    '''\n",
    "    folds = get_folds(df, fold_length = cross_val['fold_length'], fold_stride = cross_val['fold_stride']) # 1 - Creating FOLDS\n",
    "    scores =[]\n",
    "    baseline = []\n",
    "    for fold in folds:\n",
    "        # 2 - CHRONOLOGICAL TRAIN TEST SPLIT of the current FOLD\n",
    "        (fold_train, fold_test) = train_test_split(fold = fold,\n",
    "                                                train_test_ratio = cross_val['train_test_ratio'],\n",
    "                                                input_length = cross_val['input_length'] ,\n",
    "                                                )\n",
    "        # 3 - Scanninng fold_train and fold_test for SEQUENCES\n",
    "        X_train, y_train = fold_train, fold_train['return']\n",
    "        X_test, y_test = fold_test, fold_test['return']\n",
    "        model.fit(X_train, y_train)\n",
    "        rmse_model = (mean_squared_error(y_test, model.predict(X_test)))**0.5\n",
    "        scores.append(rmse_model)\n",
    "        rmse_baseline = mean_squared_error(y_test.iloc[[0]], y_train.iloc[[-1]])**0.5\n",
    "        baseline.append(rmse_baseline)\n",
    "    return np.mean(scores), np.mean(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda975f6",
   "metadata": {},
   "source": [
    "### Script to run for model on all stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd297e",
   "metadata": {},
   "source": [
    "### Models used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "631163f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:44:15.680313Z",
     "start_time": "2022-12-06T09:44:15.669340Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deca181c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:30:38.946985Z",
     "start_time": "2022-12-06T09:46:44.058925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker AAPL index # 0\n",
      "done for ticker MSFT index # 1\n",
      "done for ticker GOOG index # 2\n",
      "done for ticker AMZN index # 3\n",
      "done for ticker TSLA index # 4\n",
      "done for ticker UNH index # 5\n",
      "done for ticker XOM index # 6\n",
      "done for ticker JNJ index # 7\n",
      "done for ticker WMT index # 8\n",
      "done for ticker NVDA index # 9\n",
      "done for ticker JPM index # 10\n",
      "done for ticker V index # 11\n",
      "done for ticker CVX index # 12\n",
      "done for ticker PG index # 13\n",
      "done for ticker LLY index # 14\n",
      "done for ticker MA index # 15\n",
      "done for ticker HD index # 16\n",
      "done for ticker META index # 17\n",
      "done for ticker BAC index # 18\n",
      "done for ticker ABBV index # 19\n",
      "done for ticker PFE index # 20\n",
      "done for ticker KO index # 21\n",
      "done for ticker MRK index # 22\n",
      "done for ticker PEP index # 23\n",
      "done for ticker COST index # 24\n",
      "done for ticker ORCL index # 25\n",
      "done for ticker AVGO index # 26\n",
      "done for ticker TMO index # 27\n",
      "done for ticker MCD index # 28\n",
      "done for ticker CSCO index # 29\n",
      "done for ticker ACN index # 30\n",
      "done for ticker DHR index # 31\n",
      "done for ticker TMUS index # 32\n",
      "done for ticker ABT index # 33\n",
      "done for ticker WFC index # 34\n",
      "done for ticker DIS index # 35\n",
      "done for ticker LIN index # 36\n",
      "done for ticker NEE index # 37\n",
      "done for ticker BMY index # 38\n",
      "done for ticker NKE index # 39\n",
      "done for ticker VZ index # 40\n",
      "done for ticker TXN index # 41\n",
      "done for ticker UPS index # 42\n",
      "done for ticker COP index # 43\n",
      "done for ticker ADBE index # 44\n",
      "done for ticker CMCSA index # 45\n",
      "done for ticker CRM index # 46\n",
      "done for ticker PM index # 47\n",
      "done for ticker MS index # 48\n",
      "done for ticker AMGN index # 49\n",
      "done for ticker SCHW index # 50\n",
      "done for ticker HON index # 51\n",
      "done for ticker RTX index # 52\n",
      "done for ticker QCOM index # 53\n",
      "done for ticker T index # 54\n",
      "done for ticker IBM index # 55\n",
      "done for ticker DE index # 56\n",
      "done for ticker CVS index # 57\n",
      "done for ticker LOW index # 58\n",
      "done for ticker GS index # 59\n",
      "done for ticker UNP index # 60\n",
      "done for ticker NFLX index # 61\n",
      "done for ticker LMT index # 62\n",
      "done for ticker CAT index # 63\n",
      "done for ticker AMD index # 64\n",
      "done for ticker INTC index # 65\n",
      "done for ticker ELV index # 66\n",
      "done for ticker SPGI index # 67\n",
      "done for ticker AXP index # 68\n",
      "done for ticker SBUX index # 69\n",
      "done for ticker INTU index # 70\n",
      "done for ticker BLK index # 71\n",
      "done for ticker ADP index # 72\n",
      "done for ticker GILD index # 73\n",
      "done for ticker PLD index # 74\n",
      "done for ticker MDT index # 75\n",
      "done for ticker BA index # 76\n",
      "done for ticker AMT index # 77\n",
      "done for ticker CI index # 78\n",
      "done for ticker GE index # 79\n",
      "done for ticker TJX index # 80\n",
      "done for ticker ISRG index # 81\n",
      "done for ticker C index # 82\n",
      "done for ticker AMAT index # 83\n",
      "done for ticker PYPL index # 84\n",
      "done for ticker MDLZ index # 85\n",
      "done for ticker CB index # 86\n",
      "done for ticker SYK index # 87\n",
      "done for ticker ADI index # 88\n",
      "done for ticker MMC index # 89\n",
      "done for ticker EOG index # 90\n",
      "done for ticker NOW index # 91\n",
      "done for ticker VRTX index # 92\n",
      "done for ticker MO index # 93\n",
      "done for ticker NOC index # 94\n",
      "done for ticker EL index # 95\n",
      "done for ticker REGN index # 96\n",
      "done for ticker PGR index # 97\n",
      "done for ticker BKNG index # 98\n",
      "done for ticker DUK index # 99\n",
      "done for ticker TGT index # 100\n",
      "done for ticker SLB index # 101\n",
      "done for ticker SO index # 102\n",
      "done for ticker MMM index # 103\n",
      "done for ticker ITW index # 104\n",
      "done for ticker ZTS index # 105\n",
      "done for ticker GD index # 106\n",
      "done for ticker APD index # 107\n",
      "done for ticker HUM index # 108\n",
      "done for ticker MRNA index # 109\n",
      "done for ticker BDX index # 110\n",
      "done for ticker CSX index # 111\n",
      "done for ticker WM index # 112\n",
      "done for ticker PNC index # 113\n",
      "done for ticker HCA index # 114\n",
      "done for ticker ETN index # 115\n",
      "done for ticker USB index # 116\n",
      "done for ticker FISV index # 117\n",
      "done for ticker SHW index # 118\n",
      "done for ticker OXY index # 119\n",
      "done for ticker CL index # 120\n",
      "done for ticker MU index # 121\n",
      "done for ticker CME index # 122\n",
      "done for ticker AON index # 123\n",
      "done for ticker LRCX index # 124\n",
      "done for ticker BSX index # 125\n",
      "done for ticker EQIX index # 126\n",
      "done for ticker TFC index # 127\n",
      "done for ticker PXD index # 128\n",
      "done for ticker CHTR index # 129\n",
      "done for ticker CCI index # 130\n",
      "done for ticker MET index # 131\n",
      "done for ticker ATVI index # 132\n",
      "done for ticker ICE index # 133\n",
      "done for ticker MPC index # 134\n",
      "done for ticker NSC index # 135\n",
      "done for ticker DG index # 136\n",
      "done for ticker GM index # 137\n",
      "done for ticker EMR index # 138\n",
      "done for ticker F index # 139\n",
      "done for ticker KLAC index # 140\n",
      "done for ticker MCO index # 141\n",
      "done for ticker FCX index # 142\n",
      "done for ticker KDP index # 143\n",
      "done for ticker MNST index # 144\n",
      "done for ticker MCK index # 145\n",
      "done for ticker VLO index # 146\n",
      "done for ticker ORLY index # 147\n",
      "done for ticker ADM index # 148\n",
      "done for ticker PSX index # 149\n",
      "done for ticker PSA index # 150\n",
      "done for ticker SRE index # 151\n",
      "done for ticker SNPS index # 152\n",
      "done for ticker MAR index # 153\n",
      "done for ticker D index # 154\n",
      "done for ticker GIS index # 155\n",
      "done for ticker AEP index # 156\n",
      "done for ticker AZO index # 157\n",
      "done for ticker KHC index # 158\n",
      "done for ticker APH index # 159\n",
      "done for ticker HSY index # 160\n",
      "done for ticker CNC index # 161\n",
      "done for ticker CTVA index # 162\n",
      "done for ticker EW index # 163\n",
      "done for ticker CTAS index # 164\n",
      "done for ticker A index # 165\n",
      "done for ticker ROP index # 166\n",
      "done for ticker JCI index # 167\n",
      "done for ticker CDNS index # 168\n",
      "done for ticker FDX index # 169\n",
      "done for ticker NXPI index # 170\n",
      "done for ticker AIG index # 171\n",
      "done for ticker KMB index # 172\n",
      "done for ticker AFL index # 173\n",
      "done for ticker HES index # 174\n",
      "done for ticker MSI index # 175\n",
      "done for ticker PAYX index # 176\n",
      "done for ticker DVN index # 177\n",
      "done for ticker TRV index # 178\n",
      "done for ticker BIIB index # 179\n",
      "done for ticker DXCM index # 180\n",
      "done for ticker SYY index # 181\n",
      "done for ticker LHX index # 182\n",
      "done for ticker RSG index # 183\n",
      "done for ticker ENPH index # 184\n",
      "done for ticker ECL index # 185\n",
      "done for ticker ADSK index # 186\n",
      "done for ticker MCHP index # 187\n",
      "done for ticker ANET index # 188\n",
      "done for ticker KMI index # 189\n",
      "done for ticker CMG index # 190\n",
      "done for ticker FTNT index # 191\n",
      "done for ticker AJG index # 192\n",
      "done for ticker STZ index # 193\n",
      "done for ticker TT index # 194\n",
      "done for ticker WMB index # 195\n",
      "done for ticker MSCI index # 196\n",
      "done for ticker O index # 197\n",
      "done for ticker IQV index # 198\n",
      "done for ticker TEL index # 199\n",
      "done for ticker ROST index # 200\n",
      "done for ticker PRU index # 201\n",
      "done for ticker EXC index # 202\n",
      "done for ticker PH index # 203\n",
      "done for ticker FIS index # 204\n",
      "done for ticker SPG index # 205\n",
      "done for ticker COF index # 206\n",
      "done for ticker NUE index # 207\n",
      "done for ticker XEL index # 208\n",
      "done for ticker HLT index # 209\n",
      "done for ticker CARR index # 210\n",
      "done for ticker PCAR index # 211\n",
      "done for ticker BK index # 212\n",
      "done for ticker NEM index # 213\n",
      "done for ticker DOW index # 214\n",
      "done for ticker EA index # 215\n",
      "done for ticker WBA index # 216\n",
      "done for ticker DD index # 217\n",
      "done for ticker ALL index # 218\n",
      "done for ticker YUM index # 219\n",
      "done for ticker AMP index # 220\n",
      "done for ticker CMI index # 221\n",
      "done for ticker ILMN index # 222\n",
      "done for ticker TDG index # 223\n",
      "done for ticker IDXX index # 224\n",
      "done for ticker ED index # 225\n",
      "done for ticker KR index # 226\n",
      "done for ticker ABC index # 227\n",
      "done for ticker DLTR index # 228\n",
      "done for ticker RMD index # 229\n",
      "done for ticker ALB index # 230\n",
      "done for ticker HAL index # 231\n",
      "done for ticker NDAQ index # 232\n",
      "done for ticker LVS index # 233\n",
      "done for ticker ODFL index # 234\n",
      "done for ticker WELL index # 235\n",
      "done for ticker AME index # 236\n",
      "done for ticker CSGP index # 237\n",
      "done for ticker OTIS index # 238\n",
      "done for ticker MTD index # 239\n",
      "done for ticker SBAC index # 240\n",
      "done for ticker ON index # 241\n",
      "done for ticker VICI index # 242\n",
      "done for ticker DLR index # 243\n",
      "done for ticker KEYS index # 244\n",
      "done for ticker PPG index # 245\n",
      "done for ticker WEC index # 246\n",
      "done for ticker CTSH index # 247\n",
      "done for ticker ROK index # 248\n",
      "done for ticker GWW index # 249\n",
      "done for ticker PCG index # 250\n",
      "done for ticker HPQ index # 251\n",
      "done for ticker FAST index # 252\n",
      "done for ticker DFS index # 253\n",
      "done for ticker MTB index # 254\n",
      "done for ticker PEG index # 255\n",
      "done for ticker OKE index # 256\n",
      "done for ticker DHI index # 257\n",
      "done for ticker APTV index # 258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker BKR index # 259\n",
      "done for ticker GLW index # 260\n",
      "done for ticker LYB index # 261\n",
      "done for ticker ES index # 262\n",
      "done for ticker BAX index # 263\n",
      "done for ticker STT index # 264\n",
      "done for ticker VRSK index # 265\n",
      "done for ticker TROW index # 266\n",
      "done for ticker WBD index # 267\n",
      "done for ticker AWK index # 268\n",
      "done for ticker IT index # 269\n",
      "done for ticker GPN index # 270\n",
      "done for ticker HRL index # 271\n",
      "done for ticker FANG index # 272\n",
      "done for ticker WTW index # 273\n",
      "done for ticker RJF index # 274\n",
      "done for ticker GPC index # 275\n",
      "done for ticker IFF index # 276\n",
      "done for ticker CDW index # 277\n",
      "done for ticker TSCO index # 278\n",
      "done for ticker FITB index # 279\n",
      "done for ticker ARE index # 280\n",
      "done for ticker URI index # 281\n",
      "done for ticker ZBH index # 282\n",
      "done for ticker K index # 283\n",
      "done for ticker LEN index # 284\n",
      "done for ticker EBAY index # 285\n",
      "done for ticker EIX index # 286\n",
      "done for ticker CBRE index # 287\n",
      "done for ticker EFX index # 288\n",
      "done for ticker VMC index # 289\n",
      "done for ticker TSN index # 290\n",
      "done for ticker HIG index # 291\n",
      "done for ticker FTV index # 292\n",
      "done for ticker WY index # 293\n",
      "done for ticker EQR index # 294\n",
      "done for ticker AVB index # 295\n",
      "done for ticker MKC index # 296\n",
      "done for ticker ETR index # 297\n",
      "done for ticker LUV index # 298\n",
      "done for ticker ULTA index # 299\n",
      "done for ticker AEE index # 300\n",
      "done for ticker MLM index # 301\n",
      "done for ticker FE index # 302\n",
      "done for ticker PFG index # 303\n",
      "done for ticker FRC index # 304\n",
      "done for ticker DTE index # 305\n",
      "done for ticker DAL index # 306\n",
      "done for ticker HBAN index # 307\n",
      "done for ticker IR index # 308\n",
      "done for ticker CTRA index # 309\n",
      "done for ticker ANSS index # 310\n",
      "done for ticker ACGL index # 311\n",
      "done for ticker PPL index # 312\n",
      "done for ticker RF index # 313\n",
      "done for ticker VRSN index # 314\n",
      "done for ticker LH index # 315\n",
      "done for ticker EXR index # 316\n",
      "done for ticker PWR index # 317\n",
      "done for ticker CF index # 318\n",
      "done for ticker CAH index # 319\n",
      "done for ticker CFG index # 320\n",
      "done for ticker XYL index # 321\n",
      "done for ticker HPE index # 322\n",
      "done for ticker EPAM index # 323\n",
      "done for ticker DOV index # 324\n",
      "done for ticker WAT index # 325\n",
      "done for ticker WRB index # 326\n",
      "done for ticker TDY index # 327\n",
      "done for ticker PAYC index # 328\n",
      "done for ticker ROL index # 329\n",
      "done for ticker NTRS index # 330\n",
      "done for ticker MRO index # 331\n",
      "done for ticker CNP index # 332\n",
      "done for ticker INVH index # 333\n",
      "done for ticker CHD index # 334\n",
      "done for ticker AES index # 335\n",
      "done for ticker MOH index # 336\n",
      "done for ticker JBHT index # 337\n",
      "done for ticker MAA index # 338\n",
      "done for ticker BBY index # 339\n",
      "done for ticker CLX index # 340\n",
      "done for ticker HOLX index # 341\n",
      "done for ticker WAB index # 342\n",
      "done for ticker DRI index # 343\n",
      "done for ticker EXPD index # 344\n",
      "done for ticker STE index # 345\n",
      "done for ticker AMCR index # 346\n",
      "done for ticker VTR index # 347\n",
      "done for ticker IEX index # 348\n",
      "done for ticker CAG index # 349\n",
      "done for ticker CMS index # 350\n",
      "done for ticker KEY index # 351\n",
      "done for ticker MPWR index # 352\n",
      "done for ticker BALL index # 353\n",
      "done for ticker J index # 354\n",
      "done for ticker BR index # 355\n",
      "done for ticker GRMN index # 356\n",
      "done for ticker PKI index # 357\n",
      "done for ticker TTWO index # 358\n",
      "done for ticker INCY index # 359\n",
      "done for ticker FDS index # 360\n",
      "done for ticker MOS index # 361\n",
      "done for ticker SEDG index # 362\n",
      "done for ticker CINF index # 363\n",
      "done for ticker ABMD index # 364\n",
      "done for ticker DGX index # 365\n",
      "done for ticker WST index # 366\n",
      "done for ticker ATO index # 367\n",
      "done for ticker TRGP index # 368\n",
      "done for ticker BRO index # 369\n",
      "done for ticker SYF index # 370\n",
      "done for ticker FOX index # 371\n",
      "done for ticker FOXA index # 372\n",
      "done for ticker NTAP index # 373\n",
      "done for ticker FMC index # 374\n",
      "done for ticker EQT index # 375\n",
      "done for ticker OMC index # 376\n",
      "done for ticker SJM index # 377\n",
      "done for ticker LYV index # 378\n",
      "done for ticker CPB index # 379\n",
      "done for ticker HWM index # 380\n",
      "done for ticker CPRT index # 381\n",
      "done for ticker AVY index # 382\n",
      "done for ticker IRM index # 383\n",
      "done for ticker COO index # 384\n",
      "done for ticker ALGN index # 385\n",
      "done for ticker SWKS index # 386\n",
      "done for ticker EXPE index # 387\n",
      "done for ticker RCL index # 388\n",
      "done for ticker ETSY index # 389\n",
      "done for ticker APA index # 390\n",
      "done for ticker GEN index # 391\n",
      "done for ticker TXT index # 392\n",
      "done for ticker LDOS index # 393\n",
      "done for ticker LKQ index # 394\n",
      "done for ticker TER index # 395\n",
      "done for ticker PTC index # 396\n",
      "done for ticker TRMB index # 397\n",
      "done for ticker AKAM index # 398\n",
      "done for ticker NVR index # 399\n",
      "done for ticker UAL index # 400\n",
      "done for ticker LNT index # 401\n",
      "done for ticker FLT index # 402\n",
      "done for ticker KIM index # 403\n",
      "done for ticker ZBRA index # 404\n",
      "done for ticker TYL index # 405\n",
      "done for ticker DPZ index # 406\n",
      "done for ticker JKHY index # 407\n",
      "done for ticker MGM index # 408\n",
      "done for ticker ESS index # 409\n",
      "done for ticker L index # 410\n",
      "done for ticker PEAK index # 411\n",
      "done for ticker MTCH index # 412\n",
      "done for ticker NDSN index # 413\n",
      "done for ticker EVRG index # 414\n",
      "done for ticker IPG index # 415\n",
      "done for ticker BEN index # 416\n",
      "done for ticker CBOE index # 417\n",
      "done for ticker TECH index # 418\n",
      "done for ticker SIVB index # 419\n",
      "done for ticker VFC index # 420\n",
      "done for ticker IP index # 421\n",
      "done for ticker HST index # 422\n",
      "done for ticker UDR index # 423\n",
      "done for ticker POOL index # 424\n",
      "done for ticker RE index # 425\n",
      "done for ticker PARA index # 426\n",
      "done for ticker SNA index # 427\n",
      "done for ticker CPT index # 428\n",
      "done for ticker LW index # 429\n",
      "done for ticker PKG index # 430\n",
      "done for ticker CRL index # 431\n",
      "done for ticker SWK index # 432\n",
      "done for ticker BIO index # 433\n",
      "done for ticker WDC index # 434\n",
      "done for ticker CHRW index # 435\n",
      "done for ticker STX index # 436\n",
      "done for ticker MAS index # 437\n",
      "done for ticker GL index # 438\n",
      "done for ticker CE index # 439\n",
      "done for ticker REG index # 440\n",
      "done for ticker NI index # 441\n",
      "done for ticker BXP index # 442\n",
      "done for ticker HSIC index # 443\n",
      "done for ticker CCL index # 444\n",
      "done for ticker TFX index # 445\n",
      "done for ticker CZR index # 446\n",
      "done for ticker NWS index # 447\n",
      "done for ticker NWSA index # 448\n",
      "done for ticker KMX index # 449\n",
      "done for ticker EMN index # 450\n",
      "done for ticker JNPR index # 451\n",
      "done for ticker PHM index # 452\n",
      "done for ticker CDAY index # 453\n",
      "done for ticker ALLE index # 454\n",
      "done for ticker QRVO index # 455\n",
      "done for ticker BWA index # 456\n",
      "done for ticker NRG index # 457\n",
      "done for ticker MKTX index # 458\n",
      "done for ticker WRK index # 459\n",
      "done for ticker UHS index # 460\n",
      "done for ticker FFIV index # 461\n",
      "done for ticker AOS index # 462\n",
      "done for ticker CMA index # 463\n",
      "done for ticker AAL index # 464\n",
      "done for ticker BBWI index # 465\n",
      "done for ticker HII index # 466\n",
      "done for ticker AAP index # 467\n",
      "done for ticker TPR index # 468\n",
      "done for ticker FRT index # 469\n",
      "done for ticker IVZ index # 470\n",
      "done for ticker PNW index # 471\n",
      "done for ticker HAS index # 472\n",
      "done for ticker WYNN index # 473\n",
      "done for ticker FBHS index # 474\n",
      "done for ticker SBNY index # 475\n",
      "done for ticker DISH index # 476\n",
      "done for ticker RHI index # 477\n",
      "done for ticker WHR index # 478\n",
      "done for ticker ZION index # 479\n",
      "done for ticker CTLT index # 480\n",
      "done for ticker PNR index # 481\n",
      "done for ticker SEE index # 482\n",
      "done for ticker RL index # 483\n",
      "done for ticker NCLH index # 484\n",
      "done for ticker DXC index # 485\n",
      "done for ticker GNRC index # 486\n",
      "done for ticker AIZ index # 487\n",
      "done for ticker XRAY index # 488\n",
      "done for ticker LNC index # 489\n",
      "done for ticker DVA index # 490\n",
      "done for ticker MHK index # 491\n",
      "done for ticker LUMN index # 492\n",
      "done for ticker ALK index # 493\n",
      "done for ticker NWL index # 494\n",
      "done for ticker VNO index # 495\n",
      "done for ticker TAP index # 496\n"
     ]
    }
   ],
   "source": [
    "dict_score = {}\n",
    "tickers = utils.return_tickers()\n",
    "for ticker in tickers:\n",
    "        dict_score[ticker] = cross_validate_ml(prep.ready_to_train_df(ticker), XGBRegressor())\n",
    "        print(f\"done for ticker {ticker} index # {tickers.index(ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ae10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486892dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566fdfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9246528c",
   "metadata": {},
   "source": [
    "### Cross val scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abfbf79c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:31:13.692301Z",
     "start_time": "2022-12-06T10:31:13.665214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004796076311898096 0.02096529727516623\n"
     ]
    }
   ],
   "source": [
    "rmse= []\n",
    "baseline = []\n",
    "for key in dict_score.keys():\n",
    "    rmse.append(dict_score[key][0])\n",
    "    baseline.append(dict_score[key][1])\n",
    "print(np.mean(rmse), np.mean(baseline) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51edde51",
   "metadata": {},
   "source": [
    "## Custom grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e3eb5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-05T15:25:40.338767Z",
     "start_time": "2022-12-05T15:25:40.312234Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depth = [2, 5, 8]\n",
    "criterion = ['friedman_mse', 'squared_error', 'mse']\n",
    "learning_rate=[0.01, 0.1, 0.2] \n",
    "n_estimators=[100, 200, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb51d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31721416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:19:16.229421Z",
     "start_time": "2022-12-06T09:19:16.199027Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_gridsearch(df, model, max_depth=[2,3,4], criterion = ['friedman_mse', 'squared_error', 'mse'], n_estimator=[50, 75, 100], learning_rate=[0.08, 0.1, 0.12], loss=['squared_error', 'absolute_error', 'huber']):\n",
    "    counter = 0\n",
    "    rmse = []\n",
    "    baseline = []\n",
    "    params = []\n",
    "    for max_depth_i in max_depth:\n",
    "        for criterion_i in criterion:\n",
    "            for n_estimator_i in n_estimator:\n",
    "                for learning_rate_i in learning_rate:\n",
    "                    for loss_i in loss:\n",
    "                        test = cross_validate_ml(df = df, model = model(max_depth=max_depth_i,\n",
    "                                                                   criterion = criterion_i,\n",
    "                                                                   n_estimators = n_estimator_i,\n",
    "                                                                   learning_rate = learning_rate_i,\n",
    "                                                                   loss = loss_i))\n",
    "                        rmse.append(test[0])\n",
    "                        baseline.append(test[1])\n",
    "                        params.append((max_depth_i, criterion_i, n_estimator_i, learning_rate_i))\n",
    "                        counter += 1\n",
    "                        print(f'model {counter} done with parameters: max_depth = {max_depth_i}, criterion = {criterion_i}, estimators = {n_estimator_i}, learning rate = {learning_rate_i}, loss = {loss_i}, rmse = {test[0]}')\n",
    "    idx_min = np.argmin(rmse)\n",
    "    best_params = params[idx_min]\n",
    "    \n",
    "    return best_params, rmse, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d689aa2",
   "metadata": {},
   "source": [
    "### Model results on one stock on a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0abc63ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:44:15.668227Z",
     "start_time": "2022-12-06T09:19:29.718526Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002958185553002535\n",
      "model 2 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.006407018407676127\n",
      "model 3 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.004428068812946768\n",
      "model 4 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.002776729479836416\n",
      "model 5 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.005372760873905239\n",
      "model 6 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.003432291380610513\n",
      "model 7 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.0026067464673304175\n",
      "model 8 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.0048061652599137505\n",
      "model 9 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030851908081413497\n",
      "model 10 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0026039764209378246\n",
      "model 11 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.00477931059886155\n",
      "model 12 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.003132758828900319\n",
      "model 13 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0027137581776954262\n",
      "model 14 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0042761812198638905\n",
      "model 15 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.002879525183277261\n",
      "model 16 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.002675563304797706\n",
      "model 17 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.004136672056708204\n",
      "model 18 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0028028874014932675\n",
      "model 19 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.00263901736188292\n",
      "model 20 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.004243777202286242\n",
      "model 21 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0028285475219040047\n",
      "model 22 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0026600684459853496\n",
      "model 23 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003929768715246046\n",
      "model 24 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.002738377789027572\n",
      "model 25 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.0026049910482777846\n",
      "model 26 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0038342459402143712\n",
      "model 27 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0027842983111944423\n",
      "model 28 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002946016742644336\n",
      "model 29 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.006400529273369599\n",
      "model 30 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.004428402988000523\n",
      "model 31 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.00274092287895698\n",
      "model 32 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.005369459505880697\n",
      "model 33 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0034536502342627138\n",
      "model 34 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.0026201916948474895\n",
      "model 35 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.00481556253048331\n",
      "model 36 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030731467770259437\n",
      "model 37 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0026640675291778127\n",
      "model 38 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004781983704751462\n",
      "model 39 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0031020685649265803\n",
      "model 40 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.002641066825947065\n",
      "model 41 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.004317992084797235\n",
      "model 42 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0028619443825098193\n",
      "model 43 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0025911692981691096\n",
      "model 44 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.004111814141492845\n",
      "model 45 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.002768306658624108\n",
      "model 46 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.0026757573258833324\n",
      "model 47 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.004186948590438916\n",
      "model 48 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0028185742191962665\n",
      "model 49 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.002659309412779239\n",
      "model 50 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003949072152121796\n",
      "model 51 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0027222261058379204\n",
      "model 52 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002705998569738018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 53 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0038499484792597865\n",
      "model 54 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.002795539577918935\n",
      "model 55 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.0029099191595345383\n",
      "model 56 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.006407991680854807\n",
      "model 57 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.004428402988000523\n",
      "model 58 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0028373111145722184\n",
      "model 59 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.005372474799411161\n",
      "model 60 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0034288454838289927\n",
      "model 61 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.0025617175515441487\n",
      "model 62 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.0048133561585675414\n",
      "model 63 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030763780454744144\n",
      "model 64 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0026350705093279\n",
      "model 65 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004773342022038409\n",
      "model 66 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0031281830813940513\n",
      "model 67 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0026786321052186756\n",
      "model 68 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.004333966762045556\n",
      "model 69 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.002845647774894737\n",
      "model 70 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0026488261570874246\n",
      "model 71 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0040720219215136045\n",
      "model 72 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0027838512151926002\n",
      "model 73 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.0026434954217452755\n",
      "model 74 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.004142942835776003\n",
      "model 75 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.002837433961437816\n",
      "model 76 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0026015436713270864\n",
      "model 77 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003915021453609067\n",
      "model 78 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.002733796570629045\n",
      "model 79 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002590710250606715\n",
      "model 80 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.003836455439472309\n",
      "model 81 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0028310156316547074\n",
      "model 82 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002705103403928138\n",
      "model 83 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.005055674970022903\n",
      "model 84 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.003942692273722824\n",
      "model 85 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0025979372722968975\n",
      "model 86 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.004457668793358413\n",
      "model 87 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0032871980973670925\n",
      "model 88 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002639942667455135\n",
      "model 89 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.004204336452061385\n",
      "model 90 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030337239385201673\n",
      "model 91 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0025862176325708685\n",
      "model 92 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.00413893352920339\n",
      "model 93 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0030566455793911897\n",
      "model 94 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0025916442432565383\n",
      "model 95 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.00392530690915287\n",
      "model 96 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.002810890632321019\n",
      "model 97 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0026127529836289937\n",
      "model 98 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0038031379441063966\n",
      "model 99 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.002786888933712554\n",
      "model 100 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.002557329832721479\n",
      "model 101 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.003849198727467321\n",
      "model 102 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0028084278300910463\n",
      "model 103 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0025990298794970937\n",
      "model 104 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.0037592214174426305\n",
      "model 105 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0026895703301677075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 106 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.0025730424155033494\n",
      "model 107 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0036681061284255722\n",
      "model 108 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0027251777091245326\n",
      "model 109 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002702793151349547\n",
      "model 110 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.005058232367804183\n",
      "model 111 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.003941806328485692\n",
      "model 112 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.002588842026698095\n",
      "model 113 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.004427805540319038\n",
      "model 114 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.003268000952219227\n",
      "model 115 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002579248928301906\n",
      "model 116 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.004172891100709107\n",
      "model 117 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030140686492261992\n",
      "model 118 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.002613291387439508\n",
      "model 119 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004139953071470512\n",
      "model 120 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.003080924745809816\n",
      "model 121 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0026145555792715857\n",
      "model 122 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.003921975417612286\n",
      "model 123 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0027913629970269323\n",
      "model 124 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.002587307414146831\n",
      "model 125 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.003799259214174869\n",
      "model 126 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0027115296373352686\n",
      "model 127 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.002601336330933987\n",
      "model 128 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.0038066571748408824\n",
      "model 129 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.002808894661718752\n",
      "model 130 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0026127988262543223\n",
      "model 131 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.0037446865076462914\n",
      "model 132 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.002685108521693579\n",
      "model 133 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002634867846287722\n",
      "model 134 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.003733008379825462\n",
      "model 135 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0026192326431160044\n",
      "model 136 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002724326707987458\n",
      "model 137 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.005044860223031934\n",
      "model 138 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.0039340807041176136\n",
      "model 139 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0026069569351037753\n",
      "model 140 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.0044098313370142415\n",
      "model 141 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.003304580254359149\n",
      "model 142 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002652285999047112\n",
      "model 143 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.00422005058828855\n",
      "model 144 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.002941688447540913\n",
      "model 145 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.002650852313756527\n",
      "model 146 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.0041723361975041715\n",
      "model 147 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0030481177907805516\n",
      "model 148 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.002647907064336234\n",
      "model 149 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0039274727502588164\n",
      "model 150 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.002783644662704949\n",
      "model 151 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0026850857188837887\n",
      "model 152 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0038454450583371423\n",
      "model 153 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0027657041578346194\n",
      "model 154 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.0025795467664902786\n",
      "model 155 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.003925606315613257\n",
      "model 156 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0028148348736183842\n",
      "model 157 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0026357231847853823\n",
      "model 158 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003745041132907287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 159 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.002699105329518561\n",
      "model 160 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002601949405101594\n",
      "model 161 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0036977284985453603\n",
      "model 162 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0027447041132625722\n",
      "model 163 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002758204626328357\n",
      "model 164 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.004962315891742368\n",
      "model 165 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.003755602715526272\n",
      "model 166 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0028201793905030946\n",
      "model 167 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.004377564048505937\n",
      "model 168 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.003315656914245619\n",
      "model 169 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.0027571244856977664\n",
      "model 170 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.004088070021363021\n",
      "model 171 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.003059518646427285\n",
      "model 172 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0027493575860221735\n",
      "model 173 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004037023468753866\n",
      "model 174 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.003122135588575402\n",
      "model 175 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0027805589220566613\n",
      "model 176 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0038261411672781322\n",
      "model 177 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0028689319120608105\n",
      "model 178 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0027474853227340396\n",
      "model 179 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0037876683726842096\n",
      "model 180 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0028343406482770153\n",
      "model 181 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.0027827520136940245\n",
      "model 182 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.0038275716196665426\n",
      "model 183 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0030061031913663073\n",
      "model 184 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0028641024564482363\n",
      "model 185 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003675794027779795\n",
      "model 186 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0028617470584144803\n",
      "model 187 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.0027819085919339732\n",
      "model 188 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0037322326772521604\n",
      "model 189 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.002859199248754659\n",
      "model 190 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.0028216246715976863\n",
      "model 191 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.0049191235829356104\n",
      "model 192 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.0037387215313132395\n",
      "model 193 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.002843667545519211\n",
      "model 194 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.00439015237329631\n",
      "model 195 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0033074000213341066\n",
      "model 196 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002730438251678527\n",
      "model 197 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.0040828874531489215\n",
      "model 198 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030158147085443373\n",
      "model 199 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0027748639528769456\n",
      "model 200 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004081739919150697\n",
      "model 201 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0031098377803180006\n",
      "model 202 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0028109126274234626\n",
      "model 203 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0038678451510520445\n",
      "model 204 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0029069862124411865\n",
      "model 205 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0028351433629591177\n",
      "model 206 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.003825465067210109\n",
      "model 207 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0028548669489826795\n",
      "model 208 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.002779061332540999\n",
      "model 209 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.0037924076379733714\n",
      "model 210 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0029269825240330802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 211 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.002822949349514258\n",
      "model 212 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003671004260923169\n",
      "model 213 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0027898015141627564\n",
      "model 214 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002789152927054379\n",
      "model 215 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.003691289937380604\n",
      "model 216 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0028131115634149615\n",
      "model 217 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002799139551941181\n",
      "model 218 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.004973818289138408\n",
      "model 219 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.003748542432319123\n",
      "model 220 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0027927302306963797\n",
      "model 221 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.004335859131408847\n",
      "model 222 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0033200876306249266\n",
      "model 223 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002807844523165585\n",
      "model 224 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.004042848200822164\n",
      "model 225 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030667195280845664\n",
      "model 226 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0027740729107799005\n",
      "model 227 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004034942483539463\n",
      "model 228 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.003147541038843053\n",
      "model 229 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.002807550771254751\n",
      "model 230 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0038697693960322646\n",
      "model 231 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0028860769585803145\n",
      "model 232 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.00277620080412421\n",
      "model 233 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0038790739000200787\n",
      "model 234 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.002832208965267212\n",
      "model 235 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.002767648470364058\n",
      "model 236 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.003728316443935388\n",
      "model 237 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0029278226365026107\n",
      "model 238 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.002827328916762123\n",
      "model 239 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.0037037792842529284\n",
      "model 240 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0028500206611104984\n",
      "model 241 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.0027272080473247054\n",
      "model 242 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.00378886572935585\n",
      "model 243 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0027762487865231975\n"
     ]
    }
   ],
   "source": [
    "gs_IP = custom_gridsearch(prep.ready_to_train_df('IP'), model = GradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e30b574b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:34:38.067317Z",
     "start_time": "2022-12-06T10:34:38.029878Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 'friedman_mse', 100, 0.08),\n",
       " [0.002958185553002535,\n",
       "  0.006407018407676127,\n",
       "  0.004428068812946768,\n",
       "  0.002776729479836416,\n",
       "  0.005372760873905239,\n",
       "  0.003432291380610513,\n",
       "  0.0026067464673304175,\n",
       "  0.0048061652599137505,\n",
       "  0.0030851908081413497,\n",
       "  0.0026039764209378246,\n",
       "  0.00477931059886155,\n",
       "  0.003132758828900319,\n",
       "  0.0027137581776954262,\n",
       "  0.0042761812198638905,\n",
       "  0.002879525183277261,\n",
       "  0.002675563304797706,\n",
       "  0.004136672056708204,\n",
       "  0.0028028874014932675,\n",
       "  0.00263901736188292,\n",
       "  0.004243777202286242,\n",
       "  0.0028285475219040047,\n",
       "  0.0026600684459853496,\n",
       "  0.003929768715246046,\n",
       "  0.002738377789027572,\n",
       "  0.0026049910482777846,\n",
       "  0.0038342459402143712,\n",
       "  0.0027842983111944423,\n",
       "  0.002946016742644336,\n",
       "  0.006400529273369599,\n",
       "  0.004428402988000523,\n",
       "  0.00274092287895698,\n",
       "  0.005369459505880697,\n",
       "  0.0034536502342627138,\n",
       "  0.0026201916948474895,\n",
       "  0.00481556253048331,\n",
       "  0.0030731467770259437,\n",
       "  0.0026640675291778127,\n",
       "  0.004781983704751462,\n",
       "  0.0031020685649265803,\n",
       "  0.002641066825947065,\n",
       "  0.004317992084797235,\n",
       "  0.0028619443825098193,\n",
       "  0.0025911692981691096,\n",
       "  0.004111814141492845,\n",
       "  0.002768306658624108,\n",
       "  0.0026757573258833324,\n",
       "  0.004186948590438916,\n",
       "  0.0028185742191962665,\n",
       "  0.002659309412779239,\n",
       "  0.003949072152121796,\n",
       "  0.0027222261058379204,\n",
       "  0.002705998569738018,\n",
       "  0.0038499484792597865,\n",
       "  0.002795539577918935,\n",
       "  0.0029099191595345383,\n",
       "  0.006407991680854807,\n",
       "  0.004428402988000523,\n",
       "  0.0028373111145722184,\n",
       "  0.005372474799411161,\n",
       "  0.0034288454838289927,\n",
       "  0.0025617175515441487,\n",
       "  0.0048133561585675414,\n",
       "  0.0030763780454744144,\n",
       "  0.0026350705093279,\n",
       "  0.004773342022038409,\n",
       "  0.0031281830813940513,\n",
       "  0.0026786321052186756,\n",
       "  0.004333966762045556,\n",
       "  0.002845647774894737,\n",
       "  0.0026488261570874246,\n",
       "  0.0040720219215136045,\n",
       "  0.0027838512151926002,\n",
       "  0.0026434954217452755,\n",
       "  0.004142942835776003,\n",
       "  0.002837433961437816,\n",
       "  0.0026015436713270864,\n",
       "  0.003915021453609067,\n",
       "  0.002733796570629045,\n",
       "  0.002590710250606715,\n",
       "  0.003836455439472309,\n",
       "  0.0028310156316547074,\n",
       "  0.002705103403928138,\n",
       "  0.005055674970022903,\n",
       "  0.003942692273722824,\n",
       "  0.0025979372722968975,\n",
       "  0.004457668793358413,\n",
       "  0.0032871980973670925,\n",
       "  0.002639942667455135,\n",
       "  0.004204336452061385,\n",
       "  0.0030337239385201673,\n",
       "  0.0025862176325708685,\n",
       "  0.00413893352920339,\n",
       "  0.0030566455793911897,\n",
       "  0.0025916442432565383,\n",
       "  0.00392530690915287,\n",
       "  0.002810890632321019,\n",
       "  0.0026127529836289937,\n",
       "  0.0038031379441063966,\n",
       "  0.002786888933712554,\n",
       "  0.002557329832721479,\n",
       "  0.003849198727467321,\n",
       "  0.0028084278300910463,\n",
       "  0.0025990298794970937,\n",
       "  0.0037592214174426305,\n",
       "  0.0026895703301677075,\n",
       "  0.0025730424155033494,\n",
       "  0.0036681061284255722,\n",
       "  0.0027251777091245326,\n",
       "  0.002702793151349547,\n",
       "  0.005058232367804183,\n",
       "  0.003941806328485692,\n",
       "  0.002588842026698095,\n",
       "  0.004427805540319038,\n",
       "  0.003268000952219227,\n",
       "  0.002579248928301906,\n",
       "  0.004172891100709107,\n",
       "  0.0030140686492261992,\n",
       "  0.002613291387439508,\n",
       "  0.004139953071470512,\n",
       "  0.003080924745809816,\n",
       "  0.0026145555792715857,\n",
       "  0.003921975417612286,\n",
       "  0.0027913629970269323,\n",
       "  0.002587307414146831,\n",
       "  0.003799259214174869,\n",
       "  0.0027115296373352686,\n",
       "  0.002601336330933987,\n",
       "  0.0038066571748408824,\n",
       "  0.002808894661718752,\n",
       "  0.0026127988262543223,\n",
       "  0.0037446865076462914,\n",
       "  0.002685108521693579,\n",
       "  0.002634867846287722,\n",
       "  0.003733008379825462,\n",
       "  0.0026192326431160044,\n",
       "  0.002724326707987458,\n",
       "  0.005044860223031934,\n",
       "  0.0039340807041176136,\n",
       "  0.0026069569351037753,\n",
       "  0.0044098313370142415,\n",
       "  0.003304580254359149,\n",
       "  0.002652285999047112,\n",
       "  0.00422005058828855,\n",
       "  0.002941688447540913,\n",
       "  0.002650852313756527,\n",
       "  0.0041723361975041715,\n",
       "  0.0030481177907805516,\n",
       "  0.002647907064336234,\n",
       "  0.0039274727502588164,\n",
       "  0.002783644662704949,\n",
       "  0.0026850857188837887,\n",
       "  0.0038454450583371423,\n",
       "  0.0027657041578346194,\n",
       "  0.0025795467664902786,\n",
       "  0.003925606315613257,\n",
       "  0.0028148348736183842,\n",
       "  0.0026357231847853823,\n",
       "  0.003745041132907287,\n",
       "  0.002699105329518561,\n",
       "  0.002601949405101594,\n",
       "  0.0036977284985453603,\n",
       "  0.0027447041132625722,\n",
       "  0.002758204626328357,\n",
       "  0.004962315891742368,\n",
       "  0.003755602715526272,\n",
       "  0.0028201793905030946,\n",
       "  0.004377564048505937,\n",
       "  0.003315656914245619,\n",
       "  0.0027571244856977664,\n",
       "  0.004088070021363021,\n",
       "  0.003059518646427285,\n",
       "  0.0027493575860221735,\n",
       "  0.004037023468753866,\n",
       "  0.003122135588575402,\n",
       "  0.0027805589220566613,\n",
       "  0.0038261411672781322,\n",
       "  0.0028689319120608105,\n",
       "  0.0027474853227340396,\n",
       "  0.0037876683726842096,\n",
       "  0.0028343406482770153,\n",
       "  0.0027827520136940245,\n",
       "  0.0038275716196665426,\n",
       "  0.0030061031913663073,\n",
       "  0.0028641024564482363,\n",
       "  0.003675794027779795,\n",
       "  0.0028617470584144803,\n",
       "  0.0027819085919339732,\n",
       "  0.0037322326772521604,\n",
       "  0.002859199248754659,\n",
       "  0.0028216246715976863,\n",
       "  0.0049191235829356104,\n",
       "  0.0037387215313132395,\n",
       "  0.002843667545519211,\n",
       "  0.00439015237329631,\n",
       "  0.0033074000213341066,\n",
       "  0.002730438251678527,\n",
       "  0.0040828874531489215,\n",
       "  0.0030158147085443373,\n",
       "  0.0027748639528769456,\n",
       "  0.004081739919150697,\n",
       "  0.0031098377803180006,\n",
       "  0.0028109126274234626,\n",
       "  0.0038678451510520445,\n",
       "  0.0029069862124411865,\n",
       "  0.0028351433629591177,\n",
       "  0.003825465067210109,\n",
       "  0.0028548669489826795,\n",
       "  0.002779061332540999,\n",
       "  0.0037924076379733714,\n",
       "  0.0029269825240330802,\n",
       "  0.002822949349514258,\n",
       "  0.003671004260923169,\n",
       "  0.0027898015141627564,\n",
       "  0.002789152927054379,\n",
       "  0.003691289937380604,\n",
       "  0.0028131115634149615,\n",
       "  0.002799139551941181,\n",
       "  0.004973818289138408,\n",
       "  0.003748542432319123,\n",
       "  0.0027927302306963797,\n",
       "  0.004335859131408847,\n",
       "  0.0033200876306249266,\n",
       "  0.002807844523165585,\n",
       "  0.004042848200822164,\n",
       "  0.0030667195280845664,\n",
       "  0.0027740729107799005,\n",
       "  0.004034942483539463,\n",
       "  0.003147541038843053,\n",
       "  0.002807550771254751,\n",
       "  0.0038697693960322646,\n",
       "  0.0028860769585803145,\n",
       "  0.00277620080412421,\n",
       "  0.0038790739000200787,\n",
       "  0.002832208965267212,\n",
       "  0.002767648470364058,\n",
       "  0.003728316443935388,\n",
       "  0.0029278226365026107,\n",
       "  0.002827328916762123,\n",
       "  0.0037037792842529284,\n",
       "  0.0028500206611104984,\n",
       "  0.0027272080473247054,\n",
       "  0.00378886572935585,\n",
       "  0.0027762487865231975],\n",
       " [(2, 'friedman_mse', 50, 0.08),\n",
       "  (2, 'friedman_mse', 50, 0.08),\n",
       "  (2, 'friedman_mse', 50, 0.08),\n",
       "  (2, 'friedman_mse', 50, 0.1),\n",
       "  (2, 'friedman_mse', 50, 0.1),\n",
       "  (2, 'friedman_mse', 50, 0.1),\n",
       "  (2, 'friedman_mse', 50, 0.12),\n",
       "  (2, 'friedman_mse', 50, 0.12),\n",
       "  (2, 'friedman_mse', 50, 0.12),\n",
       "  (2, 'friedman_mse', 75, 0.08),\n",
       "  (2, 'friedman_mse', 75, 0.08),\n",
       "  (2, 'friedman_mse', 75, 0.08),\n",
       "  (2, 'friedman_mse', 75, 0.1),\n",
       "  (2, 'friedman_mse', 75, 0.1),\n",
       "  (2, 'friedman_mse', 75, 0.1),\n",
       "  (2, 'friedman_mse', 75, 0.12),\n",
       "  (2, 'friedman_mse', 75, 0.12),\n",
       "  (2, 'friedman_mse', 75, 0.12),\n",
       "  (2, 'friedman_mse', 100, 0.08),\n",
       "  (2, 'friedman_mse', 100, 0.08),\n",
       "  (2, 'friedman_mse', 100, 0.08),\n",
       "  (2, 'friedman_mse', 100, 0.1),\n",
       "  (2, 'friedman_mse', 100, 0.1),\n",
       "  (2, 'friedman_mse', 100, 0.1),\n",
       "  (2, 'friedman_mse', 100, 0.12),\n",
       "  (2, 'friedman_mse', 100, 0.12),\n",
       "  (2, 'friedman_mse', 100, 0.12),\n",
       "  (2, 'squared_error', 50, 0.08),\n",
       "  (2, 'squared_error', 50, 0.08),\n",
       "  (2, 'squared_error', 50, 0.08),\n",
       "  (2, 'squared_error', 50, 0.1),\n",
       "  (2, 'squared_error', 50, 0.1),\n",
       "  (2, 'squared_error', 50, 0.1),\n",
       "  (2, 'squared_error', 50, 0.12),\n",
       "  (2, 'squared_error', 50, 0.12),\n",
       "  (2, 'squared_error', 50, 0.12),\n",
       "  (2, 'squared_error', 75, 0.08),\n",
       "  (2, 'squared_error', 75, 0.08),\n",
       "  (2, 'squared_error', 75, 0.08),\n",
       "  (2, 'squared_error', 75, 0.1),\n",
       "  (2, 'squared_error', 75, 0.1),\n",
       "  (2, 'squared_error', 75, 0.1),\n",
       "  (2, 'squared_error', 75, 0.12),\n",
       "  (2, 'squared_error', 75, 0.12),\n",
       "  (2, 'squared_error', 75, 0.12),\n",
       "  (2, 'squared_error', 100, 0.08),\n",
       "  (2, 'squared_error', 100, 0.08),\n",
       "  (2, 'squared_error', 100, 0.08),\n",
       "  (2, 'squared_error', 100, 0.1),\n",
       "  (2, 'squared_error', 100, 0.1),\n",
       "  (2, 'squared_error', 100, 0.1),\n",
       "  (2, 'squared_error', 100, 0.12),\n",
       "  (2, 'squared_error', 100, 0.12),\n",
       "  (2, 'squared_error', 100, 0.12),\n",
       "  (2, 'mse', 50, 0.08),\n",
       "  (2, 'mse', 50, 0.08),\n",
       "  (2, 'mse', 50, 0.08),\n",
       "  (2, 'mse', 50, 0.1),\n",
       "  (2, 'mse', 50, 0.1),\n",
       "  (2, 'mse', 50, 0.1),\n",
       "  (2, 'mse', 50, 0.12),\n",
       "  (2, 'mse', 50, 0.12),\n",
       "  (2, 'mse', 50, 0.12),\n",
       "  (2, 'mse', 75, 0.08),\n",
       "  (2, 'mse', 75, 0.08),\n",
       "  (2, 'mse', 75, 0.08),\n",
       "  (2, 'mse', 75, 0.1),\n",
       "  (2, 'mse', 75, 0.1),\n",
       "  (2, 'mse', 75, 0.1),\n",
       "  (2, 'mse', 75, 0.12),\n",
       "  (2, 'mse', 75, 0.12),\n",
       "  (2, 'mse', 75, 0.12),\n",
       "  (2, 'mse', 100, 0.08),\n",
       "  (2, 'mse', 100, 0.08),\n",
       "  (2, 'mse', 100, 0.08),\n",
       "  (2, 'mse', 100, 0.1),\n",
       "  (2, 'mse', 100, 0.1),\n",
       "  (2, 'mse', 100, 0.1),\n",
       "  (2, 'mse', 100, 0.12),\n",
       "  (2, 'mse', 100, 0.12),\n",
       "  (2, 'mse', 100, 0.12),\n",
       "  (3, 'friedman_mse', 50, 0.08),\n",
       "  (3, 'friedman_mse', 50, 0.08),\n",
       "  (3, 'friedman_mse', 50, 0.08),\n",
       "  (3, 'friedman_mse', 50, 0.1),\n",
       "  (3, 'friedman_mse', 50, 0.1),\n",
       "  (3, 'friedman_mse', 50, 0.1),\n",
       "  (3, 'friedman_mse', 50, 0.12),\n",
       "  (3, 'friedman_mse', 50, 0.12),\n",
       "  (3, 'friedman_mse', 50, 0.12),\n",
       "  (3, 'friedman_mse', 75, 0.08),\n",
       "  (3, 'friedman_mse', 75, 0.08),\n",
       "  (3, 'friedman_mse', 75, 0.08),\n",
       "  (3, 'friedman_mse', 75, 0.1),\n",
       "  (3, 'friedman_mse', 75, 0.1),\n",
       "  (3, 'friedman_mse', 75, 0.1),\n",
       "  (3, 'friedman_mse', 75, 0.12),\n",
       "  (3, 'friedman_mse', 75, 0.12),\n",
       "  (3, 'friedman_mse', 75, 0.12),\n",
       "  (3, 'friedman_mse', 100, 0.08),\n",
       "  (3, 'friedman_mse', 100, 0.08),\n",
       "  (3, 'friedman_mse', 100, 0.08),\n",
       "  (3, 'friedman_mse', 100, 0.1),\n",
       "  (3, 'friedman_mse', 100, 0.1),\n",
       "  (3, 'friedman_mse', 100, 0.1),\n",
       "  (3, 'friedman_mse', 100, 0.12),\n",
       "  (3, 'friedman_mse', 100, 0.12),\n",
       "  (3, 'friedman_mse', 100, 0.12),\n",
       "  (3, 'squared_error', 50, 0.08),\n",
       "  (3, 'squared_error', 50, 0.08),\n",
       "  (3, 'squared_error', 50, 0.08),\n",
       "  (3, 'squared_error', 50, 0.1),\n",
       "  (3, 'squared_error', 50, 0.1),\n",
       "  (3, 'squared_error', 50, 0.1),\n",
       "  (3, 'squared_error', 50, 0.12),\n",
       "  (3, 'squared_error', 50, 0.12),\n",
       "  (3, 'squared_error', 50, 0.12),\n",
       "  (3, 'squared_error', 75, 0.08),\n",
       "  (3, 'squared_error', 75, 0.08),\n",
       "  (3, 'squared_error', 75, 0.08),\n",
       "  (3, 'squared_error', 75, 0.1),\n",
       "  (3, 'squared_error', 75, 0.1),\n",
       "  (3, 'squared_error', 75, 0.1),\n",
       "  (3, 'squared_error', 75, 0.12),\n",
       "  (3, 'squared_error', 75, 0.12),\n",
       "  (3, 'squared_error', 75, 0.12),\n",
       "  (3, 'squared_error', 100, 0.08),\n",
       "  (3, 'squared_error', 100, 0.08),\n",
       "  (3, 'squared_error', 100, 0.08),\n",
       "  (3, 'squared_error', 100, 0.1),\n",
       "  (3, 'squared_error', 100, 0.1),\n",
       "  (3, 'squared_error', 100, 0.1),\n",
       "  (3, 'squared_error', 100, 0.12),\n",
       "  (3, 'squared_error', 100, 0.12),\n",
       "  (3, 'squared_error', 100, 0.12),\n",
       "  (3, 'mse', 50, 0.08),\n",
       "  (3, 'mse', 50, 0.08),\n",
       "  (3, 'mse', 50, 0.08),\n",
       "  (3, 'mse', 50, 0.1),\n",
       "  (3, 'mse', 50, 0.1),\n",
       "  (3, 'mse', 50, 0.1),\n",
       "  (3, 'mse', 50, 0.12),\n",
       "  (3, 'mse', 50, 0.12),\n",
       "  (3, 'mse', 50, 0.12),\n",
       "  (3, 'mse', 75, 0.08),\n",
       "  (3, 'mse', 75, 0.08),\n",
       "  (3, 'mse', 75, 0.08),\n",
       "  (3, 'mse', 75, 0.1),\n",
       "  (3, 'mse', 75, 0.1),\n",
       "  (3, 'mse', 75, 0.1),\n",
       "  (3, 'mse', 75, 0.12),\n",
       "  (3, 'mse', 75, 0.12),\n",
       "  (3, 'mse', 75, 0.12),\n",
       "  (3, 'mse', 100, 0.08),\n",
       "  (3, 'mse', 100, 0.08),\n",
       "  (3, 'mse', 100, 0.08),\n",
       "  (3, 'mse', 100, 0.1),\n",
       "  (3, 'mse', 100, 0.1),\n",
       "  (3, 'mse', 100, 0.1),\n",
       "  (3, 'mse', 100, 0.12),\n",
       "  (3, 'mse', 100, 0.12),\n",
       "  (3, 'mse', 100, 0.12),\n",
       "  (4, 'friedman_mse', 50, 0.08),\n",
       "  (4, 'friedman_mse', 50, 0.08),\n",
       "  (4, 'friedman_mse', 50, 0.08),\n",
       "  (4, 'friedman_mse', 50, 0.1),\n",
       "  (4, 'friedman_mse', 50, 0.1),\n",
       "  (4, 'friedman_mse', 50, 0.1),\n",
       "  (4, 'friedman_mse', 50, 0.12),\n",
       "  (4, 'friedman_mse', 50, 0.12),\n",
       "  (4, 'friedman_mse', 50, 0.12),\n",
       "  (4, 'friedman_mse', 75, 0.08),\n",
       "  (4, 'friedman_mse', 75, 0.08),\n",
       "  (4, 'friedman_mse', 75, 0.08),\n",
       "  (4, 'friedman_mse', 75, 0.1),\n",
       "  (4, 'friedman_mse', 75, 0.1),\n",
       "  (4, 'friedman_mse', 75, 0.1),\n",
       "  (4, 'friedman_mse', 75, 0.12),\n",
       "  (4, 'friedman_mse', 75, 0.12),\n",
       "  (4, 'friedman_mse', 75, 0.12),\n",
       "  (4, 'friedman_mse', 100, 0.08),\n",
       "  (4, 'friedman_mse', 100, 0.08),\n",
       "  (4, 'friedman_mse', 100, 0.08),\n",
       "  (4, 'friedman_mse', 100, 0.1),\n",
       "  (4, 'friedman_mse', 100, 0.1),\n",
       "  (4, 'friedman_mse', 100, 0.1),\n",
       "  (4, 'friedman_mse', 100, 0.12),\n",
       "  (4, 'friedman_mse', 100, 0.12),\n",
       "  (4, 'friedman_mse', 100, 0.12),\n",
       "  (4, 'squared_error', 50, 0.08),\n",
       "  (4, 'squared_error', 50, 0.08),\n",
       "  (4, 'squared_error', 50, 0.08),\n",
       "  (4, 'squared_error', 50, 0.1),\n",
       "  (4, 'squared_error', 50, 0.1),\n",
       "  (4, 'squared_error', 50, 0.1),\n",
       "  (4, 'squared_error', 50, 0.12),\n",
       "  (4, 'squared_error', 50, 0.12),\n",
       "  (4, 'squared_error', 50, 0.12),\n",
       "  (4, 'squared_error', 75, 0.08),\n",
       "  (4, 'squared_error', 75, 0.08),\n",
       "  (4, 'squared_error', 75, 0.08),\n",
       "  (4, 'squared_error', 75, 0.1),\n",
       "  (4, 'squared_error', 75, 0.1),\n",
       "  (4, 'squared_error', 75, 0.1),\n",
       "  (4, 'squared_error', 75, 0.12),\n",
       "  (4, 'squared_error', 75, 0.12),\n",
       "  (4, 'squared_error', 75, 0.12),\n",
       "  (4, 'squared_error', 100, 0.08),\n",
       "  (4, 'squared_error', 100, 0.08),\n",
       "  (4, 'squared_error', 100, 0.08),\n",
       "  (4, 'squared_error', 100, 0.1),\n",
       "  (4, 'squared_error', 100, 0.1),\n",
       "  (4, 'squared_error', 100, 0.1),\n",
       "  (4, 'squared_error', 100, 0.12),\n",
       "  (4, 'squared_error', 100, 0.12),\n",
       "  (4, 'squared_error', 100, 0.12),\n",
       "  (4, 'mse', 50, 0.08),\n",
       "  (4, 'mse', 50, 0.08),\n",
       "  (4, 'mse', 50, 0.08),\n",
       "  (4, 'mse', 50, 0.1),\n",
       "  (4, 'mse', 50, 0.1),\n",
       "  (4, 'mse', 50, 0.1),\n",
       "  (4, 'mse', 50, 0.12),\n",
       "  (4, 'mse', 50, 0.12),\n",
       "  (4, 'mse', 50, 0.12),\n",
       "  (4, 'mse', 75, 0.08),\n",
       "  (4, 'mse', 75, 0.08),\n",
       "  (4, 'mse', 75, 0.08),\n",
       "  (4, 'mse', 75, 0.1),\n",
       "  (4, 'mse', 75, 0.1),\n",
       "  (4, 'mse', 75, 0.1),\n",
       "  (4, 'mse', 75, 0.12),\n",
       "  (4, 'mse', 75, 0.12),\n",
       "  (4, 'mse', 75, 0.12),\n",
       "  (4, 'mse', 100, 0.08),\n",
       "  (4, 'mse', 100, 0.08),\n",
       "  (4, 'mse', 100, 0.08),\n",
       "  (4, 'mse', 100, 0.1),\n",
       "  (4, 'mse', 100, 0.1),\n",
       "  (4, 'mse', 100, 0.1),\n",
       "  (4, 'mse', 100, 0.12),\n",
       "  (4, 'mse', 100, 0.12),\n",
       "  (4, 'mse', 100, 0.12)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34910d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:46:22.145038Z",
     "start_time": "2022-12-06T09:46:22.116797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018969723709451568"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee46770",
   "metadata": {},
   "source": [
    "### Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f8da562",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:35:35.451616Z",
     "start_time": "2022-12-06T11:35:35.419318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=0.9)),\n",
       "                (&#x27;gradientboostingregressor&#x27;,\n",
       "                 GradientBoostingRegressor(max_depth=2))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=0.9)),\n",
       "                (&#x27;gradientboostingregressor&#x27;,\n",
       "                 GradientBoostingRegressor(max_depth=2))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=0.9)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=2)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('pca', PCA(n_components=0.9)),\n",
       "                ('gradientboostingregressor',\n",
       "                 GradientBoostingRegressor(max_depth=2))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(PCA(), GradientBoostingRegressor())\n",
    "\n",
    "params = {'pca__n_components':0.9,\n",
    "                'gradientboostingregressor__max_depth':2,\n",
    "                'gradientboostingregressor__criterion':'friedman_mse',\n",
    "                'gradientboostingregressor__n_estimators':100,\n",
    "                'gradientboostingregressor__learning_rate':0.1}\n",
    "pipe.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f62ee419",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:49:38.402435Z",
     "start_time": "2022-12-06T11:49:38.378925Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d71bc8f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:52:55.212467Z",
     "start_time": "2022-12-06T11:49:52.225946Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model AAPL index # 0 saved\n",
      "Model MSFT index # 1 saved\n",
      "Model GOOG index # 2 saved\n",
      "Model AMZN index # 3 saved\n",
      "Model TSLA index # 4 saved\n",
      "Model UNH index # 5 saved\n",
      "Model XOM index # 6 saved\n",
      "Model JNJ index # 7 saved\n",
      "Model WMT index # 8 saved\n",
      "Model NVDA index # 9 saved\n",
      "Model JPM index # 10 saved\n",
      "Model V index # 11 saved\n",
      "Model CVX index # 12 saved\n",
      "Model PG index # 13 saved\n",
      "Model LLY index # 14 saved\n",
      "Model MA index # 15 saved\n",
      "Model HD index # 16 saved\n",
      "Model META index # 17 saved\n",
      "Model BAC index # 18 saved\n",
      "Model ABBV index # 19 saved\n",
      "Model PFE index # 20 saved\n",
      "Model KO index # 21 saved\n",
      "Model MRK index # 22 saved\n",
      "Model PEP index # 23 saved\n",
      "Model COST index # 24 saved\n",
      "Model ORCL index # 25 saved\n",
      "Model AVGO index # 26 saved\n",
      "Model TMO index # 27 saved\n",
      "Model MCD index # 28 saved\n",
      "Model CSCO index # 29 saved\n",
      "Model ACN index # 30 saved\n",
      "Model DHR index # 31 saved\n",
      "Model TMUS index # 32 saved\n",
      "Model ABT index # 33 saved\n",
      "Model WFC index # 34 saved\n",
      "Model DIS index # 35 saved\n",
      "Model LIN index # 36 saved\n",
      "Model NEE index # 37 saved\n",
      "Model BMY index # 38 saved\n",
      "Model NKE index # 39 saved\n",
      "Model VZ index # 40 saved\n",
      "Model TXN index # 41 saved\n",
      "Model UPS index # 42 saved\n",
      "Model COP index # 43 saved\n",
      "Model ADBE index # 44 saved\n",
      "Model CMCSA index # 45 saved\n",
      "Model CRM index # 46 saved\n",
      "Model PM index # 47 saved\n",
      "Model MS index # 48 saved\n",
      "Model AMGN index # 49 saved\n",
      "Model SCHW index # 50 saved\n",
      "Model HON index # 51 saved\n",
      "Model RTX index # 52 saved\n",
      "Model QCOM index # 53 saved\n",
      "Model T index # 54 saved\n",
      "Model IBM index # 55 saved\n",
      "Model DE index # 56 saved\n",
      "Model CVS index # 57 saved\n",
      "Model LOW index # 58 saved\n",
      "Model GS index # 59 saved\n",
      "Model UNP index # 60 saved\n",
      "Model NFLX index # 61 saved\n",
      "Model LMT index # 62 saved\n",
      "Model CAT index # 63 saved\n",
      "Model AMD index # 64 saved\n",
      "Model INTC index # 65 saved\n",
      "Model ELV index # 66 saved\n",
      "Model SPGI index # 67 saved\n",
      "Model AXP index # 68 saved\n",
      "Model SBUX index # 69 saved\n",
      "Model INTU index # 70 saved\n",
      "Model BLK index # 71 saved\n",
      "Model ADP index # 72 saved\n",
      "Model GILD index # 73 saved\n",
      "Model PLD index # 74 saved\n",
      "Model MDT index # 75 saved\n",
      "Model BA index # 76 saved\n",
      "Model AMT index # 77 saved\n",
      "Model CI index # 78 saved\n",
      "Model GE index # 79 saved\n",
      "Model TJX index # 80 saved\n",
      "Model ISRG index # 81 saved\n",
      "Model C index # 82 saved\n",
      "Model AMAT index # 83 saved\n",
      "Model PYPL index # 84 saved\n",
      "Model MDLZ index # 85 saved\n",
      "Model CB index # 86 saved\n",
      "Model SYK index # 87 saved\n",
      "Model ADI index # 88 saved\n",
      "Model MMC index # 89 saved\n",
      "Model EOG index # 90 saved\n",
      "Model NOW index # 91 saved\n",
      "Model VRTX index # 92 saved\n",
      "Model MO index # 93 saved\n",
      "Model NOC index # 94 saved\n",
      "Model EL index # 95 saved\n",
      "Model REGN index # 96 saved\n",
      "Model PGR index # 97 saved\n",
      "Model BKNG index # 98 saved\n",
      "Model DUK index # 99 saved\n",
      "Model TGT index # 100 saved\n",
      "Model SLB index # 101 saved\n",
      "Model SO index # 102 saved\n",
      "Model MMM index # 103 saved\n",
      "Model ITW index # 104 saved\n",
      "Model ZTS index # 105 saved\n",
      "Model GD index # 106 saved\n",
      "Model APD index # 107 saved\n",
      "Model HUM index # 108 saved\n",
      "Model MRNA index # 109 saved\n",
      "Model BDX index # 110 saved\n",
      "Model CSX index # 111 saved\n",
      "Model WM index # 112 saved\n",
      "Model PNC index # 113 saved\n",
      "Model HCA index # 114 saved\n",
      "Model ETN index # 115 saved\n",
      "Model USB index # 116 saved\n",
      "Model FISV index # 117 saved\n",
      "Model SHW index # 118 saved\n",
      "Model OXY index # 119 saved\n",
      "Model CL index # 120 saved\n",
      "Model MU index # 121 saved\n",
      "Model CME index # 122 saved\n",
      "Model AON index # 123 saved\n",
      "Model LRCX index # 124 saved\n",
      "Model BSX index # 125 saved\n",
      "Model EQIX index # 126 saved\n",
      "Model TFC index # 127 saved\n",
      "Model PXD index # 128 saved\n",
      "Model CHTR index # 129 saved\n",
      "Model CCI index # 130 saved\n",
      "Model MET index # 131 saved\n",
      "Model ATVI index # 132 saved\n",
      "Model ICE index # 133 saved\n",
      "Model MPC index # 134 saved\n",
      "Model NSC index # 135 saved\n",
      "Model DG index # 136 saved\n",
      "Model GM index # 137 saved\n",
      "Model EMR index # 138 saved\n",
      "Model F index # 139 saved\n",
      "Model KLAC index # 140 saved\n",
      "Model MCO index # 141 saved\n",
      "Model FCX index # 142 saved\n",
      "Model KDP index # 143 saved\n",
      "Model MNST index # 144 saved\n",
      "Model MCK index # 145 saved\n",
      "Model VLO index # 146 saved\n",
      "Model ORLY index # 147 saved\n",
      "Model ADM index # 148 saved\n",
      "Model PSX index # 149 saved\n",
      "Model PSA index # 150 saved\n",
      "Model SRE index # 151 saved\n",
      "Model SNPS index # 152 saved\n",
      "Model MAR index # 153 saved\n",
      "Model D index # 154 saved\n",
      "Model GIS index # 155 saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m pipe\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m prep\u001b[38;5;241m.\u001b[39mready_to_train_df(ticker)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreturn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../raw_data/models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_GradientBoostingRegressor_PCA.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m index # \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtickers\u001b[38;5;241m.\u001b[39mindex(ticker)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/pipeline.py:382\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    381\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 382\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:668\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 668\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:745\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    738\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[1;32m    739\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    740\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    741\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    742\u001b[0m     )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/ensemble/_gb.py:247\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    244\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    246\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 247\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[1;32m    250\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[1;32m    251\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[1;32m    252\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    260\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/tree/_classes.py:1342\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \n\u001b[1;32m   1316\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1342\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/tree/_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    449\u001b[0m         splitter,\n\u001b[1;32m    450\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    456\u001b[0m     )\n\u001b[0;32m--> 458\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tickers = utils.return_tickers()\n",
    "for ticker in tickers:\n",
    "    model = pipe\n",
    "    df = prep.ready_to_train_df(ticker)\n",
    "    model.fit(df, df['return'])\n",
    "    joblib.dump(model, f\"../raw_data/models/{ticker}_GradientBoostingRegressor_PCA.joblib\")\n",
    "    print(f\"Model {ticker} index # {tickers.index(ticker)} saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "96c0634c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T13:15:26.061218Z",
     "start_time": "2022-12-06T13:04:59.639516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error for AAPL: 0.02080858444447846, index # 0\n",
      "error for MSFT: 0.015206903102651569, index # 1\n",
      "error for GOOG: 0.01324129329511892, index # 2\n",
      "error for AMZN: 0.025921345625733096, index # 3\n",
      "error for TSLA: 0.02871726724948178, index # 4\n",
      "error for UNH: 0.016088380768116174, index # 5\n",
      "error for XOM: 0.011418204953547556, index # 6\n",
      "error for JNJ: 0.009702425815101424, index # 7\n",
      "error for WMT: 0.011635213790065694, index # 8\n",
      "error for NVDA: 0.030239008505021907, index # 9\n",
      "error for JPM: 0.019454786442615595, index # 10\n",
      "error for V: 0.014494151045072203, index # 11\n",
      "error for CVX: 0.011894398832484792, index # 12\n",
      "error for PG: 0.009758867761764848, index # 13\n",
      "error for LLY: 0.012987168614986378, index # 14\n",
      "error for MA: 0.01748168349774195, index # 15\n",
      "error for HD: 0.01564395006624534, index # 16\n",
      "error for META: 0.018509409936715134, index # 17\n",
      "error for BAC: 0.02276847295146326, index # 18\n",
      "error for ABBV: 0.014604184631600759, index # 19\n",
      "error for PFE: 0.013143502145700425, index # 20\n",
      "error for KO: 0.010164652596368704, index # 21\n",
      "error for MRK: 0.01397781523982144, index # 22\n",
      "error for PEP: 0.009952656827050573, index # 23\n",
      "error for COST: 0.013698402136005783, index # 24\n",
      "error for ORCL: 0.018659888482619933, index # 25\n",
      "error for AVGO: 0.01887549777347928, index # 26\n",
      "error for TMO: 0.014904905233801963, index # 27\n",
      "error for MCD: 0.0121689255198695, index # 28\n",
      "error for CSCO: 0.0195436066628582, index # 29\n",
      "error for ACN: 0.015981815958134242, index # 30\n",
      "error for DHR: 0.0129016915218231, index # 31\n",
      "error for TMUS: 0.022938444524277183, index # 32\n",
      "error for ABT: 0.012507655730229198, index # 33\n",
      "error for WFC: 0.018975205024064197, index # 34\n",
      "error for DIS: 0.014898582177552484, index # 35\n",
      "error for LIN: 0.015766912701443054, index # 36\n",
      "error for NEE: 0.011438683924751315, index # 37\n",
      "error for BMY: 0.014362539779907509, index # 38\n",
      "error for NKE: 0.0147979622574597, index # 39\n",
      "error for VZ: 0.01309752828217947, index # 40\n",
      "error for TXN: 0.019518808586378344, index # 41\n",
      "error for UPS: 0.010848610057569982, index # 42\n",
      "error for COP: 0.014221246732371661, index # 43\n",
      "error for ADBE: 0.02056004895259774, index # 44\n",
      "error for CMCSA: 0.017340340088391998, index # 45\n",
      "error for CRM: 0.02310928376468414, index # 46\n",
      "error for PM: 0.01128194905840243, index # 47\n",
      "error for MS: 0.022867367466732316, index # 48\n",
      "error for AMGN: 0.01591795552064675, index # 49\n",
      "error for SCHW: 0.021435841819156837, index # 50\n",
      "error for HON: 0.015036726037374624, index # 51\n",
      "error for RTX: 0.012938765622689934, index # 52\n",
      "error for QCOM: 0.020481980917153815, index # 53\n",
      "error for T: 0.013433469493766875, index # 54\n",
      "error for IBM: 0.012693307762429468, index # 55\n",
      "error for DE: 0.016808289100177835, index # 56\n",
      "error for CVS: 0.014477624489780593, index # 57\n",
      "error for LOW: 0.017038033741086624, index # 58\n",
      "error for GS: 0.018232405545151684, index # 59\n",
      "error for UNP: 0.014935538978574036, index # 60\n",
      "error for NFLX: 0.032430532558877755, index # 61\n",
      "error for LMT: 0.012815173144390362, index # 62\n",
      "error for CAT: 0.01654829517788269, index # 63\n",
      "error for AMD: 0.034386341835034064, index # 64\n",
      "error for INTC: 0.01889702829277902, index # 65\n",
      "error for ELV: 0.016293602966390684, index # 66\n",
      "error for SPGI: 0.015401065156301868, index # 67\n",
      "error for AXP: 0.01810971334062052, index # 68\n",
      "error for SBUX: 0.017553144701620407, index # 69\n",
      "error for INTU: 0.017658096748867324, index # 70\n",
      "error for BLK: 0.018073902398183905, index # 71\n",
      "error for ADP: 0.012070471082678107, index # 72\n",
      "error for GILD: 0.02006275353114126, index # 73\n",
      "error for PLD: 0.019230907821314168, index # 74\n",
      "error for MDT: 0.013046947182214459, index # 75\n",
      "error for BA: 0.01574018160688211, index # 76\n",
      "error for AMT: 0.024474551560711925, index # 77\n",
      "error for CI: 0.01881990910227, index # 78\n",
      "error for GE: 0.015926485314377514, index # 79\n",
      "error for TJX: 0.015125677540557152, index # 80\n",
      "error for ISRG: 0.02430432081246293, index # 81\n",
      "error for C: 0.022836261559069027, index # 82\n",
      "error for AMAT: 0.021556272492312418, index # 83\n",
      "error for PYPL: 0.016803979102506528, index # 84\n",
      "error for MDLZ: 0.011936330101828896, index # 85\n",
      "error for CB: 0.014392157951485568, index # 86\n",
      "error for SYK: 0.013296413850855306, index # 87\n",
      "error for ADI: 0.01995420179238292, index # 88\n",
      "error for MMC: 0.013760388105844918, index # 89\n",
      "error for EOG: 0.018110602632233005, index # 90\n",
      "error for NOW: 0.020944192540086785, index # 91\n",
      "error for VRTX: 0.028870947846977652, index # 92\n",
      "error for MO: 0.011918603772385556, index # 93\n",
      "error for NOC: 0.01261510175901291, index # 94\n",
      "error for EL: 0.014690111048460797, index # 95\n",
      "error for REGN: 0.030876821605116848, index # 96\n",
      "error for PGR: 0.013854905918376993, index # 97\n",
      "error for BKNG: 0.030028500781640526, index # 98\n",
      "error for DUK: 0.012867240156965158, index # 99\n",
      "error for TGT: 0.01670897796081488, index # 100\n",
      "error for SLB: 0.016757283617937133, index # 101\n",
      "error for SO: 0.010025637617944165, index # 102\n",
      "error for MMM: 0.011450076719635141, index # 103\n",
      "error for ITW: 0.013258698566779439, index # 104\n",
      "error for ZTS: 0.012466327832465868, index # 105\n",
      "error for GD: 0.013314167151078598, index # 106\n",
      "error for APD: 0.013916723164728188, index # 107\n",
      "error for HUM: 0.02023482872588081, index # 108\n",
      "error for MRNA: 0.03971158980759704, index # 109\n",
      "error for BDX: 0.011832695553074913, index # 110\n",
      "error for CSX: 0.017800570406920325, index # 111\n",
      "error for WM: 0.01211191927757532, index # 112\n",
      "error for PNC: 0.017828870510379822, index # 113\n",
      "error for HCA: 0.017431013186885218, index # 114\n",
      "error for ETN: 0.01516132971263054, index # 115\n",
      "error for USB: 0.016481163935841252, index # 116\n",
      "error for FISV: 0.014651290019222527, index # 117\n",
      "error for SHW: 0.01471847303362707, index # 118\n",
      "error for OXY: 0.014967590791971916, index # 119\n",
      "error for CL: 0.010649584576903987, index # 120\n",
      "error for MU: 0.030245113108138558, index # 121\n",
      "error for CME: 0.0185190788985151, index # 122\n",
      "error for AON: 0.013911272675672754, index # 123\n",
      "error for LRCX: 0.024948815643892328, index # 124\n",
      "error for BSX: 0.019357583401638116, index # 125\n",
      "error for EQIX: 0.031179044645595563, index # 126\n",
      "error for TFC: 0.016813252558882268, index # 127\n",
      "error for PXD: 0.020220721896954636, index # 128\n",
      "error for CHTR: 0.015306061894472011, index # 129\n",
      "error for CCI: 0.023355673225867936, index # 130\n",
      "error for MET: 0.019151095701099803, index # 131\n",
      "error for ATVI: 0.022166039225167248, index # 132\n",
      "error for ICE: 0.02173239797766626, index # 133\n",
      "error for MPC: 0.020020226312147258, index # 134\n",
      "error for NSC: 0.017531659798254298, index # 135\n",
      "error for DG: 0.013554065343089686, index # 136\n",
      "error for GM: 0.016152145239849185, index # 137\n",
      "error for EMR: 0.014172366338614363, index # 138\n",
      "error for F: 0.02216229822680444, index # 139\n",
      "error for KLAC: 0.02228566180383642, index # 140\n",
      "error for MCO: 0.017765500691267196, index # 141\n",
      "error for FCX: 0.025302872592264176, index # 142\n",
      "error for KDP: 0.01256268759633473, index # 143\n",
      "error for MNST: 0.02636665962496747, index # 144\n",
      "error for MCK: 0.015386157780363497, index # 145\n",
      "error for VLO: 0.020820551784867643, index # 146\n",
      "error for ORLY: 0.01647323181241713, index # 147\n",
      "error for ADM: 0.016218602420603582, index # 148\n",
      "error for PSX: 0.015567482377471987, index # 149\n",
      "error for PSA: 0.015268966844429521, index # 150\n",
      "error for SRE: 0.01260966576675844, index # 151\n",
      "error for SNPS: 0.016356265371581975, index # 152\n",
      "error for MAR: 0.017086179847706315, index # 153\n",
      "error for D: 0.011010961235765178, index # 154\n",
      "error for GIS: 0.010053432173964542, index # 155\n",
      "error for AEP: 0.012661153846892058, index # 156\n",
      "error for AZO: 0.01450098094666688, index # 157\n",
      "error for KHC: 0.013623639166862043, index # 158\n",
      "error for APH: 0.01699504253482457, index # 159\n",
      "error for HSY: 0.010941501461157912, index # 160\n",
      "error for CNC: 0.02310670799472875, index # 161\n",
      "error for CTVA: 0.016766912699734446, index # 162\n",
      "error for EW: 0.015768426122432958, index # 163\n",
      "error for CTAS: 0.014416473373510794, index # 164\n",
      "error for A: 0.0200146602407246, index # 165\n",
      "error for ROP: 0.015227972766610885, index # 166\n",
      "error for JCI: 0.018910108399479476, index # 167\n",
      "error for CDNS: 0.02072460129935855, index # 168\n",
      "error for FDX: 0.015304488361195539, index # 169\n",
      "error for NXPI: 0.022158127003713973, index # 170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error for AIG: 0.02623298861597308, index # 171\n",
      "error for KMB: 0.010044893303547187, index # 172\n",
      "error for AFL: 0.017269296463913494, index # 173\n",
      "error for HES: 0.018097448452239012, index # 174\n",
      "error for MSI: 0.021781456514435398, index # 175\n",
      "error for PAYX: 0.01408025549042231, index # 176\n",
      "error for DVN: 0.018664267525193713, index # 177\n",
      "error for TRV: 0.014568436701258212, index # 178\n",
      "error for BIIB: 0.021704168472556556, index # 179\n",
      "error for DXCM: 0.02924236780784924, index # 180\n",
      "error for SYY: 0.012113470752171888, index # 181\n",
      "error for LHX: 0.01659833540419641, index # 182\n",
      "error for RSG: 0.012873325902549911, index # 183\n",
      "error for ENPH: 0.04554726201311579, index # 184\n",
      "error for ECL: 0.01204785102598911, index # 185\n",
      "error for ADSK: 0.02093281805989299, index # 186\n",
      "error for MCHP: 0.02007263298989747, index # 187\n",
      "error for ANET: 0.021389327642160748, index # 188\n",
      "error for KMI: 0.015039343910533031, index # 189\n",
      "error for CMG: 0.021392499650357073, index # 190\n",
      "error for FTNT: 0.020687788742595598, index # 191\n",
      "error for AJG: 0.012806940504811466, index # 192\n",
      "error for STZ: 0.01611289405736805, index # 193\n",
      "error for TT: 0.017197451714788686, index # 194\n",
      "error for WMB: 0.026838560886288436, index # 195\n",
      "error for MSCI: 0.017413038467578357, index # 196\n",
      "error for O: 0.015147829307847725, index # 197\n",
      "error for IQV: 0.013451936495486039, index # 198\n",
      "error for TEL: 0.015327721325797392, index # 199\n",
      "error for ROST: 0.016809950928756628, index # 200\n",
      "error for PRU: 0.019807162198492646, index # 201\n",
      "error for EXC: 0.013399451313800916, index # 202\n",
      "error for PH: 0.015935960525318885, index # 203\n",
      "error for FIS: 0.014925654141362397, index # 204\n",
      "error for SPG: 0.016585198138692046, index # 205\n",
      "error for COF: 0.024082330264883094, index # 206\n",
      "error for NUE: 0.020468713501274764, index # 207\n",
      "error for XEL: 0.012893358132332493, index # 208\n",
      "error for HLT: 0.014890542538879378, index # 209\n",
      "error for CARR: 0.01670319824682539, index # 210\n",
      "error for PCAR: 0.018012515000804617, index # 211\n",
      "error for BK: 0.01819423003963098, index # 212\n",
      "error for NEM: 0.018010861401026458, index # 213\n",
      "error for DOW: 0.016948793988987257, index # 214\n",
      "error for EA: 0.021120700853942572, index # 215\n",
      "error for WBA: 0.014482304037135326, index # 216\n",
      "error for DD: 0.014086291699085209, index # 217\n",
      "error for ALL: 0.014796333833442503, index # 218\n",
      "error for YUM: 0.014915751024207974, index # 219\n",
      "error for AMP: 0.0193368973852491, index # 220\n",
      "error for CMI: 0.020708058288801734, index # 221\n",
      "error for ILMN: 0.02999410815491444, index # 222\n",
      "error for TDG: 0.015464309839284604, index # 223\n",
      "error for IDXX: 0.016151790662997538, index # 224\n",
      "error for ED: 0.0097234946616872, index # 225\n",
      "error for KR: 0.014563923888291216, index # 226\n",
      "error for ABC: 0.014455137212407711, index # 227\n",
      "error for DLTR: 0.01933346408258106, index # 228\n",
      "error for RMD: 0.016014472775998777, index # 229\n",
      "error for ALB: 0.01775031656741293, index # 230\n",
      "error for HAL: 0.020216178829958225, index # 231\n",
      "error for NDAQ: 0.020928284488201093, index # 232\n",
      "error for LVS: 0.029680261400227174, index # 233\n",
      "error for ODFL: 0.023097069626647412, index # 234\n",
      "error for WELL: 0.014262891279177448, index # 235\n",
      "error for AME: 0.014266396405555522, index # 236\n",
      "error for CSGP: 0.021330885451390082, index # 237\n",
      "error for OTIS: 0.012773598880342487, index # 238\n",
      "error for MTD: 0.014748210482760479, index # 239\n",
      "error for SBAC: 0.0316983258365107, index # 240\n",
      "error for ON: 0.03234993521729171, index # 241\n",
      "error for VICI: 0.014187448587575022, index # 242\n",
      "error for DLR: 0.01716736419016985, index # 243\n",
      "error for KEYS: 0.015453557920580776, index # 244\n",
      "error for PPG: 0.01413753058663191, index # 245\n",
      "error for WEC: 0.010057291514267491, index # 246\n",
      "error for CTSH: 0.021459641745456393, index # 247\n",
      "error for ROK: 0.01774902006503629, index # 248\n",
      "error for GWW: 0.013980910181300956, index # 249\n",
      "error for PCG: 0.01463956397696714, index # 250\n",
      "error for HPQ: 0.019227931324980085, index # 251\n",
      "error for FAST: 0.017641234391185198, index # 252\n",
      "error for DFS: 0.02060427144076729, index # 253\n",
      "error for MTB: 0.015963709088921973, index # 254\n",
      "error for PEG: 0.013167111210128825, index # 255\n",
      "error for OKE: 0.01547750633296027, index # 256\n",
      "error for DHI: 0.02596174010873323, index # 257\n",
      "error for APTV: 0.017472976444337034, index # 258\n",
      "error for BKR: 0.02050174327480154, index # 259\n",
      "error for GLW: 0.02507703433750266, index # 260\n",
      "error for LYB: 0.017772781208232533, index # 261\n",
      "error for ES: 0.011344814270850438, index # 262\n",
      "error for BAX: 0.013190249428102471, index # 263\n",
      "error for STT: 0.019824904296530166, index # 264\n",
      "error for VRSK: 0.010716277506869829, index # 265\n",
      "error for TROW: 0.018362103313790748, index # 266\n",
      "error for WBD: 0.017590341871583422, index # 267\n",
      "error for AWK: 0.011035629099839532, index # 268\n",
      "error for IT: 0.019621410899052984, index # 269\n",
      "error for GPN: 0.01711202949804222, index # 270\n",
      "error for HRL: 0.011685768772174003, index # 271\n",
      "error for FANG: 0.02337802224338813, index # 272\n",
      "error for WTW: 0.011852518692771206, index # 273\n",
      "error for RJF: 0.019975364440696045, index # 274\n",
      "error for GPC: 0.011821351297444297, index # 275\n",
      "error for IFF: 0.012299963148912551, index # 276\n",
      "error for CDW: 0.014301233520052978, index # 277\n",
      "error for TSCO: 0.020224328595621777, index # 278\n",
      "error for FITB: 0.02343739716716442, index # 279\n",
      "error for ARE: 0.015648267280623726, index # 280\n",
      "error for URI: 0.02691797129012809, index # 281\n",
      "error for ZBH: 0.014565374712721967, index # 282\n",
      "error for K: 0.010379139797040835, index # 283\n",
      "error for LEN: 0.026492274346576545, index # 284\n",
      "error for EBAY: 0.020481137448317366, index # 285\n",
      "error for EIX: 0.01549740253811132, index # 286\n",
      "error for CBRE: 0.02641989461763989, index # 287\n",
      "error for EFX: 0.014054515060552797, index # 288\n",
      "error for VMC: 0.018535744061165895, index # 289\n",
      "error for TSN: 0.019013274134316752, index # 290\n",
      "error for HIG: 0.024453236830782973, index # 291\n",
      "error for FTV: 0.012768475735511337, index # 292\n",
      "error for WY: 0.016566382247320308, index # 293\n",
      "error for EQR: 0.017006856247030223, index # 294\n",
      "error for AVB: 0.01598980411956489, index # 295\n",
      "error for MKC: 0.010690290681551435, index # 296\n",
      "error for ETR: 0.012169660411455565, index # 297\n",
      "error for LUV: 0.01883921542860733, index # 298\n",
      "error for ULTA: 0.02325053249910784, index # 299\n",
      "error for AEE: 0.010880092932976315, index # 300\n",
      "error for MLM: 0.017960952524178085, index # 301\n",
      "error for FE: 0.013244935714376552, index # 302\n",
      "error for PFG: 0.02091815581259288, index # 303\n",
      "error for FRC: 0.013258390383141994, index # 304\n",
      "error for DTE: 0.010959266100439895, index # 305\n",
      "error for DAL: 0.026034311071048907, index # 306\n",
      "error for HBAN: 0.025951001782164203, index # 307\n",
      "error for IR: 0.017830788919052133, index # 308\n",
      "error for CTRA: 0.021273729468227302, index # 309\n",
      "error for ANSS: 0.0203300505939035, index # 310\n",
      "error for ACGL: 0.012722901758915373, index # 311\n",
      "error for PPL: 0.012481390495147208, index # 312\n",
      "error for RF: 0.023982081212409648, index # 313\n",
      "error for VRSN: 0.025935251083574928, index # 314\n",
      "error for LH: 0.013500797922779369, index # 315\n",
      "error for EXR: 0.01765826090394039, index # 316\n",
      "error for PWR: 0.025492910713818758, index # 317\n",
      "error for CF: 0.02398982684922666, index # 318\n",
      "error for CAH: 0.014359170555210207, index # 319\n",
      "error for CFG: 0.016193900143477987, index # 320\n",
      "error for XYL: 0.013004606066429477, index # 321\n",
      "error for HPE: 0.016936339048830608, index # 322\n",
      "error for EPAM: 0.018499269165149285, index # 323\n",
      "error for DOV: 0.01513244016206905, index # 324\n",
      "error for WAT: 0.0174248605652119, index # 325\n",
      "error for WRB: 0.01317884980580816, index # 326\n",
      "error for TDY: 0.01894640658667909, index # 327\n",
      "error for PAYC: 0.023832552825672672, index # 328\n",
      "error for ROL: 0.016055996723953192, index # 329\n",
      "error for NTRS: 0.0164723540574587, index # 330\n",
      "error for MRO: 0.018370676270803302, index # 331\n",
      "error for CNP: 0.016249633922941608, index # 332\n",
      "error for INVH: 0.01164864824563041, index # 333\n",
      "error for CHD: 0.011924015770313125, index # 334\n",
      "error for AES: 0.026146435091569643, index # 335\n",
      "error for MOH: 0.02238324608894229, index # 336\n",
      "error for JBHT: 0.01917546649264544, index # 337\n",
      "error for MAA: 0.01563207612625731, index # 338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error for BBY: 0.02259895085256741, index # 339\n",
      "error for CLX: 0.010893149995019754, index # 340\n",
      "error for HOLX: 0.02401210455362336, index # 341\n",
      "error for WAB: 0.01867023623987163, index # 342\n",
      "error for DRI: 0.01762725072741479, index # 343\n",
      "error for EXPD: 0.017604251098831923, index # 344\n",
      "error for STE: 0.016394736323715825, index # 345\n",
      "error for AMCR: 0.01397378921528542, index # 346\n",
      "error for VTR: 0.01718916330107004, index # 347\n",
      "error for IEX: 0.014971740875558776, index # 348\n",
      "error for CAG: 0.01125936375379594, index # 349\n",
      "error for CMS: 0.015222050189753034, index # 350\n",
      "error for KEY: 0.021573250810868125, index # 351\n",
      "error for MPWR: 0.024305378923281953, index # 352\n",
      "error for BALL: 0.014116463543082188, index # 353\n",
      "error for J: 0.019149960703062463, index # 354\n",
      "error for BR: 0.013913353431214722, index # 355\n",
      "error for GRMN: 0.020981714773733607, index # 356\n",
      "error for PKI: 0.018911490560713656, index # 357\n",
      "error for TTWO: 0.026558156274094205, index # 358\n",
      "error for INCY: 0.03479921041701149, index # 359\n",
      "error for FDS: 0.017528531298265256, index # 360\n",
      "error for MOS: 0.02364842193346282, index # 361\n",
      "error for SEDG: 0.0322752098177442, index # 362\n",
      "error for CINF: 0.013258626857696618, index # 363\n",
      "error for ABMD: 0.03070936273084842, index # 364\n",
      "error for DGX: 0.014799788739299291, index # 365\n",
      "error for WST: 0.017062834961672955, index # 366\n",
      "error for ATO: 0.011165490233032786, index # 367\n",
      "error for TRGP: 0.023296497462581323, index # 368\n",
      "error for BRO: 0.013867496527058328, index # 369\n",
      "error for SYF: 0.01656452472273087, index # 370\n",
      "error for FOX: 0.015823198509934057, index # 371\n",
      "error for FOXA: 0.01673317761993442, index # 372\n",
      "error for NTAP: 0.028363212729606836, index # 373\n",
      "error for FMC: 0.0183561453539706, index # 374\n",
      "error for EQT: 0.016065820748877396, index # 375\n",
      "error for OMC: 0.014187897738800572, index # 376\n",
      "error for SJM: 0.012219382475012383, index # 377\n",
      "error for LYV: 0.023863448282854784, index # 378\n",
      "error for CPB: 0.011389890584769823, index # 379\n",
      "error for HWM: 0.021576557403675863, index # 380\n",
      "error for CPRT: 0.016657922896710895, index # 381\n",
      "error for AVY: 0.014664680271236429, index # 382\n",
      "error for IRM: 0.016309632184973367, index # 383\n",
      "error for COO: 0.017020287844178115, index # 384\n",
      "error for ALGN: 0.03223436948227136, index # 385\n",
      "error for SWKS: 0.032849285566287564, index # 386\n",
      "error for EXPE: 0.020231924140354546, index # 387\n",
      "error for RCL: 0.023956803882934696, index # 388\n",
      "error for ETSY: 0.02899264537875439, index # 389\n",
      "error for APA: 0.017207981670973573, index # 390\n",
      "error for GEN: 0.019973470198077686, index # 391\n",
      "error for TXT: 0.020238234925902647, index # 392\n",
      "error for LDOS: 0.013391578933282373, index # 393\n",
      "error for LKQ: 0.01886233439132849, index # 394\n",
      "error for TER: 0.026327262118332858, index # 395\n",
      "error for PTC: 0.025194238568919315, index # 396\n",
      "error for TRMB: 0.025270312149454275, index # 397\n",
      "error for AKAM: 0.03364651104103524, index # 398\n",
      "error for NVR: 0.019411377769220896, index # 399\n",
      "error for UAL: 0.032067559694917314, index # 400\n",
      "error for LNT: 0.011464997968870804, index # 401\n",
      "error for FLT: 0.015757094568480172, index # 402\n",
      "error for KIM: 0.01858510397289007, index # 403\n",
      "error for ZBRA: 0.020518474777091004, index # 404\n",
      "error for TYL: 0.02042588459293404, index # 405\n",
      "error for DPZ: 0.018640613421759065, index # 406\n",
      "error for JKHY: 0.015381185508758463, index # 407\n",
      "error for MGM: 0.027921559120149, index # 408\n",
      "error for ESS: 0.01484727970506186, index # 409\n",
      "error for L: 0.013484754521768673, index # 410\n",
      "error for PEAK: 0.017046851881601977, index # 411\n",
      "error for MTCH: 0.023814805877151797, index # 412\n",
      "error for NDSN: 0.019425734604012573, index # 413\n",
      "error for EVRG: 0.011741347204804246, index # 414\n",
      "error for IPG: 0.020607790924972457, index # 415\n",
      "error for BEN: 0.01728923836085612, index # 416\n",
      "error for CBOE: 0.012979901386473902, index # 417\n",
      "error for TECH: 0.016294767965975076, index # 418\n",
      "error for SIVB: 0.020255286622276256, index # 419\n",
      "error for VFC: 0.01477104354139621, index # 420\n",
      "error for IP: 0.01791054323850132, index # 421\n",
      "error for HST: 0.021377065315942884, index # 422\n",
      "error for UDR: 0.016774673117282743, index # 423\n",
      "error for POOL: 0.01960242377607989, index # 424\n",
      "error for RE: 0.013794525969670584, index # 425\n",
      "error for PARA: 0.01902944123001167, index # 426\n",
      "error for SNA: 0.014195247496701154, index # 427\n",
      "error for CPT: 0.016638549653408774, index # 428\n",
      "error for LW: 0.014265659321068115, index # 429\n",
      "error for PKG: 0.01636623250662027, index # 430\n",
      "error for CRL: 0.01640679851318596, index # 431\n",
      "error for SWK: 0.01584401784188028, index # 432\n",
      "error for BIO: 0.016268003968053397, index # 433\n",
      "error for WDC: 0.028501074003276235, index # 434\n",
      "error for CHRW: 0.015943934666724755, index # 435\n",
      "error for STX: 0.025205860908794155, index # 436\n",
      "error for MAS: 0.020054141015790675, index # 437\n",
      "error for GL: 0.01393371201634575, index # 438\n",
      "error for CE: 0.019325487009685182, index # 439\n",
      "error for REG: 0.017754717876049795, index # 440\n",
      "error for NI: 0.012464398436183782, index # 441\n",
      "error for BXP: 0.016119748766422068, index # 442\n",
      "error for HSIC: 0.014060386378664134, index # 443\n",
      "error for CCL: 0.017251174316706964, index # 444\n",
      "error for TFX: 0.013362533545834181, index # 445\n",
      "error for CZR: 0.027569034205272246, index # 446\n",
      "error for NWS: 0.013903846984432412, index # 447\n",
      "error for NWSA: 0.014011024368894193, index # 448\n",
      "error for KMX: 0.023486451120855543, index # 449\n",
      "error for EMN: 0.01622865441331161, index # 450\n",
      "error for JNPR: 0.028241931581152208, index # 451\n",
      "error for PHM: 0.026157407461372757, index # 452\n",
      "error for CDAY: 0.021961630716702214, index # 453\n",
      "error for ALLE: 0.012529373210961264, index # 454\n",
      "error for QRVO: 0.020579024861236306, index # 455\n",
      "error for BWA: 0.019184564024252983, index # 456\n",
      "error for NRG: 0.01915587386939218, index # 457\n",
      "error for MKTX: 0.023884530239280038, index # 458\n",
      "error for WRK: 0.01762378088325782, index # 459\n",
      "error for UHS: 0.01669594843553005, index # 460\n",
      "error for FFIV: 0.03036969875093553, index # 461\n",
      "error for AOS: 0.018397471969152045, index # 462\n",
      "error for CMA: 0.019288538794030387, index # 463\n",
      "error for AAL: 0.035633018092483815, index # 464\n",
      "error for BBWI: 0.020034582602256875, index # 465\n",
      "error for HII: 0.014080989605520058, index # 466\n",
      "error for AAP: 0.017185442353159695, index # 467\n",
      "error for TPR: 0.021892475224442463, index # 468\n",
      "error for FRT: 0.014925114115215053, index # 469\n",
      "error for IVZ: 0.020217534388049695, index # 470\n",
      "error for PNW: 0.011806615897805513, index # 471\n",
      "error for HAS: 0.01560445815160341, index # 472\n",
      "error for WYNN: 0.028152938703653565, index # 473\n",
      "error for FBHS: 0.015453416812395027, index # 474\n",
      "error for SBNY: 0.01801374649857103, index # 475\n",
      "error for DISH: 0.020553359750552315, index # 476\n",
      "error for RHI: 0.019118214896933466, index # 477\n",
      "error for WHR: 0.019753264042965808, index # 478\n",
      "error for ZION: 0.022559047512685743, index # 479\n",
      "error for CTLT: 0.017795941133468565, index # 480\n",
      "error for PNR: 0.016362309506117848, index # 481\n",
      "error for SEE: 0.016906400291024366, index # 482\n",
      "error for RL: 0.01977403839766242, index # 483\n",
      "error for NCLH: 0.02284793663590886, index # 484\n",
      "error for DXC: 0.02311483352058883, index # 485\n",
      "error for GNRC: 0.01942291859822142, index # 486\n",
      "error for AIZ: 0.01566292647530512, index # 487\n",
      "error for XRAY: 0.01329078636309716, index # 488\n",
      "error for LNC: 0.023012607454109445, index # 489\n",
      "error for DVA: 0.014695201212818252, index # 490\n",
      "error for MHK: 0.01889022314900316, index # 491\n",
      "error for LUMN: 0.015542212651491303, index # 492\n",
      "error for ALK: 0.023836213256357965, index # 493\n",
      "error for NWL: 0.01696878204116547, index # 494\n",
      "error for VNO: 0.016322204587737876, index # 495\n",
      "error for TAP: 0.013600338505203572, index # 496\n"
     ]
    }
   ],
   "source": [
    "tickers = utils.return_tickers()\n",
    "scores = []\n",
    "for ticker in tickers:\n",
    "    model = pipe\n",
    "    df = prep.ready_to_train_df(ticker)\n",
    "    model.fit(df, df['return'])\n",
    "    error = (mean_squared_error(df['return'], model.predict(df)))**0.5\n",
    "    scores.append(error)\n",
    "    print(f\"error for {ticker}: {error}, index # {tickers.index(ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0b699c6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T13:16:09.456364Z",
     "start_time": "2022-12-06T13:16:09.425536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018041846747860253"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4489b4b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-06T13:25:36.327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline done for AAPL index # 0\n",
      "baseline done for MSFT index # 1\n",
      "baseline done for GOOG index # 2\n",
      "baseline done for AMZN index # 3\n",
      "baseline done for TSLA index # 4\n",
      "baseline done for UNH index # 5\n",
      "baseline done for XOM index # 6\n",
      "baseline done for JNJ index # 7\n",
      "baseline done for WMT index # 8\n",
      "baseline done for NVDA index # 9\n",
      "baseline done for JPM index # 10\n",
      "baseline done for V index # 11\n",
      "baseline done for CVX index # 12\n",
      "baseline done for PG index # 13\n",
      "baseline done for LLY index # 14\n",
      "baseline done for MA index # 15\n",
      "baseline done for HD index # 16\n",
      "baseline done for META index # 17\n",
      "baseline done for BAC index # 18\n",
      "baseline done for ABBV index # 19\n",
      "baseline done for PFE index # 20\n",
      "baseline done for KO index # 21\n",
      "baseline done for MRK index # 22\n",
      "baseline done for PEP index # 23\n",
      "baseline done for COST index # 24\n",
      "baseline done for ORCL index # 25\n",
      "baseline done for AVGO index # 26\n",
      "baseline done for TMO index # 27\n",
      "baseline done for MCD index # 28\n",
      "baseline done for CSCO index # 29\n",
      "baseline done for ACN index # 30\n",
      "baseline done for DHR index # 31\n",
      "baseline done for TMUS index # 32\n",
      "baseline done for ABT index # 33\n",
      "baseline done for WFC index # 34\n",
      "baseline done for DIS index # 35\n",
      "baseline done for LIN index # 36\n",
      "baseline done for NEE index # 37\n",
      "baseline done for BMY index # 38\n",
      "baseline done for NKE index # 39\n",
      "baseline done for VZ index # 40\n",
      "baseline done for TXN index # 41\n",
      "baseline done for UPS index # 42\n",
      "baseline done for COP index # 43\n",
      "baseline done for ADBE index # 44\n",
      "baseline done for CMCSA index # 45\n",
      "baseline done for CRM index # 46\n",
      "baseline done for PM index # 47\n",
      "baseline done for MS index # 48\n",
      "baseline done for AMGN index # 49\n",
      "baseline done for SCHW index # 50\n",
      "baseline done for HON index # 51\n",
      "baseline done for RTX index # 52\n",
      "baseline done for QCOM index # 53\n",
      "baseline done for T index # 54\n",
      "baseline done for IBM index # 55\n",
      "baseline done for DE index # 56\n",
      "baseline done for CVS index # 57\n",
      "baseline done for LOW index # 58\n",
      "baseline done for GS index # 59\n",
      "baseline done for UNP index # 60\n",
      "baseline done for NFLX index # 61\n",
      "baseline done for LMT index # 62\n",
      "baseline done for CAT index # 63\n",
      "baseline done for AMD index # 64\n",
      "baseline done for INTC index # 65\n",
      "baseline done for ELV index # 66\n",
      "baseline done for SPGI index # 67\n",
      "baseline done for AXP index # 68\n",
      "baseline done for SBUX index # 69\n",
      "baseline done for INTU index # 70\n",
      "baseline done for BLK index # 71\n",
      "baseline done for ADP index # 72\n",
      "baseline done for GILD index # 73\n",
      "baseline done for PLD index # 74\n",
      "baseline done for MDT index # 75\n",
      "baseline done for BA index # 76\n",
      "baseline done for AMT index # 77\n",
      "baseline done for CI index # 78\n",
      "baseline done for GE index # 79\n",
      "baseline done for TJX index # 80\n",
      "baseline done for ISRG index # 81\n",
      "baseline done for C index # 82\n",
      "baseline done for AMAT index # 83\n",
      "baseline done for PYPL index # 84\n",
      "baseline done for MDLZ index # 85\n",
      "baseline done for CB index # 86\n",
      "baseline done for SYK index # 87\n",
      "baseline done for ADI index # 88\n",
      "baseline done for MMC index # 89\n",
      "baseline done for EOG index # 90\n",
      "baseline done for NOW index # 91\n",
      "baseline done for VRTX index # 92\n",
      "baseline done for MO index # 93\n"
     ]
    }
   ],
   "source": [
    "tickers = utils.return_tickers()\n",
    "baseline = []\n",
    "for ticker in tickers:\n",
    "    df = prep.ready_to_train_df(ticker)\n",
    "    for i in range(1, len(df), 1):\n",
    "        error = (mean_squared_error(df.loc[i,['return']], df.loc[i-1,['return']]))**0.5\n",
    "        baseline.append(error)\n",
    "    print(f\"baseline done for {ticker} index # {tickers.index(ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07c493c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T12:03:01.191322Z",
     "start_time": "2022-12-06T12:03:01.158597Z"
    }
   },
   "outputs": [],
   "source": [
    "model = joblib.load(f\"../raw_data/models/QCOM_GradientBoostingRegressor_PCA.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b46b16c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T12:03:02.397114Z",
     "start_time": "2022-12-06T12:03:02.199991Z"
    }
   },
   "outputs": [],
   "source": [
    "df_1 = prep.ready_to_test('QCOM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aa28cfa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T13:24:16.608190Z",
     "start_time": "2022-12-06T13:24:16.578272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.loc[1,['return']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8837d38e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T13:23:15.174749Z",
     "start_time": "2022-12-06T13:23:15.144138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00568573397632699"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.loc[0,'return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "266d94a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T13:23:43.019633Z",
     "start_time": "2022-12-06T13:23:42.960077Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got <class 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0433386837893297\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00568573397632699\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/metrics/_regression.py:442\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_squared_error\u001b[39m(\n\u001b[1;32m    383\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    384\u001b[0m ):\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    446\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/metrics/_regression.py:100\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m        correct keyword.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    102\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/utils/validation.py:384\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_consistent_length\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \n\u001b[1;32m    376\u001b[0m \u001b[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    385\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/utils/validation.py:384\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_consistent_length\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \n\u001b[1;32m    376\u001b[0m \u001b[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[43m_num_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    385\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dynamic/lib/python3.10/site-packages/sklearn/utils/validation.py:321\u001b[0m, in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    319\u001b[0m         x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(message)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'float'>"
     ]
    }
   ],
   "source": [
    "mean_squared_error(0.0433386837893297, 0.00568573397632699)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4630a2c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T13:21:30.634228Z",
     "start_time": "2022-12-06T13:21:30.605742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009597257927532077"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.at[5,'return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a7b8e139",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T12:03:07.481037Z",
     "start_time": "2022-12-06T12:03:07.445902Z"
    }
   },
   "outputs": [],
   "source": [
    "error = (mean_squared_error(df_1['return'], y_pred))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4bf85265",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T12:03:07.873043Z",
     "start_time": "2022-12-06T12:03:07.845165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027926908567585146"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caab9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = utils.return_tickers()\n",
    "y_pred = []\n",
    "for ticker in tickers:\n",
    "    model = joblib.load(model, f\"model/{ticker}_GradientBoostingRegressor_PCA.joblib\")\n",
    "    y_pred.append(model.predict(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
