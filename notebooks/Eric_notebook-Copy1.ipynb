{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e0f1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:19:27.106744Z",
     "start_time": "2022-12-02T08:19:26.858703Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d1bd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:19:27.179623Z",
     "start_time": "2022-12-02T08:19:27.177831Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41a4a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:19:28.868554Z",
     "start_time": "2022-12-02T08:19:28.537505Z"
    }
   },
   "outputs": [],
   "source": [
    "from dynamic_portfolio.utils import load_csv\n",
    "from dynamic_portfolio.utils import features_creation, clean_data\n",
    "from dynamic_portfolio.preprocess import scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d01300",
   "metadata": {},
   "source": [
    "# Loading data and creating clean dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9420e32e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:52:46.475428Z",
     "start_time": "2022-12-02T08:52:46.368828Z"
    }
   },
   "outputs": [],
   "source": [
    "df = features_creation('META')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6363374",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:52:48.330587Z",
     "start_time": "2022-12-02T08:52:48.324007Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaebcbd",
   "metadata": {},
   "source": [
    "# Cross vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc31db2",
   "metadata": {},
   "source": [
    "We use sklearn's time series split to break up the data in different folds\n",
    "We use a sklearn example to run the first tests.\n",
    "\n",
    "Metrics used : \n",
    " - rmse\n",
    " - mae\n",
    " - R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c719f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:19:59.345931Z",
     "start_time": "2022-12-02T08:19:59.326127Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aff74a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:55:44.151243Z",
     "start_time": "2022-12-02T08:55:44.141942Z"
    }
   },
   "outputs": [],
   "source": [
    "ts_cv = TimeSeriesSplit(\n",
    "    n_splits=20,\n",
    "    gap=0,\n",
    "    max_train_size=252,\n",
    "    test_size=45,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc575dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = list(ts_cv.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcdc4ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:29:09.695999Z",
     "start_time": "2022-12-02T08:29:09.687968Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286365a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:21:46.267703Z",
     "start_time": "2022-12-02T08:21:46.259434Z"
    }
   },
   "outputs": [],
   "source": [
    "model = HistGradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f16d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:36:25.880376Z",
     "start_time": "2022-12-02T08:31:50.247799Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse = []\n",
    "for train_index, test_index in ts_cv.split(X):\n",
    "    cv_train, cv_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    model = model.fit(X,y)\n",
    "    predictions = model.predict(cv_test)\n",
    "    true_values = cv_test['return']\n",
    "    rmse.append(np.sqrt(mean_squared_error(true_values, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d38cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:37:48.457651Z",
     "start_time": "2022-12-02T08:37:48.449394Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(rmse)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f2a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:54:01.669818Z",
     "start_time": "2022-12-02T08:54:01.661065Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, X, y, cv):\n",
    "    cv_results = cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n",
    "    )\n",
    "    mae = -cv_results[\"test_neg_mean_absolute_error\"]\n",
    "    rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\n",
    "    print(\n",
    "        f\"Mean Absolute Error:     {mae.mean():.6f} +/- {mae.std():.6f}\\n\"\n",
    "        f\"Root Mean Squared Error: {rmse.mean():.6f} +/- {rmse.std():.6f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b4b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7200e5be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:56:07.338741Z",
     "start_time": "2022-12-02T08:55:50.048380Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(model, X, y, ts_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d32eb",
   "metadata": {},
   "source": [
    "# PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e2fe0e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:37:24.095118Z",
     "start_time": "2022-12-06T10:37:24.069698Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe089e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-05T16:13:09.647733Z",
     "start_time": "2022-12-05T16:13:09.417555Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.9)\n",
    "pca.fit(prep.ready_to_train_df('IBM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef574ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-05T16:13:24.046816Z",
     "start_time": "2022-12-05T16:13:24.021951Z"
    }
   },
   "outputs": [],
   "source": [
    "n_pcs= pca.n_components_ # get number of component\n",
    "# get the index of the most important feature on EACH component\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "initial_feature_names = ibm.columns\n",
    "# get the most important feature names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a7e7de",
   "metadata": {},
   "source": [
    "## Using PCA analysis on all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0569e7e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:37:12.647715Z",
     "start_time": "2022-12-06T10:37:12.621135Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4585c435",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:38:17.401120Z",
     "start_time": "2022-12-06T10:38:17.380088Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('pca', PCA()),\n",
       "  ('gradientboostingregressor', GradientBoostingRegressor())],\n",
       " 'verbose': False,\n",
       " 'pca': PCA(),\n",
       " 'gradientboostingregressor': GradientBoostingRegressor(),\n",
       " 'pca__copy': True,\n",
       " 'pca__iterated_power': 'auto',\n",
       " 'pca__n_components': None,\n",
       " 'pca__n_oversamples': 10,\n",
       " 'pca__power_iteration_normalizer': 'auto',\n",
       " 'pca__random_state': None,\n",
       " 'pca__svd_solver': 'auto',\n",
       " 'pca__tol': 0.0,\n",
       " 'pca__whiten': False,\n",
       " 'gradientboostingregressor__alpha': 0.9,\n",
       " 'gradientboostingregressor__ccp_alpha': 0.0,\n",
       " 'gradientboostingregressor__criterion': 'friedman_mse',\n",
       " 'gradientboostingregressor__init': None,\n",
       " 'gradientboostingregressor__learning_rate': 0.1,\n",
       " 'gradientboostingregressor__loss': 'squared_error',\n",
       " 'gradientboostingregressor__max_depth': 3,\n",
       " 'gradientboostingregressor__max_features': None,\n",
       " 'gradientboostingregressor__max_leaf_nodes': None,\n",
       " 'gradientboostingregressor__min_impurity_decrease': 0.0,\n",
       " 'gradientboostingregressor__min_samples_leaf': 1,\n",
       " 'gradientboostingregressor__min_samples_split': 2,\n",
       " 'gradientboostingregressor__min_weight_fraction_leaf': 0.0,\n",
       " 'gradientboostingregressor__n_estimators': 100,\n",
       " 'gradientboostingregressor__n_iter_no_change': None,\n",
       " 'gradientboostingregressor__random_state': None,\n",
       " 'gradientboostingregressor__subsample': 1.0,\n",
       " 'gradientboostingregressor__tol': 0.0001,\n",
       " 'gradientboostingregressor__validation_fraction': 0.1,\n",
       " 'gradientboostingregressor__verbose': 0,\n",
       " 'gradientboostingregressor__warm_start': False}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bab560e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:39:58.699205Z",
     "start_time": "2022-12-06T10:39:58.666792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=0.9)),\n",
       "                (&#x27;gradientboostingregressor&#x27;,\n",
       "                 GradientBoostingRegressor(learning_rate=0.08))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=0.9)),\n",
       "                (&#x27;gradientboostingregressor&#x27;,\n",
       "                 GradientBoostingRegressor(learning_rate=0.08))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=0.9)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.08)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('pca', PCA(n_components=0.9)),\n",
       "                ('gradientboostingregressor',\n",
       "                 GradientBoostingRegressor(learning_rate=0.08))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(PCA(), GradientBoostingRegressor())\n",
    "\n",
    "params = {'pca__n_components':0.9,\n",
    "                'gradientboostingregressor__max_depth':3,\n",
    "                'gradientboostingregressor__criterion':'friedman_mse',\n",
    "                'gradientboostingregressor__n_estimators':100,\n",
    "                'gradientboostingregressor__learning_rate':0.08}\n",
    "pipe.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ec3bfb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:03:57.855604Z",
     "start_time": "2022-12-06T10:42:04.427027Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker AAPL index # 0\n",
      "done for ticker MSFT index # 1\n",
      "done for ticker GOOG index # 2\n",
      "done for ticker AMZN index # 3\n",
      "done for ticker TSLA index # 4\n",
      "done for ticker UNH index # 5\n",
      "done for ticker XOM index # 6\n",
      "done for ticker JNJ index # 7\n",
      "done for ticker WMT index # 8\n",
      "done for ticker NVDA index # 9\n",
      "done for ticker JPM index # 10\n",
      "done for ticker V index # 11\n",
      "done for ticker CVX index # 12\n",
      "done for ticker PG index # 13\n",
      "done for ticker LLY index # 14\n",
      "done for ticker MA index # 15\n",
      "done for ticker HD index # 16\n",
      "done for ticker META index # 17\n",
      "done for ticker BAC index # 18\n",
      "done for ticker ABBV index # 19\n",
      "done for ticker PFE index # 20\n",
      "done for ticker KO index # 21\n",
      "done for ticker MRK index # 22\n",
      "done for ticker PEP index # 23\n",
      "done for ticker COST index # 24\n",
      "done for ticker ORCL index # 25\n",
      "done for ticker AVGO index # 26\n",
      "done for ticker TMO index # 27\n",
      "done for ticker MCD index # 28\n",
      "done for ticker CSCO index # 29\n",
      "done for ticker ACN index # 30\n",
      "done for ticker DHR index # 31\n",
      "done for ticker TMUS index # 32\n",
      "done for ticker ABT index # 33\n",
      "done for ticker WFC index # 34\n",
      "done for ticker DIS index # 35\n",
      "done for ticker LIN index # 36\n",
      "done for ticker NEE index # 37\n",
      "done for ticker BMY index # 38\n",
      "done for ticker NKE index # 39\n",
      "done for ticker VZ index # 40\n",
      "done for ticker TXN index # 41\n",
      "done for ticker UPS index # 42\n",
      "done for ticker COP index # 43\n",
      "done for ticker ADBE index # 44\n",
      "done for ticker CMCSA index # 45\n",
      "done for ticker CRM index # 46\n",
      "done for ticker PM index # 47\n",
      "done for ticker MS index # 48\n",
      "done for ticker AMGN index # 49\n",
      "done for ticker SCHW index # 50\n",
      "done for ticker HON index # 51\n",
      "done for ticker RTX index # 52\n",
      "done for ticker QCOM index # 53\n",
      "done for ticker T index # 54\n",
      "done for ticker IBM index # 55\n",
      "done for ticker DE index # 56\n",
      "done for ticker CVS index # 57\n",
      "done for ticker LOW index # 58\n",
      "done for ticker GS index # 59\n",
      "done for ticker UNP index # 60\n",
      "done for ticker NFLX index # 61\n",
      "done for ticker LMT index # 62\n",
      "done for ticker CAT index # 63\n",
      "done for ticker AMD index # 64\n",
      "done for ticker INTC index # 65\n",
      "done for ticker ELV index # 66\n",
      "done for ticker SPGI index # 67\n",
      "done for ticker AXP index # 68\n",
      "done for ticker SBUX index # 69\n",
      "done for ticker INTU index # 70\n",
      "done for ticker BLK index # 71\n",
      "done for ticker ADP index # 72\n",
      "done for ticker GILD index # 73\n",
      "done for ticker PLD index # 74\n",
      "done for ticker MDT index # 75\n",
      "done for ticker BA index # 76\n",
      "done for ticker AMT index # 77\n",
      "done for ticker CI index # 78\n",
      "done for ticker GE index # 79\n",
      "done for ticker TJX index # 80\n",
      "done for ticker ISRG index # 81\n",
      "done for ticker C index # 82\n",
      "done for ticker AMAT index # 83\n",
      "done for ticker PYPL index # 84\n",
      "done for ticker MDLZ index # 85\n",
      "done for ticker CB index # 86\n",
      "done for ticker SYK index # 87\n",
      "done for ticker ADI index # 88\n",
      "done for ticker MMC index # 89\n",
      "done for ticker EOG index # 90\n",
      "done for ticker NOW index # 91\n",
      "done for ticker VRTX index # 92\n",
      "done for ticker MO index # 93\n",
      "done for ticker NOC index # 94\n",
      "done for ticker EL index # 95\n",
      "done for ticker REGN index # 96\n",
      "done for ticker PGR index # 97\n",
      "done for ticker BKNG index # 98\n",
      "done for ticker DUK index # 99\n",
      "done for ticker TGT index # 100\n",
      "done for ticker SLB index # 101\n",
      "done for ticker SO index # 102\n",
      "done for ticker MMM index # 103\n",
      "done for ticker ITW index # 104\n",
      "done for ticker ZTS index # 105\n",
      "done for ticker GD index # 106\n",
      "done for ticker APD index # 107\n",
      "done for ticker HUM index # 108\n",
      "done for ticker MRNA index # 109\n",
      "done for ticker BDX index # 110\n",
      "done for ticker CSX index # 111\n",
      "done for ticker WM index # 112\n",
      "done for ticker PNC index # 113\n",
      "done for ticker HCA index # 114\n",
      "done for ticker ETN index # 115\n",
      "done for ticker USB index # 116\n",
      "done for ticker FISV index # 117\n",
      "done for ticker SHW index # 118\n",
      "done for ticker OXY index # 119\n",
      "done for ticker CL index # 120\n",
      "done for ticker MU index # 121\n",
      "done for ticker CME index # 122\n",
      "done for ticker AON index # 123\n",
      "done for ticker LRCX index # 124\n",
      "done for ticker BSX index # 125\n",
      "done for ticker EQIX index # 126\n",
      "done for ticker TFC index # 127\n",
      "done for ticker PXD index # 128\n",
      "done for ticker CHTR index # 129\n",
      "done for ticker CCI index # 130\n",
      "done for ticker MET index # 131\n",
      "done for ticker ATVI index # 132\n",
      "done for ticker ICE index # 133\n",
      "done for ticker MPC index # 134\n",
      "done for ticker NSC index # 135\n",
      "done for ticker DG index # 136\n",
      "done for ticker GM index # 137\n",
      "done for ticker EMR index # 138\n",
      "done for ticker F index # 139\n",
      "done for ticker KLAC index # 140\n",
      "done for ticker MCO index # 141\n",
      "done for ticker FCX index # 142\n",
      "done for ticker KDP index # 143\n",
      "done for ticker MNST index # 144\n",
      "done for ticker MCK index # 145\n",
      "done for ticker VLO index # 146\n",
      "done for ticker ORLY index # 147\n",
      "done for ticker ADM index # 148\n",
      "done for ticker PSX index # 149\n",
      "done for ticker PSA index # 150\n",
      "done for ticker SRE index # 151\n",
      "done for ticker SNPS index # 152\n",
      "done for ticker MAR index # 153\n",
      "done for ticker D index # 154\n",
      "done for ticker GIS index # 155\n",
      "done for ticker AEP index # 156\n",
      "done for ticker AZO index # 157\n",
      "done for ticker KHC index # 158\n",
      "done for ticker APH index # 159\n",
      "done for ticker HSY index # 160\n",
      "done for ticker CNC index # 161\n",
      "done for ticker CTVA index # 162\n",
      "done for ticker EW index # 163\n",
      "done for ticker CTAS index # 164\n",
      "done for ticker A index # 165\n",
      "done for ticker ROP index # 166\n",
      "done for ticker JCI index # 167\n",
      "done for ticker CDNS index # 168\n",
      "done for ticker FDX index # 169\n",
      "done for ticker NXPI index # 170\n",
      "done for ticker AIG index # 171\n",
      "done for ticker KMB index # 172\n",
      "done for ticker AFL index # 173\n",
      "done for ticker HES index # 174\n",
      "done for ticker MSI index # 175\n",
      "done for ticker PAYX index # 176\n",
      "done for ticker DVN index # 177\n",
      "done for ticker TRV index # 178\n",
      "done for ticker BIIB index # 179\n",
      "done for ticker DXCM index # 180\n",
      "done for ticker SYY index # 181\n",
      "done for ticker LHX index # 182\n",
      "done for ticker RSG index # 183\n",
      "done for ticker ENPH index # 184\n",
      "done for ticker ECL index # 185\n",
      "done for ticker ADSK index # 186\n",
      "done for ticker MCHP index # 187\n",
      "done for ticker ANET index # 188\n",
      "done for ticker KMI index # 189\n",
      "done for ticker CMG index # 190\n",
      "done for ticker FTNT index # 191\n",
      "done for ticker AJG index # 192\n",
      "done for ticker STZ index # 193\n",
      "done for ticker TT index # 194\n",
      "done for ticker WMB index # 195\n",
      "done for ticker MSCI index # 196\n",
      "done for ticker O index # 197\n",
      "done for ticker IQV index # 198\n",
      "done for ticker TEL index # 199\n",
      "done for ticker ROST index # 200\n",
      "done for ticker PRU index # 201\n",
      "done for ticker EXC index # 202\n",
      "done for ticker PH index # 203\n",
      "done for ticker FIS index # 204\n",
      "done for ticker SPG index # 205\n",
      "done for ticker COF index # 206\n",
      "done for ticker NUE index # 207\n",
      "done for ticker XEL index # 208\n",
      "done for ticker HLT index # 209\n",
      "done for ticker CARR index # 210\n",
      "done for ticker PCAR index # 211\n",
      "done for ticker BK index # 212\n",
      "done for ticker NEM index # 213\n",
      "done for ticker DOW index # 214\n",
      "done for ticker EA index # 215\n",
      "done for ticker WBA index # 216\n",
      "done for ticker DD index # 217\n",
      "done for ticker ALL index # 218\n",
      "done for ticker YUM index # 219\n",
      "done for ticker AMP index # 220\n",
      "done for ticker CMI index # 221\n",
      "done for ticker ILMN index # 222\n",
      "done for ticker TDG index # 223\n",
      "done for ticker IDXX index # 224\n",
      "done for ticker ED index # 225\n",
      "done for ticker KR index # 226\n",
      "done for ticker ABC index # 227\n",
      "done for ticker DLTR index # 228\n",
      "done for ticker RMD index # 229\n",
      "done for ticker ALB index # 230\n",
      "done for ticker HAL index # 231\n",
      "done for ticker NDAQ index # 232\n",
      "done for ticker LVS index # 233\n",
      "done for ticker ODFL index # 234\n",
      "done for ticker WELL index # 235\n",
      "done for ticker AME index # 236\n",
      "done for ticker CSGP index # 237\n",
      "done for ticker OTIS index # 238\n",
      "done for ticker MTD index # 239\n",
      "done for ticker SBAC index # 240\n",
      "done for ticker ON index # 241\n",
      "done for ticker VICI index # 242\n",
      "done for ticker DLR index # 243\n",
      "done for ticker KEYS index # 244\n",
      "done for ticker PPG index # 245\n",
      "done for ticker WEC index # 246\n",
      "done for ticker CTSH index # 247\n",
      "done for ticker ROK index # 248\n",
      "done for ticker GWW index # 249\n",
      "done for ticker PCG index # 250\n",
      "done for ticker HPQ index # 251\n",
      "done for ticker FAST index # 252\n",
      "done for ticker DFS index # 253\n",
      "done for ticker MTB index # 254\n",
      "done for ticker PEG index # 255\n",
      "done for ticker OKE index # 256\n",
      "done for ticker DHI index # 257\n",
      "done for ticker APTV index # 258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker BKR index # 259\n",
      "done for ticker GLW index # 260\n",
      "done for ticker LYB index # 261\n",
      "done for ticker ES index # 262\n",
      "done for ticker BAX index # 263\n",
      "done for ticker STT index # 264\n",
      "done for ticker VRSK index # 265\n",
      "done for ticker TROW index # 266\n",
      "done for ticker WBD index # 267\n",
      "done for ticker AWK index # 268\n",
      "done for ticker IT index # 269\n",
      "done for ticker GPN index # 270\n",
      "done for ticker HRL index # 271\n",
      "done for ticker FANG index # 272\n",
      "done for ticker WTW index # 273\n",
      "done for ticker RJF index # 274\n",
      "done for ticker GPC index # 275\n",
      "done for ticker IFF index # 276\n",
      "done for ticker CDW index # 277\n",
      "done for ticker TSCO index # 278\n",
      "done for ticker FITB index # 279\n",
      "done for ticker ARE index # 280\n",
      "done for ticker URI index # 281\n",
      "done for ticker ZBH index # 282\n",
      "done for ticker K index # 283\n",
      "done for ticker LEN index # 284\n",
      "done for ticker EBAY index # 285\n",
      "done for ticker EIX index # 286\n",
      "done for ticker CBRE index # 287\n",
      "done for ticker EFX index # 288\n",
      "done for ticker VMC index # 289\n",
      "done for ticker TSN index # 290\n",
      "done for ticker HIG index # 291\n",
      "done for ticker FTV index # 292\n",
      "done for ticker WY index # 293\n",
      "done for ticker EQR index # 294\n",
      "done for ticker AVB index # 295\n",
      "done for ticker MKC index # 296\n",
      "done for ticker ETR index # 297\n",
      "done for ticker LUV index # 298\n",
      "done for ticker ULTA index # 299\n",
      "done for ticker AEE index # 300\n",
      "done for ticker MLM index # 301\n",
      "done for ticker FE index # 302\n",
      "done for ticker PFG index # 303\n",
      "done for ticker FRC index # 304\n",
      "done for ticker DTE index # 305\n",
      "done for ticker DAL index # 306\n",
      "done for ticker HBAN index # 307\n",
      "done for ticker IR index # 308\n",
      "done for ticker CTRA index # 309\n",
      "done for ticker ANSS index # 310\n",
      "done for ticker ACGL index # 311\n",
      "done for ticker PPL index # 312\n",
      "done for ticker RF index # 313\n",
      "done for ticker VRSN index # 314\n",
      "done for ticker LH index # 315\n",
      "done for ticker EXR index # 316\n",
      "done for ticker PWR index # 317\n",
      "done for ticker CF index # 318\n",
      "done for ticker CAH index # 319\n",
      "done for ticker CFG index # 320\n",
      "done for ticker XYL index # 321\n",
      "done for ticker HPE index # 322\n",
      "done for ticker EPAM index # 323\n",
      "done for ticker DOV index # 324\n",
      "done for ticker WAT index # 325\n",
      "done for ticker WRB index # 326\n",
      "done for ticker TDY index # 327\n",
      "done for ticker PAYC index # 328\n",
      "done for ticker ROL index # 329\n",
      "done for ticker NTRS index # 330\n",
      "done for ticker MRO index # 331\n",
      "done for ticker CNP index # 332\n",
      "done for ticker INVH index # 333\n",
      "done for ticker CHD index # 334\n",
      "done for ticker AES index # 335\n",
      "done for ticker MOH index # 336\n",
      "done for ticker JBHT index # 337\n",
      "done for ticker MAA index # 338\n",
      "done for ticker BBY index # 339\n",
      "done for ticker CLX index # 340\n",
      "done for ticker HOLX index # 341\n",
      "done for ticker WAB index # 342\n",
      "done for ticker DRI index # 343\n",
      "done for ticker EXPD index # 344\n",
      "done for ticker STE index # 345\n",
      "done for ticker AMCR index # 346\n",
      "done for ticker VTR index # 347\n",
      "done for ticker IEX index # 348\n",
      "done for ticker CAG index # 349\n",
      "done for ticker CMS index # 350\n",
      "done for ticker KEY index # 351\n",
      "done for ticker MPWR index # 352\n",
      "done for ticker BALL index # 353\n",
      "done for ticker J index # 354\n",
      "done for ticker BR index # 355\n",
      "done for ticker GRMN index # 356\n",
      "done for ticker PKI index # 357\n",
      "done for ticker TTWO index # 358\n",
      "done for ticker INCY index # 359\n",
      "done for ticker FDS index # 360\n",
      "done for ticker MOS index # 361\n",
      "done for ticker SEDG index # 362\n",
      "done for ticker CINF index # 363\n",
      "done for ticker ABMD index # 364\n",
      "done for ticker DGX index # 365\n",
      "done for ticker WST index # 366\n",
      "done for ticker ATO index # 367\n",
      "done for ticker TRGP index # 368\n",
      "done for ticker BRO index # 369\n",
      "done for ticker SYF index # 370\n",
      "done for ticker FOX index # 371\n",
      "done for ticker FOXA index # 372\n",
      "done for ticker NTAP index # 373\n",
      "done for ticker FMC index # 374\n",
      "done for ticker EQT index # 375\n",
      "done for ticker OMC index # 376\n",
      "done for ticker SJM index # 377\n",
      "done for ticker LYV index # 378\n",
      "done for ticker CPB index # 379\n",
      "done for ticker HWM index # 380\n",
      "done for ticker CPRT index # 381\n",
      "done for ticker AVY index # 382\n",
      "done for ticker IRM index # 383\n",
      "done for ticker COO index # 384\n",
      "done for ticker ALGN index # 385\n",
      "done for ticker SWKS index # 386\n",
      "done for ticker EXPE index # 387\n",
      "done for ticker RCL index # 388\n",
      "done for ticker ETSY index # 389\n",
      "done for ticker APA index # 390\n",
      "done for ticker GEN index # 391\n",
      "done for ticker TXT index # 392\n",
      "done for ticker LDOS index # 393\n",
      "done for ticker LKQ index # 394\n",
      "done for ticker TER index # 395\n",
      "done for ticker PTC index # 396\n",
      "done for ticker TRMB index # 397\n",
      "done for ticker AKAM index # 398\n",
      "done for ticker NVR index # 399\n",
      "done for ticker UAL index # 400\n",
      "done for ticker LNT index # 401\n",
      "done for ticker FLT index # 402\n",
      "done for ticker KIM index # 403\n",
      "done for ticker ZBRA index # 404\n",
      "done for ticker TYL index # 405\n",
      "done for ticker DPZ index # 406\n",
      "done for ticker JKHY index # 407\n",
      "done for ticker MGM index # 408\n",
      "done for ticker ESS index # 409\n",
      "done for ticker L index # 410\n",
      "done for ticker PEAK index # 411\n",
      "done for ticker MTCH index # 412\n",
      "done for ticker NDSN index # 413\n",
      "done for ticker EVRG index # 414\n",
      "done for ticker IPG index # 415\n",
      "done for ticker BEN index # 416\n",
      "done for ticker CBOE index # 417\n",
      "done for ticker TECH index # 418\n",
      "done for ticker SIVB index # 419\n",
      "done for ticker VFC index # 420\n",
      "done for ticker IP index # 421\n",
      "done for ticker HST index # 422\n",
      "done for ticker UDR index # 423\n",
      "done for ticker POOL index # 424\n",
      "done for ticker RE index # 425\n",
      "done for ticker PARA index # 426\n",
      "done for ticker SNA index # 427\n",
      "done for ticker CPT index # 428\n",
      "done for ticker LW index # 429\n",
      "done for ticker PKG index # 430\n",
      "done for ticker CRL index # 431\n",
      "done for ticker SWK index # 432\n",
      "done for ticker BIO index # 433\n",
      "done for ticker WDC index # 434\n",
      "done for ticker CHRW index # 435\n",
      "done for ticker STX index # 436\n",
      "done for ticker MAS index # 437\n",
      "done for ticker GL index # 438\n",
      "done for ticker CE index # 439\n",
      "done for ticker REG index # 440\n",
      "done for ticker NI index # 441\n",
      "done for ticker BXP index # 442\n",
      "done for ticker HSIC index # 443\n",
      "done for ticker CCL index # 444\n",
      "done for ticker TFX index # 445\n",
      "done for ticker CZR index # 446\n",
      "done for ticker NWS index # 447\n",
      "done for ticker NWSA index # 448\n",
      "done for ticker KMX index # 449\n",
      "done for ticker EMN index # 450\n",
      "done for ticker JNPR index # 451\n",
      "done for ticker PHM index # 452\n",
      "done for ticker CDAY index # 453\n",
      "done for ticker ALLE index # 454\n",
      "done for ticker QRVO index # 455\n",
      "done for ticker BWA index # 456\n",
      "done for ticker NRG index # 457\n",
      "done for ticker MKTX index # 458\n",
      "done for ticker WRK index # 459\n",
      "done for ticker UHS index # 460\n",
      "done for ticker FFIV index # 461\n",
      "done for ticker AOS index # 462\n",
      "done for ticker CMA index # 463\n",
      "done for ticker AAL index # 464\n",
      "done for ticker BBWI index # 465\n",
      "done for ticker HII index # 466\n",
      "done for ticker AAP index # 467\n",
      "done for ticker TPR index # 468\n",
      "done for ticker FRT index # 469\n",
      "done for ticker IVZ index # 470\n",
      "done for ticker PNW index # 471\n",
      "done for ticker HAS index # 472\n",
      "done for ticker WYNN index # 473\n",
      "done for ticker FBHS index # 474\n",
      "done for ticker SBNY index # 475\n",
      "done for ticker DISH index # 476\n",
      "done for ticker RHI index # 477\n",
      "done for ticker WHR index # 478\n",
      "done for ticker ZION index # 479\n",
      "done for ticker CTLT index # 480\n",
      "done for ticker PNR index # 481\n",
      "done for ticker SEE index # 482\n",
      "done for ticker RL index # 483\n",
      "done for ticker NCLH index # 484\n",
      "done for ticker DXC index # 485\n",
      "done for ticker GNRC index # 486\n",
      "done for ticker AIZ index # 487\n",
      "done for ticker XRAY index # 488\n",
      "done for ticker LNC index # 489\n",
      "done for ticker DVA index # 490\n",
      "done for ticker MHK index # 491\n",
      "done for ticker LUMN index # 492\n",
      "done for ticker ALK index # 493\n",
      "done for ticker NWL index # 494\n",
      "done for ticker VNO index # 495\n",
      "done for ticker TAP index # 496\n"
     ]
    }
   ],
   "source": [
    "pca_dict_score = {}\n",
    "tickers = utils.return_tickers()\n",
    "for ticker in tickers:\n",
    "        dict_pca_score[ticker] = cross_validate_ml(prep.ready_to_train_df(ticker), pipe)\n",
    "        print(f\"done for ticker {ticker} index # {tickers.index(ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d26574e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a4a3fe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:26:43.265897Z",
     "start_time": "2022-12-06T11:06:29.287564Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker AAPL index # 0\n",
      "done for ticker MSFT index # 1\n",
      "done for ticker GOOG index # 2\n",
      "done for ticker AMZN index # 3\n",
      "done for ticker TSLA index # 4\n",
      "done for ticker UNH index # 5\n",
      "done for ticker XOM index # 6\n",
      "done for ticker JNJ index # 7\n",
      "done for ticker WMT index # 8\n",
      "done for ticker NVDA index # 9\n",
      "done for ticker JPM index # 10\n",
      "done for ticker V index # 11\n",
      "done for ticker CVX index # 12\n",
      "done for ticker PG index # 13\n",
      "done for ticker LLY index # 14\n",
      "done for ticker MA index # 15\n",
      "done for ticker HD index # 16\n",
      "done for ticker META index # 17\n",
      "done for ticker BAC index # 18\n",
      "done for ticker ABBV index # 19\n",
      "done for ticker PFE index # 20\n",
      "done for ticker KO index # 21\n",
      "done for ticker MRK index # 22\n",
      "done for ticker PEP index # 23\n",
      "done for ticker COST index # 24\n",
      "done for ticker ORCL index # 25\n",
      "done for ticker AVGO index # 26\n",
      "done for ticker TMO index # 27\n",
      "done for ticker MCD index # 28\n",
      "done for ticker CSCO index # 29\n",
      "done for ticker ACN index # 30\n",
      "done for ticker DHR index # 31\n",
      "done for ticker TMUS index # 32\n",
      "done for ticker ABT index # 33\n",
      "done for ticker WFC index # 34\n",
      "done for ticker DIS index # 35\n",
      "done for ticker LIN index # 36\n",
      "done for ticker NEE index # 37\n",
      "done for ticker BMY index # 38\n",
      "done for ticker NKE index # 39\n",
      "done for ticker VZ index # 40\n",
      "done for ticker TXN index # 41\n",
      "done for ticker UPS index # 42\n",
      "done for ticker COP index # 43\n",
      "done for ticker ADBE index # 44\n",
      "done for ticker CMCSA index # 45\n",
      "done for ticker CRM index # 46\n",
      "done for ticker PM index # 47\n",
      "done for ticker MS index # 48\n",
      "done for ticker AMGN index # 49\n",
      "done for ticker SCHW index # 50\n",
      "done for ticker HON index # 51\n",
      "done for ticker RTX index # 52\n",
      "done for ticker QCOM index # 53\n",
      "done for ticker T index # 54\n",
      "done for ticker IBM index # 55\n",
      "done for ticker DE index # 56\n",
      "done for ticker CVS index # 57\n",
      "done for ticker LOW index # 58\n",
      "done for ticker GS index # 59\n",
      "done for ticker UNP index # 60\n",
      "done for ticker NFLX index # 61\n",
      "done for ticker LMT index # 62\n",
      "done for ticker CAT index # 63\n",
      "done for ticker AMD index # 64\n",
      "done for ticker INTC index # 65\n",
      "done for ticker ELV index # 66\n",
      "done for ticker SPGI index # 67\n",
      "done for ticker AXP index # 68\n",
      "done for ticker SBUX index # 69\n",
      "done for ticker INTU index # 70\n",
      "done for ticker BLK index # 71\n",
      "done for ticker ADP index # 72\n",
      "done for ticker GILD index # 73\n",
      "done for ticker PLD index # 74\n",
      "done for ticker MDT index # 75\n",
      "done for ticker BA index # 76\n",
      "done for ticker AMT index # 77\n",
      "done for ticker CI index # 78\n",
      "done for ticker GE index # 79\n",
      "done for ticker TJX index # 80\n",
      "done for ticker ISRG index # 81\n",
      "done for ticker C index # 82\n",
      "done for ticker AMAT index # 83\n",
      "done for ticker PYPL index # 84\n",
      "done for ticker MDLZ index # 85\n",
      "done for ticker CB index # 86\n",
      "done for ticker SYK index # 87\n",
      "done for ticker ADI index # 88\n",
      "done for ticker MMC index # 89\n",
      "done for ticker EOG index # 90\n",
      "done for ticker NOW index # 91\n",
      "done for ticker VRTX index # 92\n",
      "done for ticker MO index # 93\n",
      "done for ticker NOC index # 94\n",
      "done for ticker EL index # 95\n",
      "done for ticker REGN index # 96\n",
      "done for ticker PGR index # 97\n",
      "done for ticker BKNG index # 98\n",
      "done for ticker DUK index # 99\n",
      "done for ticker TGT index # 100\n",
      "done for ticker SLB index # 101\n",
      "done for ticker SO index # 102\n",
      "done for ticker MMM index # 103\n",
      "done for ticker ITW index # 104\n",
      "done for ticker ZTS index # 105\n",
      "done for ticker GD index # 106\n",
      "done for ticker APD index # 107\n",
      "done for ticker HUM index # 108\n",
      "done for ticker MRNA index # 109\n",
      "done for ticker BDX index # 110\n",
      "done for ticker CSX index # 111\n",
      "done for ticker WM index # 112\n",
      "done for ticker PNC index # 113\n",
      "done for ticker HCA index # 114\n",
      "done for ticker ETN index # 115\n",
      "done for ticker USB index # 116\n",
      "done for ticker FISV index # 117\n",
      "done for ticker SHW index # 118\n",
      "done for ticker OXY index # 119\n",
      "done for ticker CL index # 120\n",
      "done for ticker MU index # 121\n",
      "done for ticker CME index # 122\n",
      "done for ticker AON index # 123\n",
      "done for ticker LRCX index # 124\n",
      "done for ticker BSX index # 125\n",
      "done for ticker EQIX index # 126\n",
      "done for ticker TFC index # 127\n",
      "done for ticker PXD index # 128\n",
      "done for ticker CHTR index # 129\n",
      "done for ticker CCI index # 130\n",
      "done for ticker MET index # 131\n",
      "done for ticker ATVI index # 132\n",
      "done for ticker ICE index # 133\n",
      "done for ticker MPC index # 134\n",
      "done for ticker NSC index # 135\n",
      "done for ticker DG index # 136\n",
      "done for ticker GM index # 137\n",
      "done for ticker EMR index # 138\n",
      "done for ticker F index # 139\n",
      "done for ticker KLAC index # 140\n",
      "done for ticker MCO index # 141\n",
      "done for ticker FCX index # 142\n",
      "done for ticker KDP index # 143\n",
      "done for ticker MNST index # 144\n",
      "done for ticker MCK index # 145\n",
      "done for ticker VLO index # 146\n",
      "done for ticker ORLY index # 147\n",
      "done for ticker ADM index # 148\n",
      "done for ticker PSX index # 149\n",
      "done for ticker PSA index # 150\n",
      "done for ticker SRE index # 151\n",
      "done for ticker SNPS index # 152\n",
      "done for ticker MAR index # 153\n",
      "done for ticker D index # 154\n",
      "done for ticker GIS index # 155\n",
      "done for ticker AEP index # 156\n",
      "done for ticker AZO index # 157\n",
      "done for ticker KHC index # 158\n",
      "done for ticker APH index # 159\n",
      "done for ticker HSY index # 160\n",
      "done for ticker CNC index # 161\n",
      "done for ticker CTVA index # 162\n",
      "done for ticker EW index # 163\n",
      "done for ticker CTAS index # 164\n",
      "done for ticker A index # 165\n",
      "done for ticker ROP index # 166\n",
      "done for ticker JCI index # 167\n",
      "done for ticker CDNS index # 168\n",
      "done for ticker FDX index # 169\n",
      "done for ticker NXPI index # 170\n",
      "done for ticker AIG index # 171\n",
      "done for ticker KMB index # 172\n",
      "done for ticker AFL index # 173\n",
      "done for ticker HES index # 174\n",
      "done for ticker MSI index # 175\n",
      "done for ticker PAYX index # 176\n",
      "done for ticker DVN index # 177\n",
      "done for ticker TRV index # 178\n",
      "done for ticker BIIB index # 179\n",
      "done for ticker DXCM index # 180\n",
      "done for ticker SYY index # 181\n",
      "done for ticker LHX index # 182\n",
      "done for ticker RSG index # 183\n",
      "done for ticker ENPH index # 184\n",
      "done for ticker ECL index # 185\n",
      "done for ticker ADSK index # 186\n",
      "done for ticker MCHP index # 187\n",
      "done for ticker ANET index # 188\n",
      "done for ticker KMI index # 189\n",
      "done for ticker CMG index # 190\n",
      "done for ticker FTNT index # 191\n",
      "done for ticker AJG index # 192\n",
      "done for ticker STZ index # 193\n",
      "done for ticker TT index # 194\n",
      "done for ticker WMB index # 195\n",
      "done for ticker MSCI index # 196\n",
      "done for ticker O index # 197\n",
      "done for ticker IQV index # 198\n",
      "done for ticker TEL index # 199\n",
      "done for ticker ROST index # 200\n",
      "done for ticker PRU index # 201\n",
      "done for ticker EXC index # 202\n",
      "done for ticker PH index # 203\n",
      "done for ticker FIS index # 204\n",
      "done for ticker SPG index # 205\n",
      "done for ticker COF index # 206\n",
      "done for ticker NUE index # 207\n",
      "done for ticker XEL index # 208\n",
      "done for ticker HLT index # 209\n",
      "done for ticker CARR index # 210\n",
      "done for ticker PCAR index # 211\n",
      "done for ticker BK index # 212\n",
      "done for ticker NEM index # 213\n",
      "done for ticker DOW index # 214\n",
      "done for ticker EA index # 215\n",
      "done for ticker WBA index # 216\n",
      "done for ticker DD index # 217\n",
      "done for ticker ALL index # 218\n",
      "done for ticker YUM index # 219\n",
      "done for ticker AMP index # 220\n",
      "done for ticker CMI index # 221\n",
      "done for ticker ILMN index # 222\n",
      "done for ticker TDG index # 223\n",
      "done for ticker IDXX index # 224\n",
      "done for ticker ED index # 225\n",
      "done for ticker KR index # 226\n",
      "done for ticker ABC index # 227\n",
      "done for ticker DLTR index # 228\n",
      "done for ticker RMD index # 229\n",
      "done for ticker ALB index # 230\n",
      "done for ticker HAL index # 231\n",
      "done for ticker NDAQ index # 232\n",
      "done for ticker LVS index # 233\n",
      "done for ticker ODFL index # 234\n",
      "done for ticker WELL index # 235\n",
      "done for ticker AME index # 236\n",
      "done for ticker CSGP index # 237\n",
      "done for ticker OTIS index # 238\n",
      "done for ticker MTD index # 239\n",
      "done for ticker SBAC index # 240\n",
      "done for ticker ON index # 241\n",
      "done for ticker VICI index # 242\n",
      "done for ticker DLR index # 243\n",
      "done for ticker KEYS index # 244\n",
      "done for ticker PPG index # 245\n",
      "done for ticker WEC index # 246\n",
      "done for ticker CTSH index # 247\n",
      "done for ticker ROK index # 248\n",
      "done for ticker GWW index # 249\n",
      "done for ticker PCG index # 250\n",
      "done for ticker HPQ index # 251\n",
      "done for ticker FAST index # 252\n",
      "done for ticker DFS index # 253\n",
      "done for ticker MTB index # 254\n",
      "done for ticker PEG index # 255\n",
      "done for ticker OKE index # 256\n",
      "done for ticker DHI index # 257\n",
      "done for ticker APTV index # 258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker BKR index # 259\n",
      "done for ticker GLW index # 260\n",
      "done for ticker LYB index # 261\n",
      "done for ticker ES index # 262\n",
      "done for ticker BAX index # 263\n",
      "done for ticker STT index # 264\n",
      "done for ticker VRSK index # 265\n",
      "done for ticker TROW index # 266\n",
      "done for ticker WBD index # 267\n",
      "done for ticker AWK index # 268\n",
      "done for ticker IT index # 269\n",
      "done for ticker GPN index # 270\n",
      "done for ticker HRL index # 271\n",
      "done for ticker FANG index # 272\n",
      "done for ticker WTW index # 273\n",
      "done for ticker RJF index # 274\n",
      "done for ticker GPC index # 275\n",
      "done for ticker IFF index # 276\n",
      "done for ticker CDW index # 277\n",
      "done for ticker TSCO index # 278\n",
      "done for ticker FITB index # 279\n",
      "done for ticker ARE index # 280\n",
      "done for ticker URI index # 281\n",
      "done for ticker ZBH index # 282\n",
      "done for ticker K index # 283\n",
      "done for ticker LEN index # 284\n",
      "done for ticker EBAY index # 285\n",
      "done for ticker EIX index # 286\n",
      "done for ticker CBRE index # 287\n",
      "done for ticker EFX index # 288\n",
      "done for ticker VMC index # 289\n",
      "done for ticker TSN index # 290\n",
      "done for ticker HIG index # 291\n",
      "done for ticker FTV index # 292\n",
      "done for ticker WY index # 293\n",
      "done for ticker EQR index # 294\n",
      "done for ticker AVB index # 295\n",
      "done for ticker MKC index # 296\n",
      "done for ticker ETR index # 297\n",
      "done for ticker LUV index # 298\n",
      "done for ticker ULTA index # 299\n",
      "done for ticker AEE index # 300\n",
      "done for ticker MLM index # 301\n",
      "done for ticker FE index # 302\n",
      "done for ticker PFG index # 303\n",
      "done for ticker FRC index # 304\n",
      "done for ticker DTE index # 305\n",
      "done for ticker DAL index # 306\n",
      "done for ticker HBAN index # 307\n",
      "done for ticker IR index # 308\n",
      "done for ticker CTRA index # 309\n",
      "done for ticker ANSS index # 310\n",
      "done for ticker ACGL index # 311\n",
      "done for ticker PPL index # 312\n",
      "done for ticker RF index # 313\n",
      "done for ticker VRSN index # 314\n",
      "done for ticker LH index # 315\n",
      "done for ticker EXR index # 316\n",
      "done for ticker PWR index # 317\n",
      "done for ticker CF index # 318\n",
      "done for ticker CAH index # 319\n",
      "done for ticker CFG index # 320\n",
      "done for ticker XYL index # 321\n",
      "done for ticker HPE index # 322\n",
      "done for ticker EPAM index # 323\n",
      "done for ticker DOV index # 324\n",
      "done for ticker WAT index # 325\n",
      "done for ticker WRB index # 326\n",
      "done for ticker TDY index # 327\n",
      "done for ticker PAYC index # 328\n",
      "done for ticker ROL index # 329\n",
      "done for ticker NTRS index # 330\n",
      "done for ticker MRO index # 331\n",
      "done for ticker CNP index # 332\n",
      "done for ticker INVH index # 333\n",
      "done for ticker CHD index # 334\n",
      "done for ticker AES index # 335\n",
      "done for ticker MOH index # 336\n",
      "done for ticker JBHT index # 337\n",
      "done for ticker MAA index # 338\n",
      "done for ticker BBY index # 339\n",
      "done for ticker CLX index # 340\n",
      "done for ticker HOLX index # 341\n",
      "done for ticker WAB index # 342\n",
      "done for ticker DRI index # 343\n",
      "done for ticker EXPD index # 344\n",
      "done for ticker STE index # 345\n",
      "done for ticker AMCR index # 346\n",
      "done for ticker VTR index # 347\n",
      "done for ticker IEX index # 348\n",
      "done for ticker CAG index # 349\n",
      "done for ticker CMS index # 350\n",
      "done for ticker KEY index # 351\n",
      "done for ticker MPWR index # 352\n",
      "done for ticker BALL index # 353\n",
      "done for ticker J index # 354\n",
      "done for ticker BR index # 355\n",
      "done for ticker GRMN index # 356\n",
      "done for ticker PKI index # 357\n",
      "done for ticker TTWO index # 358\n",
      "done for ticker INCY index # 359\n",
      "done for ticker FDS index # 360\n",
      "done for ticker MOS index # 361\n",
      "done for ticker SEDG index # 362\n",
      "done for ticker CINF index # 363\n",
      "done for ticker ABMD index # 364\n",
      "done for ticker DGX index # 365\n",
      "done for ticker WST index # 366\n",
      "done for ticker ATO index # 367\n",
      "done for ticker TRGP index # 368\n",
      "done for ticker BRO index # 369\n",
      "done for ticker SYF index # 370\n",
      "done for ticker FOX index # 371\n",
      "done for ticker FOXA index # 372\n",
      "done for ticker NTAP index # 373\n",
      "done for ticker FMC index # 374\n",
      "done for ticker EQT index # 375\n",
      "done for ticker OMC index # 376\n",
      "done for ticker SJM index # 377\n",
      "done for ticker LYV index # 378\n",
      "done for ticker CPB index # 379\n",
      "done for ticker HWM index # 380\n",
      "done for ticker CPRT index # 381\n",
      "done for ticker AVY index # 382\n",
      "done for ticker IRM index # 383\n",
      "done for ticker COO index # 384\n",
      "done for ticker ALGN index # 385\n",
      "done for ticker SWKS index # 386\n",
      "done for ticker EXPE index # 387\n",
      "done for ticker RCL index # 388\n",
      "done for ticker ETSY index # 389\n",
      "done for ticker APA index # 390\n",
      "done for ticker GEN index # 391\n",
      "done for ticker TXT index # 392\n",
      "done for ticker LDOS index # 393\n",
      "done for ticker LKQ index # 394\n",
      "done for ticker TER index # 395\n",
      "done for ticker PTC index # 396\n",
      "done for ticker TRMB index # 397\n",
      "done for ticker AKAM index # 398\n",
      "done for ticker NVR index # 399\n",
      "done for ticker UAL index # 400\n",
      "done for ticker LNT index # 401\n",
      "done for ticker FLT index # 402\n",
      "done for ticker KIM index # 403\n",
      "done for ticker ZBRA index # 404\n",
      "done for ticker TYL index # 405\n",
      "done for ticker DPZ index # 406\n",
      "done for ticker JKHY index # 407\n",
      "done for ticker MGM index # 408\n",
      "done for ticker ESS index # 409\n",
      "done for ticker L index # 410\n",
      "done for ticker PEAK index # 411\n",
      "done for ticker MTCH index # 412\n",
      "done for ticker NDSN index # 413\n",
      "done for ticker EVRG index # 414\n",
      "done for ticker IPG index # 415\n",
      "done for ticker BEN index # 416\n",
      "done for ticker CBOE index # 417\n",
      "done for ticker TECH index # 418\n",
      "done for ticker SIVB index # 419\n",
      "done for ticker VFC index # 420\n",
      "done for ticker IP index # 421\n",
      "done for ticker HST index # 422\n",
      "done for ticker UDR index # 423\n",
      "done for ticker POOL index # 424\n",
      "done for ticker RE index # 425\n",
      "done for ticker PARA index # 426\n",
      "done for ticker SNA index # 427\n",
      "done for ticker CPT index # 428\n",
      "done for ticker LW index # 429\n",
      "done for ticker PKG index # 430\n",
      "done for ticker CRL index # 431\n",
      "done for ticker SWK index # 432\n",
      "done for ticker BIO index # 433\n",
      "done for ticker WDC index # 434\n",
      "done for ticker CHRW index # 435\n",
      "done for ticker STX index # 436\n",
      "done for ticker MAS index # 437\n",
      "done for ticker GL index # 438\n",
      "done for ticker CE index # 439\n",
      "done for ticker REG index # 440\n",
      "done for ticker NI index # 441\n",
      "done for ticker BXP index # 442\n",
      "done for ticker HSIC index # 443\n",
      "done for ticker CCL index # 444\n",
      "done for ticker TFX index # 445\n",
      "done for ticker CZR index # 446\n",
      "done for ticker NWS index # 447\n",
      "done for ticker NWSA index # 448\n",
      "done for ticker KMX index # 449\n",
      "done for ticker EMN index # 450\n",
      "done for ticker JNPR index # 451\n",
      "done for ticker PHM index # 452\n",
      "done for ticker CDAY index # 453\n",
      "done for ticker ALLE index # 454\n",
      "done for ticker QRVO index # 455\n",
      "done for ticker BWA index # 456\n",
      "done for ticker NRG index # 457\n",
      "done for ticker MKTX index # 458\n",
      "done for ticker WRK index # 459\n",
      "done for ticker UHS index # 460\n",
      "done for ticker FFIV index # 461\n",
      "done for ticker AOS index # 462\n",
      "done for ticker CMA index # 463\n",
      "done for ticker AAL index # 464\n",
      "done for ticker BBWI index # 465\n",
      "done for ticker HII index # 466\n",
      "done for ticker AAP index # 467\n",
      "done for ticker TPR index # 468\n",
      "done for ticker FRT index # 469\n",
      "done for ticker IVZ index # 470\n",
      "done for ticker PNW index # 471\n",
      "done for ticker HAS index # 472\n",
      "done for ticker WYNN index # 473\n",
      "done for ticker FBHS index # 474\n",
      "done for ticker SBNY index # 475\n",
      "done for ticker DISH index # 476\n",
      "done for ticker RHI index # 477\n",
      "done for ticker WHR index # 478\n",
      "done for ticker ZION index # 479\n",
      "done for ticker CTLT index # 480\n",
      "done for ticker PNR index # 481\n",
      "done for ticker SEE index # 482\n",
      "done for ticker RL index # 483\n",
      "done for ticker NCLH index # 484\n",
      "done for ticker DXC index # 485\n",
      "done for ticker GNRC index # 486\n",
      "done for ticker AIZ index # 487\n",
      "done for ticker XRAY index # 488\n",
      "done for ticker LNC index # 489\n",
      "done for ticker DVA index # 490\n",
      "done for ticker MHK index # 491\n",
      "done for ticker LUMN index # 492\n",
      "done for ticker ALK index # 493\n",
      "done for ticker NWL index # 494\n",
      "done for ticker VNO index # 495\n",
      "done for ticker TAP index # 496\n"
     ]
    }
   ],
   "source": [
    "pca_dict_score = {}\n",
    "tickers = utils.return_tickers()\n",
    "for ticker in tickers:\n",
    "    pca = PCA(n_components=0.9)\n",
    "    ticker_df = prep.ready_to_train_df(ticker)\n",
    "    pca.fit(ticker_df)\n",
    "    n_pcs= pca.n_components_\n",
    "    most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "    initial_feature_names = ticker_df.columns\n",
    "    most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "    pca_df = ticker_df[list(np.unique(most_important_names))] \n",
    "    returns = ticker_df[['return']]\n",
    "    final_pca = pd.merge(pca_df, returns, how='outer', left_index=True, right_index=True)\n",
    "    pca_dict_score[ticker] = cross_validate_ml(final_pca, GradientBoostingRegressor(max_depth = 3, \n",
    "                                                                                    criterion='friedman_mse',\n",
    "                                                                                    learning_rate=0.08,\n",
    "                                                                                    n_estimators = 100))\n",
    "    print(f\"done for ticker {ticker} index # {tickers.index(ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c7fe3ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:31:28.459421Z",
     "start_time": "2022-12-06T11:31:28.426417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0033498792960944923 0.02096529727516623\n"
     ]
    }
   ],
   "source": [
    "rmse_pca= []\n",
    "baseline = []\n",
    "for key in pca_dict_score.keys():\n",
    "    rmse_pca.append(pca_dict_score[key][0])\n",
    "    baseline.append(pca_dict_score[key][1])\n",
    "print(np.mean(rmse_pca), np.mean(baseline) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c29d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:44:19.570067Z",
     "start_time": "2022-12-02T08:44:19.401956Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f203d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T08:44:22.236582Z",
     "start_time": "2022-12-02T08:44:22.123704Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.ylim(ymin=0)\n",
    "plt.title('cumulated share of explained variance')\n",
    "plt.xlabel('# of principal component used');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559acad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T08:21:43.359186Z",
     "start_time": "2022-12-06T08:21:42.741327Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dynamic_portfolio.preprocess as prep\n",
    "import dynamic_portfolio.utils as utils\n",
    "import dynamic_portfolio.cross_validate as cv\n",
    "import dynamic_portfolio.models as model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import warnings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings(action='ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f49016",
   "metadata": {},
   "source": [
    "### Cross validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97e3cc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T08:21:49.050523Z",
     "start_time": "2022-12-06T08:21:49.017552Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "cross_val = {\n",
    "    'fold_length' : 252, # Working days for 1 year\n",
    "    'fold_stride' : 60, # Step between folds, here one quarter\n",
    "    'train_test_ratio' : 0.7, # Split in fold\n",
    "    'input_length' : 0, # Number of days to move back from last train_index, here 0\n",
    "    'horizon' : 1, # Number of days ahead to make prediction, here 1\n",
    "    'output_length' : 1, # Number of targets wanted\n",
    "}\n",
    "#Split the dataset by FOLDS\n",
    "def get_folds(\n",
    "    df: pd.DataFrame,\n",
    "    fold_length = cross_val['fold_length'],\n",
    "    fold_stride = cross_val['fold_stride']):\n",
    "    '''\n",
    "    This function slides through the Time Series dataframe of shape (n_timesteps, n_features) to create folds\n",
    "    - of equal `fold_length`\n",
    "    - using `fold_stride` between each fold\n",
    "    Returns a list of folds, each as a DataFrame\n",
    "    '''\n",
    "    folds = []\n",
    "    for idx in range(0, len(df), fold_stride):\n",
    "        # Exits the loop as soon as the last fold index would exceed the last index\n",
    "        if (idx + fold_length) > len(df):\n",
    "            break\n",
    "        fold = df.iloc[idx:idx + fold_length, :]\n",
    "        folds.append(fold)\n",
    "    return folds\n",
    "#Split FOLDS by Train et Test\n",
    "#### FOR ONE FOLD !!!!!\n",
    "def train_test_split(fold: pd.DataFrame,\n",
    "                     train_test_ratio = cross_val['train_test_ratio'],\n",
    "                     input_length = cross_val['input_length']):\n",
    "    '''\n",
    "    Returns a train dataframe and a test dataframe (fold_train, fold_test)\n",
    "    from which one can sample (X,y) sequences.\n",
    "    df_train should contain all the timesteps until round(train_test_ratio * len(fold))\n",
    "    '''\n",
    "    # TRAIN SET\n",
    "    # ======================\n",
    "    last_train_idx = round(train_test_ratio * len(fold))\n",
    "    fold_train = fold.iloc[0:last_train_idx, :]\n",
    "    # TEST SET\n",
    "    # ======================\n",
    "    first_test_idx = last_train_idx - input_length \n",
    "    fold_test = fold.iloc[first_test_idx:, :]\n",
    "    return (fold_train, fold_test)\n",
    "def cross_validate_ml(df, model) :\n",
    "    '''\n",
    "    get_folds() create many FOLDS, train_test_split() create a split on ONE FOLDS.\n",
    "    The goal of this function is to make splits and sequences on each FOLDS.\n",
    "    Then, apply a model.\n",
    "    '''\n",
    "    folds = get_folds(df, fold_length = cross_val['fold_length'], fold_stride = cross_val['fold_stride']) # 1 - Creating FOLDS\n",
    "    scores =[]\n",
    "    baseline = []\n",
    "    for fold in folds:\n",
    "        # 2 - CHRONOLOGICAL TRAIN TEST SPLIT of the current FOLD\n",
    "        (fold_train, fold_test) = train_test_split(fold = fold,\n",
    "                                                train_test_ratio = cross_val['train_test_ratio'],\n",
    "                                                input_length = cross_val['input_length'] ,\n",
    "                                                )\n",
    "        # 3 - Scanninng fold_train and fold_test for SEQUENCES\n",
    "        X_train, y_train = fold_train, fold_train['return']\n",
    "        X_test, y_test = fold_test, fold_test['return']\n",
    "        model.fit(X_train, y_train)\n",
    "        rmse_model = (mean_squared_error(y_test, model.predict(X_test)))**0.5\n",
    "        scores.append(rmse_model)\n",
    "        rmse_baseline = mean_squared_error(y_test.iloc[[0]], y_train.iloc[[-1]])**0.5\n",
    "        baseline.append(rmse_baseline)\n",
    "    return np.mean(scores), np.mean(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8ea4ac",
   "metadata": {},
   "source": [
    "### Script to run for model on all stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8222d",
   "metadata": {},
   "source": [
    "### Models used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de5782db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:44:15.680313Z",
     "start_time": "2022-12-06T09:44:15.669340Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5aaa731a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:30:38.946985Z",
     "start_time": "2022-12-06T09:46:44.058925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker AAPL index # 0\n",
      "done for ticker MSFT index # 1\n",
      "done for ticker GOOG index # 2\n",
      "done for ticker AMZN index # 3\n",
      "done for ticker TSLA index # 4\n",
      "done for ticker UNH index # 5\n",
      "done for ticker XOM index # 6\n",
      "done for ticker JNJ index # 7\n",
      "done for ticker WMT index # 8\n",
      "done for ticker NVDA index # 9\n",
      "done for ticker JPM index # 10\n",
      "done for ticker V index # 11\n",
      "done for ticker CVX index # 12\n",
      "done for ticker PG index # 13\n",
      "done for ticker LLY index # 14\n",
      "done for ticker MA index # 15\n",
      "done for ticker HD index # 16\n",
      "done for ticker META index # 17\n",
      "done for ticker BAC index # 18\n",
      "done for ticker ABBV index # 19\n",
      "done for ticker PFE index # 20\n",
      "done for ticker KO index # 21\n",
      "done for ticker MRK index # 22\n",
      "done for ticker PEP index # 23\n",
      "done for ticker COST index # 24\n",
      "done for ticker ORCL index # 25\n",
      "done for ticker AVGO index # 26\n",
      "done for ticker TMO index # 27\n",
      "done for ticker MCD index # 28\n",
      "done for ticker CSCO index # 29\n",
      "done for ticker ACN index # 30\n",
      "done for ticker DHR index # 31\n",
      "done for ticker TMUS index # 32\n",
      "done for ticker ABT index # 33\n",
      "done for ticker WFC index # 34\n",
      "done for ticker DIS index # 35\n",
      "done for ticker LIN index # 36\n",
      "done for ticker NEE index # 37\n",
      "done for ticker BMY index # 38\n",
      "done for ticker NKE index # 39\n",
      "done for ticker VZ index # 40\n",
      "done for ticker TXN index # 41\n",
      "done for ticker UPS index # 42\n",
      "done for ticker COP index # 43\n",
      "done for ticker ADBE index # 44\n",
      "done for ticker CMCSA index # 45\n",
      "done for ticker CRM index # 46\n",
      "done for ticker PM index # 47\n",
      "done for ticker MS index # 48\n",
      "done for ticker AMGN index # 49\n",
      "done for ticker SCHW index # 50\n",
      "done for ticker HON index # 51\n",
      "done for ticker RTX index # 52\n",
      "done for ticker QCOM index # 53\n",
      "done for ticker T index # 54\n",
      "done for ticker IBM index # 55\n",
      "done for ticker DE index # 56\n",
      "done for ticker CVS index # 57\n",
      "done for ticker LOW index # 58\n",
      "done for ticker GS index # 59\n",
      "done for ticker UNP index # 60\n",
      "done for ticker NFLX index # 61\n",
      "done for ticker LMT index # 62\n",
      "done for ticker CAT index # 63\n",
      "done for ticker AMD index # 64\n",
      "done for ticker INTC index # 65\n",
      "done for ticker ELV index # 66\n",
      "done for ticker SPGI index # 67\n",
      "done for ticker AXP index # 68\n",
      "done for ticker SBUX index # 69\n",
      "done for ticker INTU index # 70\n",
      "done for ticker BLK index # 71\n",
      "done for ticker ADP index # 72\n",
      "done for ticker GILD index # 73\n",
      "done for ticker PLD index # 74\n",
      "done for ticker MDT index # 75\n",
      "done for ticker BA index # 76\n",
      "done for ticker AMT index # 77\n",
      "done for ticker CI index # 78\n",
      "done for ticker GE index # 79\n",
      "done for ticker TJX index # 80\n",
      "done for ticker ISRG index # 81\n",
      "done for ticker C index # 82\n",
      "done for ticker AMAT index # 83\n",
      "done for ticker PYPL index # 84\n",
      "done for ticker MDLZ index # 85\n",
      "done for ticker CB index # 86\n",
      "done for ticker SYK index # 87\n",
      "done for ticker ADI index # 88\n",
      "done for ticker MMC index # 89\n",
      "done for ticker EOG index # 90\n",
      "done for ticker NOW index # 91\n",
      "done for ticker VRTX index # 92\n",
      "done for ticker MO index # 93\n",
      "done for ticker NOC index # 94\n",
      "done for ticker EL index # 95\n",
      "done for ticker REGN index # 96\n",
      "done for ticker PGR index # 97\n",
      "done for ticker BKNG index # 98\n",
      "done for ticker DUK index # 99\n",
      "done for ticker TGT index # 100\n",
      "done for ticker SLB index # 101\n",
      "done for ticker SO index # 102\n",
      "done for ticker MMM index # 103\n",
      "done for ticker ITW index # 104\n",
      "done for ticker ZTS index # 105\n",
      "done for ticker GD index # 106\n",
      "done for ticker APD index # 107\n",
      "done for ticker HUM index # 108\n",
      "done for ticker MRNA index # 109\n",
      "done for ticker BDX index # 110\n",
      "done for ticker CSX index # 111\n",
      "done for ticker WM index # 112\n",
      "done for ticker PNC index # 113\n",
      "done for ticker HCA index # 114\n",
      "done for ticker ETN index # 115\n",
      "done for ticker USB index # 116\n",
      "done for ticker FISV index # 117\n",
      "done for ticker SHW index # 118\n",
      "done for ticker OXY index # 119\n",
      "done for ticker CL index # 120\n",
      "done for ticker MU index # 121\n",
      "done for ticker CME index # 122\n",
      "done for ticker AON index # 123\n",
      "done for ticker LRCX index # 124\n",
      "done for ticker BSX index # 125\n",
      "done for ticker EQIX index # 126\n",
      "done for ticker TFC index # 127\n",
      "done for ticker PXD index # 128\n",
      "done for ticker CHTR index # 129\n",
      "done for ticker CCI index # 130\n",
      "done for ticker MET index # 131\n",
      "done for ticker ATVI index # 132\n",
      "done for ticker ICE index # 133\n",
      "done for ticker MPC index # 134\n",
      "done for ticker NSC index # 135\n",
      "done for ticker DG index # 136\n",
      "done for ticker GM index # 137\n",
      "done for ticker EMR index # 138\n",
      "done for ticker F index # 139\n",
      "done for ticker KLAC index # 140\n",
      "done for ticker MCO index # 141\n",
      "done for ticker FCX index # 142\n",
      "done for ticker KDP index # 143\n",
      "done for ticker MNST index # 144\n",
      "done for ticker MCK index # 145\n",
      "done for ticker VLO index # 146\n",
      "done for ticker ORLY index # 147\n",
      "done for ticker ADM index # 148\n",
      "done for ticker PSX index # 149\n",
      "done for ticker PSA index # 150\n",
      "done for ticker SRE index # 151\n",
      "done for ticker SNPS index # 152\n",
      "done for ticker MAR index # 153\n",
      "done for ticker D index # 154\n",
      "done for ticker GIS index # 155\n",
      "done for ticker AEP index # 156\n",
      "done for ticker AZO index # 157\n",
      "done for ticker KHC index # 158\n",
      "done for ticker APH index # 159\n",
      "done for ticker HSY index # 160\n",
      "done for ticker CNC index # 161\n",
      "done for ticker CTVA index # 162\n",
      "done for ticker EW index # 163\n",
      "done for ticker CTAS index # 164\n",
      "done for ticker A index # 165\n",
      "done for ticker ROP index # 166\n",
      "done for ticker JCI index # 167\n",
      "done for ticker CDNS index # 168\n",
      "done for ticker FDX index # 169\n",
      "done for ticker NXPI index # 170\n",
      "done for ticker AIG index # 171\n",
      "done for ticker KMB index # 172\n",
      "done for ticker AFL index # 173\n",
      "done for ticker HES index # 174\n",
      "done for ticker MSI index # 175\n",
      "done for ticker PAYX index # 176\n",
      "done for ticker DVN index # 177\n",
      "done for ticker TRV index # 178\n",
      "done for ticker BIIB index # 179\n",
      "done for ticker DXCM index # 180\n",
      "done for ticker SYY index # 181\n",
      "done for ticker LHX index # 182\n",
      "done for ticker RSG index # 183\n",
      "done for ticker ENPH index # 184\n",
      "done for ticker ECL index # 185\n",
      "done for ticker ADSK index # 186\n",
      "done for ticker MCHP index # 187\n",
      "done for ticker ANET index # 188\n",
      "done for ticker KMI index # 189\n",
      "done for ticker CMG index # 190\n",
      "done for ticker FTNT index # 191\n",
      "done for ticker AJG index # 192\n",
      "done for ticker STZ index # 193\n",
      "done for ticker TT index # 194\n",
      "done for ticker WMB index # 195\n",
      "done for ticker MSCI index # 196\n",
      "done for ticker O index # 197\n",
      "done for ticker IQV index # 198\n",
      "done for ticker TEL index # 199\n",
      "done for ticker ROST index # 200\n",
      "done for ticker PRU index # 201\n",
      "done for ticker EXC index # 202\n",
      "done for ticker PH index # 203\n",
      "done for ticker FIS index # 204\n",
      "done for ticker SPG index # 205\n",
      "done for ticker COF index # 206\n",
      "done for ticker NUE index # 207\n",
      "done for ticker XEL index # 208\n",
      "done for ticker HLT index # 209\n",
      "done for ticker CARR index # 210\n",
      "done for ticker PCAR index # 211\n",
      "done for ticker BK index # 212\n",
      "done for ticker NEM index # 213\n",
      "done for ticker DOW index # 214\n",
      "done for ticker EA index # 215\n",
      "done for ticker WBA index # 216\n",
      "done for ticker DD index # 217\n",
      "done for ticker ALL index # 218\n",
      "done for ticker YUM index # 219\n",
      "done for ticker AMP index # 220\n",
      "done for ticker CMI index # 221\n",
      "done for ticker ILMN index # 222\n",
      "done for ticker TDG index # 223\n",
      "done for ticker IDXX index # 224\n",
      "done for ticker ED index # 225\n",
      "done for ticker KR index # 226\n",
      "done for ticker ABC index # 227\n",
      "done for ticker DLTR index # 228\n",
      "done for ticker RMD index # 229\n",
      "done for ticker ALB index # 230\n",
      "done for ticker HAL index # 231\n",
      "done for ticker NDAQ index # 232\n",
      "done for ticker LVS index # 233\n",
      "done for ticker ODFL index # 234\n",
      "done for ticker WELL index # 235\n",
      "done for ticker AME index # 236\n",
      "done for ticker CSGP index # 237\n",
      "done for ticker OTIS index # 238\n",
      "done for ticker MTD index # 239\n",
      "done for ticker SBAC index # 240\n",
      "done for ticker ON index # 241\n",
      "done for ticker VICI index # 242\n",
      "done for ticker DLR index # 243\n",
      "done for ticker KEYS index # 244\n",
      "done for ticker PPG index # 245\n",
      "done for ticker WEC index # 246\n",
      "done for ticker CTSH index # 247\n",
      "done for ticker ROK index # 248\n",
      "done for ticker GWW index # 249\n",
      "done for ticker PCG index # 250\n",
      "done for ticker HPQ index # 251\n",
      "done for ticker FAST index # 252\n",
      "done for ticker DFS index # 253\n",
      "done for ticker MTB index # 254\n",
      "done for ticker PEG index # 255\n",
      "done for ticker OKE index # 256\n",
      "done for ticker DHI index # 257\n",
      "done for ticker APTV index # 258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for ticker BKR index # 259\n",
      "done for ticker GLW index # 260\n",
      "done for ticker LYB index # 261\n",
      "done for ticker ES index # 262\n",
      "done for ticker BAX index # 263\n",
      "done for ticker STT index # 264\n",
      "done for ticker VRSK index # 265\n",
      "done for ticker TROW index # 266\n",
      "done for ticker WBD index # 267\n",
      "done for ticker AWK index # 268\n",
      "done for ticker IT index # 269\n",
      "done for ticker GPN index # 270\n",
      "done for ticker HRL index # 271\n",
      "done for ticker FANG index # 272\n",
      "done for ticker WTW index # 273\n",
      "done for ticker RJF index # 274\n",
      "done for ticker GPC index # 275\n",
      "done for ticker IFF index # 276\n",
      "done for ticker CDW index # 277\n",
      "done for ticker TSCO index # 278\n",
      "done for ticker FITB index # 279\n",
      "done for ticker ARE index # 280\n",
      "done for ticker URI index # 281\n",
      "done for ticker ZBH index # 282\n",
      "done for ticker K index # 283\n",
      "done for ticker LEN index # 284\n",
      "done for ticker EBAY index # 285\n",
      "done for ticker EIX index # 286\n",
      "done for ticker CBRE index # 287\n",
      "done for ticker EFX index # 288\n",
      "done for ticker VMC index # 289\n",
      "done for ticker TSN index # 290\n",
      "done for ticker HIG index # 291\n",
      "done for ticker FTV index # 292\n",
      "done for ticker WY index # 293\n",
      "done for ticker EQR index # 294\n",
      "done for ticker AVB index # 295\n",
      "done for ticker MKC index # 296\n",
      "done for ticker ETR index # 297\n",
      "done for ticker LUV index # 298\n",
      "done for ticker ULTA index # 299\n",
      "done for ticker AEE index # 300\n",
      "done for ticker MLM index # 301\n",
      "done for ticker FE index # 302\n",
      "done for ticker PFG index # 303\n",
      "done for ticker FRC index # 304\n",
      "done for ticker DTE index # 305\n",
      "done for ticker DAL index # 306\n",
      "done for ticker HBAN index # 307\n",
      "done for ticker IR index # 308\n",
      "done for ticker CTRA index # 309\n",
      "done for ticker ANSS index # 310\n",
      "done for ticker ACGL index # 311\n",
      "done for ticker PPL index # 312\n",
      "done for ticker RF index # 313\n",
      "done for ticker VRSN index # 314\n",
      "done for ticker LH index # 315\n",
      "done for ticker EXR index # 316\n",
      "done for ticker PWR index # 317\n",
      "done for ticker CF index # 318\n",
      "done for ticker CAH index # 319\n",
      "done for ticker CFG index # 320\n",
      "done for ticker XYL index # 321\n",
      "done for ticker HPE index # 322\n",
      "done for ticker EPAM index # 323\n",
      "done for ticker DOV index # 324\n",
      "done for ticker WAT index # 325\n",
      "done for ticker WRB index # 326\n",
      "done for ticker TDY index # 327\n",
      "done for ticker PAYC index # 328\n",
      "done for ticker ROL index # 329\n",
      "done for ticker NTRS index # 330\n",
      "done for ticker MRO index # 331\n",
      "done for ticker CNP index # 332\n",
      "done for ticker INVH index # 333\n",
      "done for ticker CHD index # 334\n",
      "done for ticker AES index # 335\n",
      "done for ticker MOH index # 336\n",
      "done for ticker JBHT index # 337\n",
      "done for ticker MAA index # 338\n",
      "done for ticker BBY index # 339\n",
      "done for ticker CLX index # 340\n",
      "done for ticker HOLX index # 341\n",
      "done for ticker WAB index # 342\n",
      "done for ticker DRI index # 343\n",
      "done for ticker EXPD index # 344\n",
      "done for ticker STE index # 345\n",
      "done for ticker AMCR index # 346\n",
      "done for ticker VTR index # 347\n",
      "done for ticker IEX index # 348\n",
      "done for ticker CAG index # 349\n",
      "done for ticker CMS index # 350\n",
      "done for ticker KEY index # 351\n",
      "done for ticker MPWR index # 352\n",
      "done for ticker BALL index # 353\n",
      "done for ticker J index # 354\n",
      "done for ticker BR index # 355\n",
      "done for ticker GRMN index # 356\n",
      "done for ticker PKI index # 357\n",
      "done for ticker TTWO index # 358\n",
      "done for ticker INCY index # 359\n",
      "done for ticker FDS index # 360\n",
      "done for ticker MOS index # 361\n",
      "done for ticker SEDG index # 362\n",
      "done for ticker CINF index # 363\n",
      "done for ticker ABMD index # 364\n",
      "done for ticker DGX index # 365\n",
      "done for ticker WST index # 366\n",
      "done for ticker ATO index # 367\n",
      "done for ticker TRGP index # 368\n",
      "done for ticker BRO index # 369\n",
      "done for ticker SYF index # 370\n",
      "done for ticker FOX index # 371\n",
      "done for ticker FOXA index # 372\n",
      "done for ticker NTAP index # 373\n",
      "done for ticker FMC index # 374\n",
      "done for ticker EQT index # 375\n",
      "done for ticker OMC index # 376\n",
      "done for ticker SJM index # 377\n",
      "done for ticker LYV index # 378\n",
      "done for ticker CPB index # 379\n",
      "done for ticker HWM index # 380\n",
      "done for ticker CPRT index # 381\n",
      "done for ticker AVY index # 382\n",
      "done for ticker IRM index # 383\n",
      "done for ticker COO index # 384\n",
      "done for ticker ALGN index # 385\n",
      "done for ticker SWKS index # 386\n",
      "done for ticker EXPE index # 387\n",
      "done for ticker RCL index # 388\n",
      "done for ticker ETSY index # 389\n",
      "done for ticker APA index # 390\n",
      "done for ticker GEN index # 391\n",
      "done for ticker TXT index # 392\n",
      "done for ticker LDOS index # 393\n",
      "done for ticker LKQ index # 394\n",
      "done for ticker TER index # 395\n",
      "done for ticker PTC index # 396\n",
      "done for ticker TRMB index # 397\n",
      "done for ticker AKAM index # 398\n",
      "done for ticker NVR index # 399\n",
      "done for ticker UAL index # 400\n",
      "done for ticker LNT index # 401\n",
      "done for ticker FLT index # 402\n",
      "done for ticker KIM index # 403\n",
      "done for ticker ZBRA index # 404\n",
      "done for ticker TYL index # 405\n",
      "done for ticker DPZ index # 406\n",
      "done for ticker JKHY index # 407\n",
      "done for ticker MGM index # 408\n",
      "done for ticker ESS index # 409\n",
      "done for ticker L index # 410\n",
      "done for ticker PEAK index # 411\n",
      "done for ticker MTCH index # 412\n",
      "done for ticker NDSN index # 413\n",
      "done for ticker EVRG index # 414\n",
      "done for ticker IPG index # 415\n",
      "done for ticker BEN index # 416\n",
      "done for ticker CBOE index # 417\n",
      "done for ticker TECH index # 418\n",
      "done for ticker SIVB index # 419\n",
      "done for ticker VFC index # 420\n",
      "done for ticker IP index # 421\n",
      "done for ticker HST index # 422\n",
      "done for ticker UDR index # 423\n",
      "done for ticker POOL index # 424\n",
      "done for ticker RE index # 425\n",
      "done for ticker PARA index # 426\n",
      "done for ticker SNA index # 427\n",
      "done for ticker CPT index # 428\n",
      "done for ticker LW index # 429\n",
      "done for ticker PKG index # 430\n",
      "done for ticker CRL index # 431\n",
      "done for ticker SWK index # 432\n",
      "done for ticker BIO index # 433\n",
      "done for ticker WDC index # 434\n",
      "done for ticker CHRW index # 435\n",
      "done for ticker STX index # 436\n",
      "done for ticker MAS index # 437\n",
      "done for ticker GL index # 438\n",
      "done for ticker CE index # 439\n",
      "done for ticker REG index # 440\n",
      "done for ticker NI index # 441\n",
      "done for ticker BXP index # 442\n",
      "done for ticker HSIC index # 443\n",
      "done for ticker CCL index # 444\n",
      "done for ticker TFX index # 445\n",
      "done for ticker CZR index # 446\n",
      "done for ticker NWS index # 447\n",
      "done for ticker NWSA index # 448\n",
      "done for ticker KMX index # 449\n",
      "done for ticker EMN index # 450\n",
      "done for ticker JNPR index # 451\n",
      "done for ticker PHM index # 452\n",
      "done for ticker CDAY index # 453\n",
      "done for ticker ALLE index # 454\n",
      "done for ticker QRVO index # 455\n",
      "done for ticker BWA index # 456\n",
      "done for ticker NRG index # 457\n",
      "done for ticker MKTX index # 458\n",
      "done for ticker WRK index # 459\n",
      "done for ticker UHS index # 460\n",
      "done for ticker FFIV index # 461\n",
      "done for ticker AOS index # 462\n",
      "done for ticker CMA index # 463\n",
      "done for ticker AAL index # 464\n",
      "done for ticker BBWI index # 465\n",
      "done for ticker HII index # 466\n",
      "done for ticker AAP index # 467\n",
      "done for ticker TPR index # 468\n",
      "done for ticker FRT index # 469\n",
      "done for ticker IVZ index # 470\n",
      "done for ticker PNW index # 471\n",
      "done for ticker HAS index # 472\n",
      "done for ticker WYNN index # 473\n",
      "done for ticker FBHS index # 474\n",
      "done for ticker SBNY index # 475\n",
      "done for ticker DISH index # 476\n",
      "done for ticker RHI index # 477\n",
      "done for ticker WHR index # 478\n",
      "done for ticker ZION index # 479\n",
      "done for ticker CTLT index # 480\n",
      "done for ticker PNR index # 481\n",
      "done for ticker SEE index # 482\n",
      "done for ticker RL index # 483\n",
      "done for ticker NCLH index # 484\n",
      "done for ticker DXC index # 485\n",
      "done for ticker GNRC index # 486\n",
      "done for ticker AIZ index # 487\n",
      "done for ticker XRAY index # 488\n",
      "done for ticker LNC index # 489\n",
      "done for ticker DVA index # 490\n",
      "done for ticker MHK index # 491\n",
      "done for ticker LUMN index # 492\n",
      "done for ticker ALK index # 493\n",
      "done for ticker NWL index # 494\n",
      "done for ticker VNO index # 495\n",
      "done for ticker TAP index # 496\n"
     ]
    }
   ],
   "source": [
    "dict_score = {}\n",
    "tickers = utils.return_tickers()\n",
    "for ticker in tickers:\n",
    "        dict_score[ticker] = cross_validate_ml(prep.ready_to_train_df(ticker), XGBRegressor())\n",
    "        print(f\"done for ticker {ticker} index # {tickers.index(ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34865b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf990b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d012c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8e88d19",
   "metadata": {},
   "source": [
    "### Cross val scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d84031e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:31:13.692301Z",
     "start_time": "2022-12-06T10:31:13.665214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004796076311898096 0.02096529727516623\n"
     ]
    }
   ],
   "source": [
    "rmse= []\n",
    "baseline = []\n",
    "for key in dict_score.keys():\n",
    "    rmse.append(dict_score[key][0])\n",
    "    baseline.append(dict_score[key][1])\n",
    "print(np.mean(rmse), np.mean(baseline) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13660a15",
   "metadata": {},
   "source": [
    "## Custom grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce79b4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-05T15:25:40.338767Z",
     "start_time": "2022-12-05T15:25:40.312234Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depth = [2, 5, 8]\n",
    "criterion = ['friedman_mse', 'squared_error', 'mse']\n",
    "learning_rate=[0.01, 0.1, 0.2] \n",
    "n_estimators=[100, 200, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7badec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2569b250",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:19:16.229421Z",
     "start_time": "2022-12-06T09:19:16.199027Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_gridsearch(df, model, max_depth=[2,3,4], criterion = ['friedman_mse', 'squared_error', 'mse'], n_estimator=[50, 75, 100], learning_rate=[0.08, 0.1, 0.12], loss=['squared_error', 'absolute_error', 'huber']):\n",
    "    counter = 0\n",
    "    rmse = []\n",
    "    baseline = []\n",
    "    params = []\n",
    "    for max_depth_i in max_depth:\n",
    "        for criterion_i in criterion:\n",
    "            for n_estimator_i in n_estimator:\n",
    "                for learning_rate_i in learning_rate:\n",
    "                    for loss_i in loss:\n",
    "                        test = cross_validate_ml(df = df, model = model(max_depth=max_depth_i,\n",
    "                                                                   criterion = criterion_i,\n",
    "                                                                   n_estimators = n_estimator_i,\n",
    "                                                                   learning_rate = learning_rate_i,\n",
    "                                                                   loss = loss_i))\n",
    "                        rmse.append(test[0])\n",
    "                        baseline.append(test[1])\n",
    "                        params.append((max_depth_i, criterion_i, n_estimator_i, learning_rate_i))\n",
    "                        counter += 1\n",
    "                        print(f'model {counter} done with parameters: max_depth = {max_depth_i}, criterion = {criterion_i}, estimators = {n_estimator_i}, learning rate = {learning_rate_i}, loss = {loss_i}, rmse = {test[0]}')\n",
    "    idx_min = np.argmin(rmse)\n",
    "    best_params = params[idx_min]\n",
    "    \n",
    "    return best_params, rmse, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b0d59c",
   "metadata": {},
   "source": [
    "### Model results on one stock on a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02ccef6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:44:15.668227Z",
     "start_time": "2022-12-06T09:19:29.718526Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002958185553002535\n",
      "model 2 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.006407018407676127\n",
      "model 3 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.004428068812946768\n",
      "model 4 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.002776729479836416\n",
      "model 5 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.005372760873905239\n",
      "model 6 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.003432291380610513\n",
      "model 7 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.0026067464673304175\n",
      "model 8 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.0048061652599137505\n",
      "model 9 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030851908081413497\n",
      "model 10 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0026039764209378246\n",
      "model 11 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.00477931059886155\n",
      "model 12 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.003132758828900319\n",
      "model 13 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0027137581776954262\n",
      "model 14 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0042761812198638905\n",
      "model 15 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.002879525183277261\n",
      "model 16 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.002675563304797706\n",
      "model 17 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.004136672056708204\n",
      "model 18 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0028028874014932675\n",
      "model 19 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.00263901736188292\n",
      "model 20 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.004243777202286242\n",
      "model 21 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0028285475219040047\n",
      "model 22 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0026600684459853496\n",
      "model 23 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003929768715246046\n",
      "model 24 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.002738377789027572\n",
      "model 25 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.0026049910482777846\n",
      "model 26 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0038342459402143712\n",
      "model 27 done with parameters: max_depth = 2, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0027842983111944423\n",
      "model 28 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002946016742644336\n",
      "model 29 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.006400529273369599\n",
      "model 30 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.004428402988000523\n",
      "model 31 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.00274092287895698\n",
      "model 32 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.005369459505880697\n",
      "model 33 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0034536502342627138\n",
      "model 34 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.0026201916948474895\n",
      "model 35 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.00481556253048331\n",
      "model 36 done with parameters: max_depth = 2, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030731467770259437\n",
      "model 37 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0026640675291778127\n",
      "model 38 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004781983704751462\n",
      "model 39 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0031020685649265803\n",
      "model 40 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.002641066825947065\n",
      "model 41 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.004317992084797235\n",
      "model 42 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0028619443825098193\n",
      "model 43 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0025911692981691096\n",
      "model 44 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.004111814141492845\n",
      "model 45 done with parameters: max_depth = 2, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.002768306658624108\n",
      "model 46 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.0026757573258833324\n",
      "model 47 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.004186948590438916\n",
      "model 48 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0028185742191962665\n",
      "model 49 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.002659309412779239\n",
      "model 50 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003949072152121796\n",
      "model 51 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0027222261058379204\n",
      "model 52 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002705998569738018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 53 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0038499484792597865\n",
      "model 54 done with parameters: max_depth = 2, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.002795539577918935\n",
      "model 55 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.0029099191595345383\n",
      "model 56 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.006407991680854807\n",
      "model 57 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.004428402988000523\n",
      "model 58 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0028373111145722184\n",
      "model 59 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.005372474799411161\n",
      "model 60 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0034288454838289927\n",
      "model 61 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.0025617175515441487\n",
      "model 62 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.0048133561585675414\n",
      "model 63 done with parameters: max_depth = 2, criterion = mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030763780454744144\n",
      "model 64 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0026350705093279\n",
      "model 65 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004773342022038409\n",
      "model 66 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0031281830813940513\n",
      "model 67 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0026786321052186756\n",
      "model 68 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.004333966762045556\n",
      "model 69 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.002845647774894737\n",
      "model 70 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0026488261570874246\n",
      "model 71 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0040720219215136045\n",
      "model 72 done with parameters: max_depth = 2, criterion = mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0027838512151926002\n",
      "model 73 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.0026434954217452755\n",
      "model 74 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.004142942835776003\n",
      "model 75 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.002837433961437816\n",
      "model 76 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0026015436713270864\n",
      "model 77 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003915021453609067\n",
      "model 78 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.002733796570629045\n",
      "model 79 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002590710250606715\n",
      "model 80 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.003836455439472309\n",
      "model 81 done with parameters: max_depth = 2, criterion = mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0028310156316547074\n",
      "model 82 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002705103403928138\n",
      "model 83 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.005055674970022903\n",
      "model 84 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.003942692273722824\n",
      "model 85 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0025979372722968975\n",
      "model 86 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.004457668793358413\n",
      "model 87 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0032871980973670925\n",
      "model 88 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002639942667455135\n",
      "model 89 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.004204336452061385\n",
      "model 90 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030337239385201673\n",
      "model 91 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0025862176325708685\n",
      "model 92 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.00413893352920339\n",
      "model 93 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0030566455793911897\n",
      "model 94 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0025916442432565383\n",
      "model 95 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.00392530690915287\n",
      "model 96 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.002810890632321019\n",
      "model 97 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0026127529836289937\n",
      "model 98 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0038031379441063966\n",
      "model 99 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.002786888933712554\n",
      "model 100 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.002557329832721479\n",
      "model 101 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.003849198727467321\n",
      "model 102 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0028084278300910463\n",
      "model 103 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0025990298794970937\n",
      "model 104 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.0037592214174426305\n",
      "model 105 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0026895703301677075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 106 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.0025730424155033494\n",
      "model 107 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0036681061284255722\n",
      "model 108 done with parameters: max_depth = 3, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0027251777091245326\n",
      "model 109 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002702793151349547\n",
      "model 110 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.005058232367804183\n",
      "model 111 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.003941806328485692\n",
      "model 112 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.002588842026698095\n",
      "model 113 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.004427805540319038\n",
      "model 114 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.003268000952219227\n",
      "model 115 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002579248928301906\n",
      "model 116 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.004172891100709107\n",
      "model 117 done with parameters: max_depth = 3, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030140686492261992\n",
      "model 118 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.002613291387439508\n",
      "model 119 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004139953071470512\n",
      "model 120 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.003080924745809816\n",
      "model 121 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0026145555792715857\n",
      "model 122 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.003921975417612286\n",
      "model 123 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0027913629970269323\n",
      "model 124 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.002587307414146831\n",
      "model 125 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.003799259214174869\n",
      "model 126 done with parameters: max_depth = 3, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0027115296373352686\n",
      "model 127 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.002601336330933987\n",
      "model 128 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.0038066571748408824\n",
      "model 129 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.002808894661718752\n",
      "model 130 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0026127988262543223\n",
      "model 131 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.0037446865076462914\n",
      "model 132 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.002685108521693579\n",
      "model 133 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002634867846287722\n",
      "model 134 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.003733008379825462\n",
      "model 135 done with parameters: max_depth = 3, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0026192326431160044\n",
      "model 136 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002724326707987458\n",
      "model 137 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.005044860223031934\n",
      "model 138 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.0039340807041176136\n",
      "model 139 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0026069569351037753\n",
      "model 140 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.0044098313370142415\n",
      "model 141 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.003304580254359149\n",
      "model 142 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002652285999047112\n",
      "model 143 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.00422005058828855\n",
      "model 144 done with parameters: max_depth = 3, criterion = mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.002941688447540913\n",
      "model 145 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.002650852313756527\n",
      "model 146 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.0041723361975041715\n",
      "model 147 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0030481177907805516\n",
      "model 148 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.002647907064336234\n",
      "model 149 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0039274727502588164\n",
      "model 150 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.002783644662704949\n",
      "model 151 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0026850857188837887\n",
      "model 152 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0038454450583371423\n",
      "model 153 done with parameters: max_depth = 3, criterion = mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0027657041578346194\n",
      "model 154 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.0025795467664902786\n",
      "model 155 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.003925606315613257\n",
      "model 156 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0028148348736183842\n",
      "model 157 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0026357231847853823\n",
      "model 158 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003745041132907287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 159 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.002699105329518561\n",
      "model 160 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002601949405101594\n",
      "model 161 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0036977284985453603\n",
      "model 162 done with parameters: max_depth = 3, criterion = mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0027447041132625722\n",
      "model 163 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002758204626328357\n",
      "model 164 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.004962315891742368\n",
      "model 165 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.003755602715526272\n",
      "model 166 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0028201793905030946\n",
      "model 167 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.004377564048505937\n",
      "model 168 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.003315656914245619\n",
      "model 169 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.0027571244856977664\n",
      "model 170 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.004088070021363021\n",
      "model 171 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.003059518646427285\n",
      "model 172 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0027493575860221735\n",
      "model 173 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004037023468753866\n",
      "model 174 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.003122135588575402\n",
      "model 175 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0027805589220566613\n",
      "model 176 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0038261411672781322\n",
      "model 177 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0028689319120608105\n",
      "model 178 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0027474853227340396\n",
      "model 179 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0037876683726842096\n",
      "model 180 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0028343406482770153\n",
      "model 181 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.0027827520136940245\n",
      "model 182 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.0038275716196665426\n",
      "model 183 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0030061031913663073\n",
      "model 184 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.0028641024564482363\n",
      "model 185 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003675794027779795\n",
      "model 186 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0028617470584144803\n",
      "model 187 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.0027819085919339732\n",
      "model 188 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.0037322326772521604\n",
      "model 189 done with parameters: max_depth = 4, criterion = friedman_mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.002859199248754659\n",
      "model 190 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.0028216246715976863\n",
      "model 191 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.0049191235829356104\n",
      "model 192 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.0037387215313132395\n",
      "model 193 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.002843667545519211\n",
      "model 194 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.00439015237329631\n",
      "model 195 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0033074000213341066\n",
      "model 196 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002730438251678527\n",
      "model 197 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.0040828874531489215\n",
      "model 198 done with parameters: max_depth = 4, criterion = squared_error, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030158147085443373\n",
      "model 199 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0027748639528769456\n",
      "model 200 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004081739919150697\n",
      "model 201 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.0031098377803180006\n",
      "model 202 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.0028109126274234626\n",
      "model 203 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0038678451510520445\n",
      "model 204 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0029069862124411865\n",
      "model 205 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.0028351433629591177\n",
      "model 206 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.003825465067210109\n",
      "model 207 done with parameters: max_depth = 4, criterion = squared_error, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.0028548669489826795\n",
      "model 208 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.002779061332540999\n",
      "model 209 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.0037924076379733714\n",
      "model 210 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0029269825240330802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 211 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.002822949349514258\n",
      "model 212 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.003671004260923169\n",
      "model 213 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0027898015141627564\n",
      "model 214 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.002789152927054379\n",
      "model 215 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.003691289937380604\n",
      "model 216 done with parameters: max_depth = 4, criterion = squared_error, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0028131115634149615\n",
      "model 217 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.08, loss = squared_error, rmse = 0.002799139551941181\n",
      "model 218 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.08, loss = absolute_error, rmse = 0.004973818289138408\n",
      "model 219 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.08, loss = huber, rmse = 0.003748542432319123\n",
      "model 220 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.1, loss = squared_error, rmse = 0.0027927302306963797\n",
      "model 221 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.1, loss = absolute_error, rmse = 0.004335859131408847\n",
      "model 222 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.1, loss = huber, rmse = 0.0033200876306249266\n",
      "model 223 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.12, loss = squared_error, rmse = 0.002807844523165585\n",
      "model 224 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.12, loss = absolute_error, rmse = 0.004042848200822164\n",
      "model 225 done with parameters: max_depth = 4, criterion = mse, estimators = 50, learning rate = 0.12, loss = huber, rmse = 0.0030667195280845664\n",
      "model 226 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.08, loss = squared_error, rmse = 0.0027740729107799005\n",
      "model 227 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.08, loss = absolute_error, rmse = 0.004034942483539463\n",
      "model 228 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.08, loss = huber, rmse = 0.003147541038843053\n",
      "model 229 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.1, loss = squared_error, rmse = 0.002807550771254751\n",
      "model 230 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.1, loss = absolute_error, rmse = 0.0038697693960322646\n",
      "model 231 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.1, loss = huber, rmse = 0.0028860769585803145\n",
      "model 232 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.12, loss = squared_error, rmse = 0.00277620080412421\n",
      "model 233 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.12, loss = absolute_error, rmse = 0.0038790739000200787\n",
      "model 234 done with parameters: max_depth = 4, criterion = mse, estimators = 75, learning rate = 0.12, loss = huber, rmse = 0.002832208965267212\n",
      "model 235 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.08, loss = squared_error, rmse = 0.002767648470364058\n",
      "model 236 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.08, loss = absolute_error, rmse = 0.003728316443935388\n",
      "model 237 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.08, loss = huber, rmse = 0.0029278226365026107\n",
      "model 238 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.1, loss = squared_error, rmse = 0.002827328916762123\n",
      "model 239 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.1, loss = absolute_error, rmse = 0.0037037792842529284\n",
      "model 240 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.1, loss = huber, rmse = 0.0028500206611104984\n",
      "model 241 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.12, loss = squared_error, rmse = 0.0027272080473247054\n",
      "model 242 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.12, loss = absolute_error, rmse = 0.00378886572935585\n",
      "model 243 done with parameters: max_depth = 4, criterion = mse, estimators = 100, learning rate = 0.12, loss = huber, rmse = 0.0027762487865231975\n"
     ]
    }
   ],
   "source": [
    "gs_IP = custom_gridsearch(prep.ready_to_train_df('IP'), model = GradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ef96cef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T10:34:38.067317Z",
     "start_time": "2022-12-06T10:34:38.029878Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 'friedman_mse', 100, 0.08),\n",
       " [0.002958185553002535,\n",
       "  0.006407018407676127,\n",
       "  0.004428068812946768,\n",
       "  0.002776729479836416,\n",
       "  0.005372760873905239,\n",
       "  0.003432291380610513,\n",
       "  0.0026067464673304175,\n",
       "  0.0048061652599137505,\n",
       "  0.0030851908081413497,\n",
       "  0.0026039764209378246,\n",
       "  0.00477931059886155,\n",
       "  0.003132758828900319,\n",
       "  0.0027137581776954262,\n",
       "  0.0042761812198638905,\n",
       "  0.002879525183277261,\n",
       "  0.002675563304797706,\n",
       "  0.004136672056708204,\n",
       "  0.0028028874014932675,\n",
       "  0.00263901736188292,\n",
       "  0.004243777202286242,\n",
       "  0.0028285475219040047,\n",
       "  0.0026600684459853496,\n",
       "  0.003929768715246046,\n",
       "  0.002738377789027572,\n",
       "  0.0026049910482777846,\n",
       "  0.0038342459402143712,\n",
       "  0.0027842983111944423,\n",
       "  0.002946016742644336,\n",
       "  0.006400529273369599,\n",
       "  0.004428402988000523,\n",
       "  0.00274092287895698,\n",
       "  0.005369459505880697,\n",
       "  0.0034536502342627138,\n",
       "  0.0026201916948474895,\n",
       "  0.00481556253048331,\n",
       "  0.0030731467770259437,\n",
       "  0.0026640675291778127,\n",
       "  0.004781983704751462,\n",
       "  0.0031020685649265803,\n",
       "  0.002641066825947065,\n",
       "  0.004317992084797235,\n",
       "  0.0028619443825098193,\n",
       "  0.0025911692981691096,\n",
       "  0.004111814141492845,\n",
       "  0.002768306658624108,\n",
       "  0.0026757573258833324,\n",
       "  0.004186948590438916,\n",
       "  0.0028185742191962665,\n",
       "  0.002659309412779239,\n",
       "  0.003949072152121796,\n",
       "  0.0027222261058379204,\n",
       "  0.002705998569738018,\n",
       "  0.0038499484792597865,\n",
       "  0.002795539577918935,\n",
       "  0.0029099191595345383,\n",
       "  0.006407991680854807,\n",
       "  0.004428402988000523,\n",
       "  0.0028373111145722184,\n",
       "  0.005372474799411161,\n",
       "  0.0034288454838289927,\n",
       "  0.0025617175515441487,\n",
       "  0.0048133561585675414,\n",
       "  0.0030763780454744144,\n",
       "  0.0026350705093279,\n",
       "  0.004773342022038409,\n",
       "  0.0031281830813940513,\n",
       "  0.0026786321052186756,\n",
       "  0.004333966762045556,\n",
       "  0.002845647774894737,\n",
       "  0.0026488261570874246,\n",
       "  0.0040720219215136045,\n",
       "  0.0027838512151926002,\n",
       "  0.0026434954217452755,\n",
       "  0.004142942835776003,\n",
       "  0.002837433961437816,\n",
       "  0.0026015436713270864,\n",
       "  0.003915021453609067,\n",
       "  0.002733796570629045,\n",
       "  0.002590710250606715,\n",
       "  0.003836455439472309,\n",
       "  0.0028310156316547074,\n",
       "  0.002705103403928138,\n",
       "  0.005055674970022903,\n",
       "  0.003942692273722824,\n",
       "  0.0025979372722968975,\n",
       "  0.004457668793358413,\n",
       "  0.0032871980973670925,\n",
       "  0.002639942667455135,\n",
       "  0.004204336452061385,\n",
       "  0.0030337239385201673,\n",
       "  0.0025862176325708685,\n",
       "  0.00413893352920339,\n",
       "  0.0030566455793911897,\n",
       "  0.0025916442432565383,\n",
       "  0.00392530690915287,\n",
       "  0.002810890632321019,\n",
       "  0.0026127529836289937,\n",
       "  0.0038031379441063966,\n",
       "  0.002786888933712554,\n",
       "  0.002557329832721479,\n",
       "  0.003849198727467321,\n",
       "  0.0028084278300910463,\n",
       "  0.0025990298794970937,\n",
       "  0.0037592214174426305,\n",
       "  0.0026895703301677075,\n",
       "  0.0025730424155033494,\n",
       "  0.0036681061284255722,\n",
       "  0.0027251777091245326,\n",
       "  0.002702793151349547,\n",
       "  0.005058232367804183,\n",
       "  0.003941806328485692,\n",
       "  0.002588842026698095,\n",
       "  0.004427805540319038,\n",
       "  0.003268000952219227,\n",
       "  0.002579248928301906,\n",
       "  0.004172891100709107,\n",
       "  0.0030140686492261992,\n",
       "  0.002613291387439508,\n",
       "  0.004139953071470512,\n",
       "  0.003080924745809816,\n",
       "  0.0026145555792715857,\n",
       "  0.003921975417612286,\n",
       "  0.0027913629970269323,\n",
       "  0.002587307414146831,\n",
       "  0.003799259214174869,\n",
       "  0.0027115296373352686,\n",
       "  0.002601336330933987,\n",
       "  0.0038066571748408824,\n",
       "  0.002808894661718752,\n",
       "  0.0026127988262543223,\n",
       "  0.0037446865076462914,\n",
       "  0.002685108521693579,\n",
       "  0.002634867846287722,\n",
       "  0.003733008379825462,\n",
       "  0.0026192326431160044,\n",
       "  0.002724326707987458,\n",
       "  0.005044860223031934,\n",
       "  0.0039340807041176136,\n",
       "  0.0026069569351037753,\n",
       "  0.0044098313370142415,\n",
       "  0.003304580254359149,\n",
       "  0.002652285999047112,\n",
       "  0.00422005058828855,\n",
       "  0.002941688447540913,\n",
       "  0.002650852313756527,\n",
       "  0.0041723361975041715,\n",
       "  0.0030481177907805516,\n",
       "  0.002647907064336234,\n",
       "  0.0039274727502588164,\n",
       "  0.002783644662704949,\n",
       "  0.0026850857188837887,\n",
       "  0.0038454450583371423,\n",
       "  0.0027657041578346194,\n",
       "  0.0025795467664902786,\n",
       "  0.003925606315613257,\n",
       "  0.0028148348736183842,\n",
       "  0.0026357231847853823,\n",
       "  0.003745041132907287,\n",
       "  0.002699105329518561,\n",
       "  0.002601949405101594,\n",
       "  0.0036977284985453603,\n",
       "  0.0027447041132625722,\n",
       "  0.002758204626328357,\n",
       "  0.004962315891742368,\n",
       "  0.003755602715526272,\n",
       "  0.0028201793905030946,\n",
       "  0.004377564048505937,\n",
       "  0.003315656914245619,\n",
       "  0.0027571244856977664,\n",
       "  0.004088070021363021,\n",
       "  0.003059518646427285,\n",
       "  0.0027493575860221735,\n",
       "  0.004037023468753866,\n",
       "  0.003122135588575402,\n",
       "  0.0027805589220566613,\n",
       "  0.0038261411672781322,\n",
       "  0.0028689319120608105,\n",
       "  0.0027474853227340396,\n",
       "  0.0037876683726842096,\n",
       "  0.0028343406482770153,\n",
       "  0.0027827520136940245,\n",
       "  0.0038275716196665426,\n",
       "  0.0030061031913663073,\n",
       "  0.0028641024564482363,\n",
       "  0.003675794027779795,\n",
       "  0.0028617470584144803,\n",
       "  0.0027819085919339732,\n",
       "  0.0037322326772521604,\n",
       "  0.002859199248754659,\n",
       "  0.0028216246715976863,\n",
       "  0.0049191235829356104,\n",
       "  0.0037387215313132395,\n",
       "  0.002843667545519211,\n",
       "  0.00439015237329631,\n",
       "  0.0033074000213341066,\n",
       "  0.002730438251678527,\n",
       "  0.0040828874531489215,\n",
       "  0.0030158147085443373,\n",
       "  0.0027748639528769456,\n",
       "  0.004081739919150697,\n",
       "  0.0031098377803180006,\n",
       "  0.0028109126274234626,\n",
       "  0.0038678451510520445,\n",
       "  0.0029069862124411865,\n",
       "  0.0028351433629591177,\n",
       "  0.003825465067210109,\n",
       "  0.0028548669489826795,\n",
       "  0.002779061332540999,\n",
       "  0.0037924076379733714,\n",
       "  0.0029269825240330802,\n",
       "  0.002822949349514258,\n",
       "  0.003671004260923169,\n",
       "  0.0027898015141627564,\n",
       "  0.002789152927054379,\n",
       "  0.003691289937380604,\n",
       "  0.0028131115634149615,\n",
       "  0.002799139551941181,\n",
       "  0.004973818289138408,\n",
       "  0.003748542432319123,\n",
       "  0.0027927302306963797,\n",
       "  0.004335859131408847,\n",
       "  0.0033200876306249266,\n",
       "  0.002807844523165585,\n",
       "  0.004042848200822164,\n",
       "  0.0030667195280845664,\n",
       "  0.0027740729107799005,\n",
       "  0.004034942483539463,\n",
       "  0.003147541038843053,\n",
       "  0.002807550771254751,\n",
       "  0.0038697693960322646,\n",
       "  0.0028860769585803145,\n",
       "  0.00277620080412421,\n",
       "  0.0038790739000200787,\n",
       "  0.002832208965267212,\n",
       "  0.002767648470364058,\n",
       "  0.003728316443935388,\n",
       "  0.0029278226365026107,\n",
       "  0.002827328916762123,\n",
       "  0.0037037792842529284,\n",
       "  0.0028500206611104984,\n",
       "  0.0027272080473247054,\n",
       "  0.00378886572935585,\n",
       "  0.0027762487865231975],\n",
       " [(2, 'friedman_mse', 50, 0.08),\n",
       "  (2, 'friedman_mse', 50, 0.08),\n",
       "  (2, 'friedman_mse', 50, 0.08),\n",
       "  (2, 'friedman_mse', 50, 0.1),\n",
       "  (2, 'friedman_mse', 50, 0.1),\n",
       "  (2, 'friedman_mse', 50, 0.1),\n",
       "  (2, 'friedman_mse', 50, 0.12),\n",
       "  (2, 'friedman_mse', 50, 0.12),\n",
       "  (2, 'friedman_mse', 50, 0.12),\n",
       "  (2, 'friedman_mse', 75, 0.08),\n",
       "  (2, 'friedman_mse', 75, 0.08),\n",
       "  (2, 'friedman_mse', 75, 0.08),\n",
       "  (2, 'friedman_mse', 75, 0.1),\n",
       "  (2, 'friedman_mse', 75, 0.1),\n",
       "  (2, 'friedman_mse', 75, 0.1),\n",
       "  (2, 'friedman_mse', 75, 0.12),\n",
       "  (2, 'friedman_mse', 75, 0.12),\n",
       "  (2, 'friedman_mse', 75, 0.12),\n",
       "  (2, 'friedman_mse', 100, 0.08),\n",
       "  (2, 'friedman_mse', 100, 0.08),\n",
       "  (2, 'friedman_mse', 100, 0.08),\n",
       "  (2, 'friedman_mse', 100, 0.1),\n",
       "  (2, 'friedman_mse', 100, 0.1),\n",
       "  (2, 'friedman_mse', 100, 0.1),\n",
       "  (2, 'friedman_mse', 100, 0.12),\n",
       "  (2, 'friedman_mse', 100, 0.12),\n",
       "  (2, 'friedman_mse', 100, 0.12),\n",
       "  (2, 'squared_error', 50, 0.08),\n",
       "  (2, 'squared_error', 50, 0.08),\n",
       "  (2, 'squared_error', 50, 0.08),\n",
       "  (2, 'squared_error', 50, 0.1),\n",
       "  (2, 'squared_error', 50, 0.1),\n",
       "  (2, 'squared_error', 50, 0.1),\n",
       "  (2, 'squared_error', 50, 0.12),\n",
       "  (2, 'squared_error', 50, 0.12),\n",
       "  (2, 'squared_error', 50, 0.12),\n",
       "  (2, 'squared_error', 75, 0.08),\n",
       "  (2, 'squared_error', 75, 0.08),\n",
       "  (2, 'squared_error', 75, 0.08),\n",
       "  (2, 'squared_error', 75, 0.1),\n",
       "  (2, 'squared_error', 75, 0.1),\n",
       "  (2, 'squared_error', 75, 0.1),\n",
       "  (2, 'squared_error', 75, 0.12),\n",
       "  (2, 'squared_error', 75, 0.12),\n",
       "  (2, 'squared_error', 75, 0.12),\n",
       "  (2, 'squared_error', 100, 0.08),\n",
       "  (2, 'squared_error', 100, 0.08),\n",
       "  (2, 'squared_error', 100, 0.08),\n",
       "  (2, 'squared_error', 100, 0.1),\n",
       "  (2, 'squared_error', 100, 0.1),\n",
       "  (2, 'squared_error', 100, 0.1),\n",
       "  (2, 'squared_error', 100, 0.12),\n",
       "  (2, 'squared_error', 100, 0.12),\n",
       "  (2, 'squared_error', 100, 0.12),\n",
       "  (2, 'mse', 50, 0.08),\n",
       "  (2, 'mse', 50, 0.08),\n",
       "  (2, 'mse', 50, 0.08),\n",
       "  (2, 'mse', 50, 0.1),\n",
       "  (2, 'mse', 50, 0.1),\n",
       "  (2, 'mse', 50, 0.1),\n",
       "  (2, 'mse', 50, 0.12),\n",
       "  (2, 'mse', 50, 0.12),\n",
       "  (2, 'mse', 50, 0.12),\n",
       "  (2, 'mse', 75, 0.08),\n",
       "  (2, 'mse', 75, 0.08),\n",
       "  (2, 'mse', 75, 0.08),\n",
       "  (2, 'mse', 75, 0.1),\n",
       "  (2, 'mse', 75, 0.1),\n",
       "  (2, 'mse', 75, 0.1),\n",
       "  (2, 'mse', 75, 0.12),\n",
       "  (2, 'mse', 75, 0.12),\n",
       "  (2, 'mse', 75, 0.12),\n",
       "  (2, 'mse', 100, 0.08),\n",
       "  (2, 'mse', 100, 0.08),\n",
       "  (2, 'mse', 100, 0.08),\n",
       "  (2, 'mse', 100, 0.1),\n",
       "  (2, 'mse', 100, 0.1),\n",
       "  (2, 'mse', 100, 0.1),\n",
       "  (2, 'mse', 100, 0.12),\n",
       "  (2, 'mse', 100, 0.12),\n",
       "  (2, 'mse', 100, 0.12),\n",
       "  (3, 'friedman_mse', 50, 0.08),\n",
       "  (3, 'friedman_mse', 50, 0.08),\n",
       "  (3, 'friedman_mse', 50, 0.08),\n",
       "  (3, 'friedman_mse', 50, 0.1),\n",
       "  (3, 'friedman_mse', 50, 0.1),\n",
       "  (3, 'friedman_mse', 50, 0.1),\n",
       "  (3, 'friedman_mse', 50, 0.12),\n",
       "  (3, 'friedman_mse', 50, 0.12),\n",
       "  (3, 'friedman_mse', 50, 0.12),\n",
       "  (3, 'friedman_mse', 75, 0.08),\n",
       "  (3, 'friedman_mse', 75, 0.08),\n",
       "  (3, 'friedman_mse', 75, 0.08),\n",
       "  (3, 'friedman_mse', 75, 0.1),\n",
       "  (3, 'friedman_mse', 75, 0.1),\n",
       "  (3, 'friedman_mse', 75, 0.1),\n",
       "  (3, 'friedman_mse', 75, 0.12),\n",
       "  (3, 'friedman_mse', 75, 0.12),\n",
       "  (3, 'friedman_mse', 75, 0.12),\n",
       "  (3, 'friedman_mse', 100, 0.08),\n",
       "  (3, 'friedman_mse', 100, 0.08),\n",
       "  (3, 'friedman_mse', 100, 0.08),\n",
       "  (3, 'friedman_mse', 100, 0.1),\n",
       "  (3, 'friedman_mse', 100, 0.1),\n",
       "  (3, 'friedman_mse', 100, 0.1),\n",
       "  (3, 'friedman_mse', 100, 0.12),\n",
       "  (3, 'friedman_mse', 100, 0.12),\n",
       "  (3, 'friedman_mse', 100, 0.12),\n",
       "  (3, 'squared_error', 50, 0.08),\n",
       "  (3, 'squared_error', 50, 0.08),\n",
       "  (3, 'squared_error', 50, 0.08),\n",
       "  (3, 'squared_error', 50, 0.1),\n",
       "  (3, 'squared_error', 50, 0.1),\n",
       "  (3, 'squared_error', 50, 0.1),\n",
       "  (3, 'squared_error', 50, 0.12),\n",
       "  (3, 'squared_error', 50, 0.12),\n",
       "  (3, 'squared_error', 50, 0.12),\n",
       "  (3, 'squared_error', 75, 0.08),\n",
       "  (3, 'squared_error', 75, 0.08),\n",
       "  (3, 'squared_error', 75, 0.08),\n",
       "  (3, 'squared_error', 75, 0.1),\n",
       "  (3, 'squared_error', 75, 0.1),\n",
       "  (3, 'squared_error', 75, 0.1),\n",
       "  (3, 'squared_error', 75, 0.12),\n",
       "  (3, 'squared_error', 75, 0.12),\n",
       "  (3, 'squared_error', 75, 0.12),\n",
       "  (3, 'squared_error', 100, 0.08),\n",
       "  (3, 'squared_error', 100, 0.08),\n",
       "  (3, 'squared_error', 100, 0.08),\n",
       "  (3, 'squared_error', 100, 0.1),\n",
       "  (3, 'squared_error', 100, 0.1),\n",
       "  (3, 'squared_error', 100, 0.1),\n",
       "  (3, 'squared_error', 100, 0.12),\n",
       "  (3, 'squared_error', 100, 0.12),\n",
       "  (3, 'squared_error', 100, 0.12),\n",
       "  (3, 'mse', 50, 0.08),\n",
       "  (3, 'mse', 50, 0.08),\n",
       "  (3, 'mse', 50, 0.08),\n",
       "  (3, 'mse', 50, 0.1),\n",
       "  (3, 'mse', 50, 0.1),\n",
       "  (3, 'mse', 50, 0.1),\n",
       "  (3, 'mse', 50, 0.12),\n",
       "  (3, 'mse', 50, 0.12),\n",
       "  (3, 'mse', 50, 0.12),\n",
       "  (3, 'mse', 75, 0.08),\n",
       "  (3, 'mse', 75, 0.08),\n",
       "  (3, 'mse', 75, 0.08),\n",
       "  (3, 'mse', 75, 0.1),\n",
       "  (3, 'mse', 75, 0.1),\n",
       "  (3, 'mse', 75, 0.1),\n",
       "  (3, 'mse', 75, 0.12),\n",
       "  (3, 'mse', 75, 0.12),\n",
       "  (3, 'mse', 75, 0.12),\n",
       "  (3, 'mse', 100, 0.08),\n",
       "  (3, 'mse', 100, 0.08),\n",
       "  (3, 'mse', 100, 0.08),\n",
       "  (3, 'mse', 100, 0.1),\n",
       "  (3, 'mse', 100, 0.1),\n",
       "  (3, 'mse', 100, 0.1),\n",
       "  (3, 'mse', 100, 0.12),\n",
       "  (3, 'mse', 100, 0.12),\n",
       "  (3, 'mse', 100, 0.12),\n",
       "  (4, 'friedman_mse', 50, 0.08),\n",
       "  (4, 'friedman_mse', 50, 0.08),\n",
       "  (4, 'friedman_mse', 50, 0.08),\n",
       "  (4, 'friedman_mse', 50, 0.1),\n",
       "  (4, 'friedman_mse', 50, 0.1),\n",
       "  (4, 'friedman_mse', 50, 0.1),\n",
       "  (4, 'friedman_mse', 50, 0.12),\n",
       "  (4, 'friedman_mse', 50, 0.12),\n",
       "  (4, 'friedman_mse', 50, 0.12),\n",
       "  (4, 'friedman_mse', 75, 0.08),\n",
       "  (4, 'friedman_mse', 75, 0.08),\n",
       "  (4, 'friedman_mse', 75, 0.08),\n",
       "  (4, 'friedman_mse', 75, 0.1),\n",
       "  (4, 'friedman_mse', 75, 0.1),\n",
       "  (4, 'friedman_mse', 75, 0.1),\n",
       "  (4, 'friedman_mse', 75, 0.12),\n",
       "  (4, 'friedman_mse', 75, 0.12),\n",
       "  (4, 'friedman_mse', 75, 0.12),\n",
       "  (4, 'friedman_mse', 100, 0.08),\n",
       "  (4, 'friedman_mse', 100, 0.08),\n",
       "  (4, 'friedman_mse', 100, 0.08),\n",
       "  (4, 'friedman_mse', 100, 0.1),\n",
       "  (4, 'friedman_mse', 100, 0.1),\n",
       "  (4, 'friedman_mse', 100, 0.1),\n",
       "  (4, 'friedman_mse', 100, 0.12),\n",
       "  (4, 'friedman_mse', 100, 0.12),\n",
       "  (4, 'friedman_mse', 100, 0.12),\n",
       "  (4, 'squared_error', 50, 0.08),\n",
       "  (4, 'squared_error', 50, 0.08),\n",
       "  (4, 'squared_error', 50, 0.08),\n",
       "  (4, 'squared_error', 50, 0.1),\n",
       "  (4, 'squared_error', 50, 0.1),\n",
       "  (4, 'squared_error', 50, 0.1),\n",
       "  (4, 'squared_error', 50, 0.12),\n",
       "  (4, 'squared_error', 50, 0.12),\n",
       "  (4, 'squared_error', 50, 0.12),\n",
       "  (4, 'squared_error', 75, 0.08),\n",
       "  (4, 'squared_error', 75, 0.08),\n",
       "  (4, 'squared_error', 75, 0.08),\n",
       "  (4, 'squared_error', 75, 0.1),\n",
       "  (4, 'squared_error', 75, 0.1),\n",
       "  (4, 'squared_error', 75, 0.1),\n",
       "  (4, 'squared_error', 75, 0.12),\n",
       "  (4, 'squared_error', 75, 0.12),\n",
       "  (4, 'squared_error', 75, 0.12),\n",
       "  (4, 'squared_error', 100, 0.08),\n",
       "  (4, 'squared_error', 100, 0.08),\n",
       "  (4, 'squared_error', 100, 0.08),\n",
       "  (4, 'squared_error', 100, 0.1),\n",
       "  (4, 'squared_error', 100, 0.1),\n",
       "  (4, 'squared_error', 100, 0.1),\n",
       "  (4, 'squared_error', 100, 0.12),\n",
       "  (4, 'squared_error', 100, 0.12),\n",
       "  (4, 'squared_error', 100, 0.12),\n",
       "  (4, 'mse', 50, 0.08),\n",
       "  (4, 'mse', 50, 0.08),\n",
       "  (4, 'mse', 50, 0.08),\n",
       "  (4, 'mse', 50, 0.1),\n",
       "  (4, 'mse', 50, 0.1),\n",
       "  (4, 'mse', 50, 0.1),\n",
       "  (4, 'mse', 50, 0.12),\n",
       "  (4, 'mse', 50, 0.12),\n",
       "  (4, 'mse', 50, 0.12),\n",
       "  (4, 'mse', 75, 0.08),\n",
       "  (4, 'mse', 75, 0.08),\n",
       "  (4, 'mse', 75, 0.08),\n",
       "  (4, 'mse', 75, 0.1),\n",
       "  (4, 'mse', 75, 0.1),\n",
       "  (4, 'mse', 75, 0.1),\n",
       "  (4, 'mse', 75, 0.12),\n",
       "  (4, 'mse', 75, 0.12),\n",
       "  (4, 'mse', 75, 0.12),\n",
       "  (4, 'mse', 100, 0.08),\n",
       "  (4, 'mse', 100, 0.08),\n",
       "  (4, 'mse', 100, 0.08),\n",
       "  (4, 'mse', 100, 0.1),\n",
       "  (4, 'mse', 100, 0.1),\n",
       "  (4, 'mse', 100, 0.1),\n",
       "  (4, 'mse', 100, 0.12),\n",
       "  (4, 'mse', 100, 0.12),\n",
       "  (4, 'mse', 100, 0.12)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2540bd97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T09:46:22.145038Z",
     "start_time": "2022-12-06T09:46:22.116797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018969723709451568"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26eb0c3",
   "metadata": {},
   "source": [
    "### Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef0db827",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T11:35:35.451616Z",
     "start_time": "2022-12-06T11:35:35.419318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=0.9)),\n",
       "                (&#x27;gradientboostingregressor&#x27;,\n",
       "                 GradientBoostingRegressor(max_depth=2))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;pca&#x27;, PCA(n_components=0.9)),\n",
       "                (&#x27;gradientboostingregressor&#x27;,\n",
       "                 GradientBoostingRegressor(max_depth=2))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=0.9)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=2)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('pca', PCA(n_components=0.9)),\n",
       "                ('gradientboostingregressor',\n",
       "                 GradientBoostingRegressor(max_depth=2))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(PCA(), GradientBoostingRegressor())\n",
    "\n",
    "params = {'pca__n_components':0.9,\n",
    "                'gradientboostingregressor__max_depth':2,\n",
    "                'gradientboostingregressor__criterion':'friedman_mse',\n",
    "                'gradientboostingregressor__n_estimators':100,\n",
    "                'gradientboostingregressor__learning_rate':0.1}\n",
    "pipe.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = utils.return_tickers()\n",
    "for ticker in tickers:\n",
    "    model = pipe\n",
    "    model.fit(prep.readytotrain(ticker))\n",
    "    joblib.dump(model, f\"../raw_data/models/{ticker}_GradientBoostingRegressor_PCA.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = utils.return_tickers()\n",
    "y_pred = []\n",
    "for ticker in tickers:\n",
    "    model = joblib.load(model, f\"model/{ticker}_GradientBoostingRegressor_PCA.joblib\")\n",
    "    y_pred.append(model.predict(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
